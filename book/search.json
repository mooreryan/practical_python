[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Python Programming for Life Scientists",
    "section": "",
    "text": "Introduction\nWelcome to Practical Python Programming for Life Scientists! This book is designed for biology and life science students with little to no prior coding experience. Rather than aiming to make you Python experts, the goal is to help you develop fundamental programming concepts and data analysis skills using Python as a practical tool.\nThe content progresses from basic syntax through algorithms, functions, classes, error handling, data science applications, and testing methodologies. Each concept is presented with life science examples to show how programming principles can enhance your research capabilities.\nThis resource serves as an introduction to computational thinking in biological contexts, providing a solid foundation to approach scientific questions from a programming perspective and to effectively incorporate data analysis using Python into your research workflow.\nNote: This book is a work in progress and will continue to evolve with student feedback. Some sections may still be in development, with the main chapters expected to be finished by May 2025.\n\nPractical Python Programming for Life Scientists by Ryan M. Moore is licensed under CC BY 4.0",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "1  Basics",
    "section": "",
    "text": "Introduction to Python\nWelcome to your first Python tutorial! In this lesson, we’ll explore the fundamental building blocks of Python programming, including:\nThis is a comprehensive tutorial that covers a lot of ground. Don’t feel pressured to master everything at once – we’ll be practicing these concepts throughout the course. Think of this as your first exposure to these ideas, and we’ll build on them step by step in the coming weeks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#introduction-to-python",
    "href": "basics.html#introduction-to-python",
    "title": "1  Basics",
    "section": "",
    "text": "What is Python?\nPython is a high-level, interpreted programming language known for its simplicity and readability. Created by Guido van Rossum in 1991, it has become one of the most popular languages in scientific computing and bioinformatics.\n\nHigh level\nPython is a high-level programming language, meaning it handles many complex computational details automatically. For example, rather than managing computer memory directly, Python does this for you. This allows biologists and researchers to focus on solving scientific problems rather than dealing with technical computing details.\n\n\nInterpreted\nPython is an interpreted language, which means you can write code and run it immediately without an extra compilation step. This makes it ideal for bioinformatics work where you often need to:\n\nTest different approaches to data analysis\nQuickly prototype analysis pipelines\nInteractively explore datasets\n\n\n\nReadable syntax\nPython’s code is designed to be readable and clear, often reading almost like English. For example:\nif dna_sequence.startswith(start_codon) and dna_sequence.endswith(stop_codon):\n    potential_genes.append(dna_sequence)\nEven if you’re new to programming, you can probably guess that this code is looking for potential genes by checking a DNA sequence for a start and a stop codon, and if found, adding the sequence to a list of potential genes.\nThis readability is particularly valuable in research settings where code needs to be shared and reviewed by collaborators.\n\n\n\nUse cases\nPython is a versatile language that can be used for a wide range of applications, including:\n\nArtificial intelligence and machine learning (e.g., TensorFlow, PyTorch)\nWeb development (Django, Flask)\nDesktop applications (PyQt, Tkinter)\nGame development (Pygame)\nAutomation and scripting\n\nAnd of course, bioinformatics and scientific computing:\n\nSequence analysis and processing (Biopython, pysam)\nPhylogenetics (ETE Toolkit)\nData visualization (matplotlib, seaborn)\nPipeline automation (snakemake for reproducible workflows)\nMicrobial ecology and microbiome analysis (QIIME)\n\n\n\nWhy Python for bioinformatics?\nPython has become a widely used tool in bioinformatics for several key reasons:\n\nRich ecosystem: Extensive libraries specifically for biological data analysis\nActive scientific community: Regular updates and support for bioinformatics tools\nIntegration capabilities: Easily connects with other bioinformatics tools and databases\nData science support: Strong support for data manipulation and statistical analysis\nReproducibility: Excellent tools for creating reproducible research workflows\n\nWhether you’re analyzing sequencing data, building analysis pipelines, or developing new computational methods, Python provides the tools and community support needed for modern biological research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#variables",
    "href": "basics.html#variables",
    "title": "1  Basics",
    "section": "Variables",
    "text": "Variables\nThink of variables as labeled containers for storing data in your program. Just as you might label test tubes in a lab to keep track of different samples, variables let you give meaningful names to your data – whether they’re numbers, text, true/false values, or more complex information.\nFor example, instead of working with raw values like this:\n\nif 47 &gt; 40:\n    print(\"Temperature too high!\")\n\nTemperature too high!\n\n\nYou can use descriptive variables to make your code clearer:\n\ntemperature = 42.3\ntemperature_threshold = 40.0\n\nif temperature &gt; temperature_threshold:\n    print(\"Temperature too high!\")\n\nTemperature too high!\n\n\nIn this section, we’ll cover:\n\nCreating and using variables\nUnderstanding basic data types (numbers, text, true/false values)\nFollowing Python’s naming conventions\nConverting between different data types\nBest practices for using variables in scientific code\n\nBy the end, you’ll be able to use variables effectively to write clear, maintainable research code.\n\nCreating variables\nIn Python, you create a variable by giving a name to a value using the = operator. Here’s a basic example:\n\nsequence_length = 1000\nspecies_name = \"Escherichia coli\"\n\nYou can then use these variables anywhere in your code by referring to their names. Variables can be combined to create new variables:\n\n# Combining text (string) variables\ngenus = \"Escherichia\"\nspecies = \"coli\"\nfull_name = genus + \" \" + species\nprint(full_name)  # Prints: Escherichia coli\n\n# Calculations with numeric variables\nreads_forward = 1000000\nreads_reverse = 950000\ntotal_reads = reads_forward + reads_reverse\nprint(total_reads)  # Prints: 1950000\n\nEscherichia coli\n1950000\n\n\nNotice how the + operator works differently depending on what type of data we’re using:\n\nWith text (strings), it joins them together\nWith numbers, it performs addition\n\nYou can also use variables in more complex calculations:\n\ngc_count = 2200\ntotal_bases = 5000\ngc_content = gc_count / total_bases\nprint(gc_content)  # Prints: 0.44\n\n0.44\n\n\nThe ability to give meaningful names to values makes your code easier to understand and modify. Instead of trying to remember what the number 5000 represents, you can use a clear variable name like total_bases.\n\n\nReassigning variables\nPython allows you to change what’s stored in a variable after you create it. Let’s see how this works:\n\nread_depth = 100\nprint(f\"Initial read depth: {read_depth}\")\n\nread_depth = 47\nprint(f\"Updated read depth: {read_depth}\")\n\nInitial read depth: 100\nUpdated read depth: 47\n\n\nThis flexibility extends even further – Python lets you change not just the value, but also the type of data a variable holds:\n\nquality_score = 30\nquality_score = \"High quality\"\nprint(quality_score)\n\nHigh quality\n\n\nWhile this flexibility can be useful, it can also lead to unexpected behavior if you’re not careful. Here’s an example that could cause problems in a sequence analysis pipeline:\n\n# Correctly calculates and prints the total number of sequences.\nsequences_per_sample = 1000\nsample_count = 5\ntotal_sequences = sequences_per_sample * sample_count\nprint(f\"total sequences: {total_sequences}\")\n\n# This one produces an unexpected result!\nsequences_per_sample = \"1000 sequences \"\nsample_count = 5\ntotal_sequences = sequences_per_sample * sample_count\nprint(f\"total sequences: {total_sequences}\")\n\ntotal sequences: 5000\ntotal sequences: 1000 sequences 1000 sequences 1000 sequences 1000 sequences 1000 sequences \n\n\nIn the second case, instead of performing multiplication, Python repeats the string \"1000 sequences \" 5 times! This is probably not what you wanted in your genomics pipeline!\nThis kind of type changing can be a common source of bugs, especially when:\n\nProcessing input from files or users\nHandling missing or invalid data\nConverting between different data formats\n\nBest practice is to be consistent with your variable types throughout your code, and explicitly convert between types when necessary.\n\nAugmented assignments\nLet’s look at a common pattern when working with variables. Here’s one way to increment a counter:\n\nread_count = 100\nread_count = read_count + 50\nprint(f\"Total reads: {read_count}\")\n\nTotal reads: 150\n\n\nPython provides a shorter way to write this using augmented assignment operators:\n\nread_count = 100\nread_count += 50\nprint(f\"Total reads: {read_count}\")\n\nTotal reads: 150\n\n\nThese augmented operators combine arithmetic with assignment. Common ones include:\n\n+=: augmented addition (increment)\n-=: augmented subtraction (decrement)\n*=: augmented multiplication\n/=: augmented division\n\nThese operators are particularly handy when updating running totals or counters, like when tracking how many sequences pass quality filters. We’ll explore more uses in the next tutorial.\n\n\n\nNamed constants\nSometimes you’ll want to define values that shouldn’t change throughout your program.\n\nGENETIC_CODE_SIZE = 64\nprint(f\"There are {GENETIC_CODE_SIZE} codons in the standard genetic code\")\n\nDNA_BASES = ['A', 'T', 'C', 'G']\nprint(f\"The DNA bases are: {DNA_BASES}\")\n\nThere are 64 codons in the standard genetic code\nThe DNA bases are: ['A', 'T', 'C', 'G']\n\n\nIn Python, we use ALL_CAPS names as a convention to indicate these values shouldn’t change. However, it’s important to understand that Python doesn’t actually prevent these values from being changed. For example:\n\nMIN_QUALITY_SCORE = 30\nprint(f\"Filtering sequences with quality scores below {MIN_QUALITY_SCORE}\")\n\nMIN_QUALITY_SCORE = 20  # We can change it, even though we shouldn't!\nprint(f\"Filtering sequences with quality scores below {MIN_QUALITY_SCORE}\")\n\nFiltering sequences with quality scores below 30\nFiltering sequences with quality scores below 20\n\n\nThink of Python variables like labels on laboratory samples: you can always move a label from one test tube to another. When you write:\n\nDNA_BASES = ['A', 'T', 'C', 'G']\nDNA_BASES = ['A', 'U', 'C', 'G']  # Oops, switched to RNA bases!\nprint(f\"These are now RNA bases: {DNA_BASES}\")\n\nThese are now RNA bases: ['A', 'U', 'C', 'G']\n\n\nYou’re not modifying the original list of DNA bases – instead, you’re creating a new list and moving the DNA_BASES label to point to it. The original list isn’t “protected” in any way. So, it’s more of a convention that ALL_CAPS variables be treated as constants in your code, even though Python won’t enforce this rule.\n\n\nDangerous assignments\nHere’s a common pitfall when naming variables in Python – accidentally overwriting built-in functions.\nPython has several built-in functions that are always available, including one called str that converts values to strings. For example:\nsequence = str()  # Creates an empty string\nsequence\nNote: if you convert this static code block to one that is runnable, and then actually run it, it would cause errors in the rest of the notebook in any place that uses the str function. If you do this, you will need to restart the notebook kernel.\nHowever, Python will let you use these built-in names as variable names (though you shouldn’t!):\nstr = \"ATCGGCTAA\"  # Don't do this!\nNow if you try to use the str function later in your code:\nquality_score = 35\nsequence_info = str(quality_score)  # This will fail!\nYou’ll get an error:\nTypeError: 'str' object is not callable\nThis error occurs because we’ve “shadowed” the built-in str function with our own variable. Python now thinks we’re trying to use the string “ATCGGCTAA” as a function, which doesn’t work!\nWe’ll discuss errors in more detail in a future lesson. For now, remember to avoid using Python’s built-in names (like str, list, dict, set, len) as variable names. You can find a complete list of built-ins in the Python documentation.\n\n\nNaming variables\nClear, descriptive variable names are crucial for writing maintainable code. When you revisit your analysis scripts months later, good variable names will help you remember what each part of your code does.\n\nValid names\nPython variable names can include:\n\nLetters (A-Z, a-z)\nNumbers (0-9, but not as the first character)\nUnderscores (_)\n\nWhile Python allows Unicode characters (like Greek letters), it’s usually better to stick with standard characters:\n\nπ = 3.14  # Possible, but not recommended\npi = 3.14  # Better!\n\n\n\nCase Sensitivity\nPython treats uppercase and lowercase letters as different characters:\n\nsequence = \"ATCG\"\nSequence = \"GCTA\"\nprint(f\"{sequence} != {Sequence}\")\n\nATCG != GCTA\n\n\nTo avoid confusion, stick with lowercase for most variable names.\n\n\nNaming Conventions\nFor multi-word variable names, Python programmers typically use snake_case (lowercase words separated by underscores):\n\n# Good -- snake case\nread_length = 150\nsequence_count = 1000\nis_high_quality = True\n\n# Avoid - camelCase or PascalCase\nreadLength = 150\nSequenceCount = 1000\n\n\n\nGuidelines for Good Names\nHere are some best practices for naming variables in your code:\nUse descriptive names that explain the variable’s purpose:\n\n# Clear and descriptive\nsequence_length = 1000\nquality_threshold = 30\n\n# Too vague\nx = 1000\nthreshold = 30\n\nUse nouns for variables that hold values:\n\nread_count = 500\ndna_sequence = \"ATCG\"\n\nBoolean variables often start with is_, has_, or similar:\n\nis_paired_end = True\nhas_adapter = False\n\nCollections (which we’ll cover later) often use plural names:\n\nsequences = [\"ATCG\", \"GCTA\"]\nquality_scores = [30, 35, 40]\n\nCommon exceptions where short names are okay:\n\ni, j, k for loop indices\nx, y, z for coordinates\nStandard abbreviations like msg for message, num for number\n\nKeep names reasonably short while still being clear:\n\n# Too long\nnumber_of_sequences_passing_quality_filter = 100\n# Better\npassing_sequences = 100\n\nRemember: your code will be read more often than it’s written, both by others and by your future self. Clear variable names make your code easier to understand and maintain.\nFor more detailed naming guidelines, check Python’s PEP 8 Style Guide.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#data-types",
    "href": "basics.html#data-types",
    "title": "1  Basics",
    "section": "Data Types",
    "text": "Data Types\nPython has many different types of data it can work with. Each data type has its own special properties and uses.\nIn this section, we’ll cover the basic data types you’ll use frequently in your code:\n\nNumbers\n\nIntegers (whole numbers, like sequence lengths or read counts)\nFloating-point numbers (decimal numbers, like expression levels or ratios)\n\nStrings (text, like DNA sequences or gene names)\nBoolean values (True/False, like whether a sequence passed quality control)\n\nWe’ll learn how to:\n\nIdentify what type of data you’re working with\nConvert between different types when needed\n\nUnderstanding these fundamental data types is crucial for handling data correctly in your programs.\n\nChecking the type of a value\nPython is a dynamically typed language, meaning a variable’s type can change during your program. While this flexibility is useful, it’s important to keep track of your data types to avoid errors in your analysis.\nYou can check a variable’s type using Python’s built-in type() function. Here’s how:\n\nsequence_length = 150\nprint(type(sequence_length))  # &lt;class 'int'&gt;\n\nsequence = \"ATCGGCTAA\"\nprint(type(sequence))  # &lt;class 'str'&gt;\n\nis_valid = True\nprint(type(is_valid))  # &lt;class 'bool'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n&lt;class 'bool'&gt;\n\n\nAs shown above, type() tells us exactly what kind of data we’re working with. This can be particularly helpful when debugging calculations that aren’t working as expected, or verifying data is in the correct format.\nDon’t worry too much about the class keyword in the output – we’ll cover classes in detail later. For now, focus on recognizing the basic types: int for integers, str for strings (text), and bool for True/False values.\n\n\nNumeric types (int, float)\nPython has two main types for handling numbers:\n\nint: Integers (whole numbers) for counting things like:\n\nNumber of sequences\nRead lengths\nGene counts\n\nfloat: Floating-point numbers (decimals) for measurements like:\n\nExpression levels\nP-values\nGC content percentages\n\n\nFor readability with large numbers, you can use underscores: 1_000_000 reads is clearer than 1000000 reads.\n\nNumeric operations\nThe operators +, -, *, / are used to perform the basic arithmetic operations.\n\nforward_reads = 1000\nreverse_reads = 800\nprint(forward_reads + reverse_reads)\nprint(forward_reads - reverse_reads)\nprint(forward_reads * 2)\nprint((forward_reads + reverse_reads) / 100)\n\n1800\n200\n2000\n18.0\n\n\nFloat division (/) always returns a float, whereas integer division (//) returns an int by performing floor division.\n\ntotal_bases = 17\nreads = 5\nprint(total_bases / reads)\nprint(total_bases // reads)\n\n3.4\n3\n\n\nThe operator ** is used for exponentiation.\n\nprint(2 ** 8)\nprint(8 ** 2)\n\n256\n64\n\n\nParentheses () can be used to group expressions and control the order of operations.\n\n# Order of operations\nprint(2 + 3 * 4)     # multiplication before addition\nprint( (2 + 3) * 4 ) # parentheses first\n\n14\n20\n\n\nModulo (%) gives remainder of division\n\nposition = 17\ncodon_position = position % 3  # Which position in codon (0, 1, or 2)\nprint(codon_position)\n\n2\n\n\nBe careful about combining negative numbers with floor division or modulo. Here are some interesting examples showing how negative numbers behave with floor division and modulo in Python:\n\n# Floor division with negative numbers\nprint(\"Floor division with negative numbers:\")\n# Rounds down to nearest integer\nprint(17 // 5)\n# Rounds down, not toward zero\nprint(-17 // 5)\nprint(17 // -5)\nprint(-17 // -5)\n\n# Modulo with negative numbers\nprint(\"\\nModulo with negative numbers:\")\nprint(17 % 5)\n# Result is positive with positive divisor\nprint(-17 % 5)\n# Result has same sign as divisor\nprint(17 % -5)\nprint(-17 % -5)\n\nFloor division with negative numbers:\n3\n-4\n-4\n3\n\nModulo with negative numbers:\n2\n3\n-3\n-2\n\n\nDon’t worry too much about the details of how negative numbers work with division and modulo operations. Just be aware that they can behave unexpectedly, and look up the specific rules if you need them.\n\n\nScientific notation\nScientific notation is essential when working with very large or small numbers:\n\n# 3.2 billion bases\ngenome_size = 3.2e9\n\n# 0.00000001 mutations per base\nmutation_rate = 1e-8\n\n\n\nPrecision Considerations\n\nIntegers\nPython can handle arbitrarily large integers, limited only by memory:\n\nbig_number = 125670495610435017239401723907559279347192756\nprint(big_number)\n\n125670495610435017239401723907559279347192756\n\n\n\n\nFloats\nFloating-point numbers have limited precision (about 15-17 decimal digits). This can affect calculations:\n\nx = 0.1\ny = 0.2\n\n# Might not be exactly 0.3\nprint(x + y)\n\n0.30000000000000004\n\n\nWhile these precision errors are usually small, they can accumulate in large-scale calculations.\n\n\n\n\nStrings\nStrings are how Python handles text data, like sequences or gene names.\n\n# Strings can use single or double quotes\nsequence = 'ATCG'\ngene_name = \"nrdA\"\nprint(sequence)\nprint(gene_name)\n\nATCG\nnrdA\n\n\nStrings are immutable – once created, they cannot be modified. For example, you can’t change individual bases in a sequence directly:\n\ndna = \"ATCG\"\n# This would raise an error:\n# dna[0] = \"G\"\n\nTry uncommenting that line and see what happens!\nYou can combine strings using the + operator:\n\n# String concatenation\nsequence_1 = \"ATCG\"\nsequence_2 = \"GCTA\"\nfull_sequence = sequence_1 + sequence_2\nprint(\"the sequence is: \" + full_sequence)\n\nthe sequence is: ATCGGCTA\n\n\nSpecial characters can be included using escape sequences:\n\n\\n for new line\n\\t for tab\n\\\\ for backslash\n\n\n# Formatting sequence output\nprint(\"Sequence 1:\\tATCG\\nSequence 2:\\tGCTA\")\n\nSequence 1: ATCG\nSequence 2: GCTA\n\n\nF-strings (format strings) are particularly useful for creating formatted output. They allow you to embed variables and expressions in strings using {expression}:\n\ngene_id = \"nrdJ\"\nposition = 37_531\n\nprint(f\"Gene {gene_id} is located at position {position}\")\n\nGene nrdJ is located at position 37531\n\n\nF-strings can also format numbers, which is useful for scientific notation and precision control:\n\n# Two decimal places\ngc_content = 0.42857142857\nprint(f\"GC content: {gc_content:.2f}\")\n\n# Scientific notation\np_value = 0.000000342\nprint(f\"P-value: {p_value:.2e}\")\n\nGC content: 0.43\nP-value: 3.42e-07\n\n\nStrings can contain Unicode characters:\n\n# Unicode characters\nprint(\"你好\")\nprint(\"こんにちは\")\n\n你好\nこんにちは\n\n\nWhile Python supports Unicode characters in variable names, it’s better to use standard ASCII characters for code:\n\n# Possible, but not recommended\nα = 0.05\nβ = 0.20\n\n# Better\nalpha = 0.05\nbeta = 0.20\n\n\nCommon string operations\nString operations are fundamental for processing and manipulating textual data, formatting output, and cleaning up input in your applications and analysis pipelines.\n\nString concatenation with +\nThe + operator joins strings together:\n\n# Joining DNA sequences\nsequence1 = \"ATCG\"\nsequence2 = \"GCTA\"\ncombined_sequence = sequence1 + sequence2\nprint(combined_sequence)\n\n# Adding labels to sequences\ngene_id = \"nrdA\"\nlabeled_sequence = gene_id + \": \" + combined_sequence\nprint(labeled_sequence)\n\nATCGGCTA\nnrdA: ATCGGCTA\n\n\n\n\nString repetition with *\nThe * operator repeats a string a specified number of times:\n\n# Repeating DNA motifs\nmotif = \"AT\"\nrepeat = motif * 3\nprint(repeat)\n\n# Creating alignment gap markers\ngap = \"-\" * 6\nprint(gap)\n\nATATAT\n------\n\n\n\n\nString indexing\nPython uses zero-based indexing to access individual characters in a string. You can also use negative indices to count from the end:\n\n# Indexing\ns = \"Hello, world!\"\nprint(s[0])\nprint(s[7])\nprint(s[-1])\nprint(s[-8])\n\nH\nw\n!\n,\n\n\n\n\nString slicing\nSlicing lets you extract parts of a string using the format [start:end]. The end index is exclusive:\n\n# Slicing\ns = \"Hello, World!\"\nprint(s[0:5])\nprint(s[7:])\nprint(s[:5])\nprint(s[-6:])\nprint(s[-12:-8])\n\nHello\nWorld!\nHello\nWorld!\nello\n\n\n\n\nString methods\nPython strings have built-in methods for common operations. Here are a few common ones:\n\n# Clean up sequence data with leading/trailing white space\nraw_sequence = \"  ATCG GCTA  \"\nclean_sequence = raw_sequence.strip()\nprint(\"|\" + raw_sequence + \"|\")\nprint(\"|\" + clean_sequence + \"|\")\n\n# Convert between upper and lower case\nmixed_sequence = \"AtCg\"\nprint(mixed_sequence.upper())\nprint(mixed_sequence.lower())\n\n# Chaining methods\nmessy_sequence = \"  AtCg  \"\nclean_upper = messy_sequence.strip().upper()\nprint(\"|\" + clean_upper + \"|\")\n\n|  ATCG GCTA  |\n|ATCG GCTA|\nATCG\natcg\n|ATCG|\n\n\n\n\n\n\nBoolean values\nBoolean values represent binary states (True/False) and are used to make decisions in code:\n\nTrue represents a condition being met\nFalse represents a condition not being met\n\n(Note: These are capitalized keywords in Python!)\nBoolean variables often use prefixes like is_, has_, or contains_ to clearly indicate their purpose:\nis_paired_end = True\nhas_adapter = False\ncontains_start_codon = True\nBoolean values are used in control flow – they drive decision-making in your code:\n\nis_high_quality = True\nif is_high_quality:\n    print(\"Sequence passes quality check!\")\n\nhas_ambiguous_bases = False\nif has_ambiguous_bases:\n    # This won't execute because condition is False\n    print(\"Warning: Sequence contains N's\")\n\nSequence passes quality check!\n\n\nBoolean values are created through comparisons, for example:\n\n# Quality score checking\nquality_score = 35\nprint(quality_score &gt; 30)\nprint(quality_score &lt; 20)\nprint(quality_score == 40)\nprint(quality_score != 35)\n\nTrue\nFalse\nFalse\nFalse\n\n\nLogical operators (and, or, not) combine boolean values:\n\n# Logical operations\nprint(True and False)\nprint(True or False)\nprint(not True)\nprint(not False)\n\nFalse\nTrue\nFalse\nTrue\n\n\nFor example, you could use logical operators to combine multiple logical statements:\nis_long_enough and is_high_quality\n\nis_exempt or exceeds_threshold\n\nComparison operators In Depth\nComparison operators are used to compare “compare” values. They return a boolean value (True or False) and are often used in conditional statements and loops to control program flow.\nThe basic comparison operators are:\n\n==: equal to\n!=: not equal to\n&lt;: strictly less than\n&lt;=: less than or equal to\n&gt;: strictly greater than\n&gt;=: greater than or equal to\n\nAdditional operators we’ll cover later:\n\nis, is not: object identity\nin, not in: sequence membership\n\nHere are a couple examples:\n\n# Basic boolean values\nis_sunny = True\nis_raining = False\n\nprint(f\"Is it sunny? {is_sunny}\")\nprint(f\"Is it raining? {is_raining}\")\n\n# Comparison operations produce boolean results\ntemperature = 25\nis_hot = temperature &gt; 30\nprint(f\"Is it hot? {is_hot}\")\n\n# Logical operations\nis_good_weather = is_sunny and not is_raining\nprint(f\"Is it good weather? {is_good_weather}\")\n\nIs it sunny? True\nIs it raining? False\nIs it hot? False\nIs it good weather? True\n\n\n\n# Comparison operations\nprint(5 == 5)\nprint(5 != 5)\nprint(5 &lt; 3)\nprint(5 &lt;= 3)\nprint(5 &lt;= 5)\nprint(5 &gt; 3)\nprint(5 &gt;= 3)\nprint(5 &gt;= 5)\n\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\n\n\n\nChained Comparisons\nComparisons can be chained together, e.g. 1 &lt; 2 &lt; 3 is equivalent to 1 &lt; 2 and 2 &lt; 3.\n\n# Chained comparisons\nprint(1 &lt; 2 &lt; 3)\nprint(1 &lt; 2 &lt; 2)\nprint(1 &lt; 2 &lt;= 2)\n\n# This one is a bit weird, but it's valid Python!\nprint(1 &lt; 2 &gt; 2)\n\nTrue\nFalse\nTrue\nFalse\n\n\nThe comparisons operators can also be used to compare the values of variables.\n\n# Check if value is in valid range\ncoverage = 30\nprint(10 &lt; coverage &lt; 50)\n\nquality_score = 35\nprint(20 &lt; quality_score &lt;= 40)\n\n# Multiple range checks\ntemperature = 37.2\nprint(37.0 &lt;= temperature &lt;= 37.5)\n\nTrue\nTrue\nTrue\n\n\n\n\nComparing Strings & Other Values\nPython’s comparison operators work beyond just numbers, allowing comparisons between various types of data. Be careful though – while some comparisons make intuitive sense, others might require careful consideration or custom implementation.\n\n# Comparison of different types\nprint(\"Hello\" == \"Hello\")\nprint(\"Hello\" == \"World\")\nprint(\"Hello\" == 5)\nprint(\"Hello\" == True)\n\n# Some non-numeric types also have a natural ordering.\nprint(\"a\" &lt; \"b\")\nprint(\"a\" &lt; \"A\")\n\n# This is a bit weird, but it's valid Python!\nprint([1, 2, 3] &lt;= [10, 20, 30])\n\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\nLogical Operators In Depth\nThink of logical operators as ways to combine or modify simple yes/no conditions in your code, much like how you might combine criteria when filtering data in Excel or selecting samples for an experiment.\nFor example, you can use logical operators to express conditions like:\n\n“If a DNA sequence is both longer than 250 bases AND has no ambiguous bases, include it in the analysis”\n“If a gene is either highly expressed OR shows significant differential expression, flag it for further study”\n“If a sample is NOT properly labeled, skip it and log a warning”\n\nThese operators (and, or, not) work similarly to the way we combine conditions in everyday language. Just as you might say “I’ll go for a run if it’s not raining AND the temperature is above 60°F,” you can write code that makes decisions based on multiple criteria.\nHere are a couple of examples:\n# In a sequence quality filtering pipeline\n#\n# Both conditions must be true\nif sequence_length &gt;= 250 and quality_score &gt;= 30:\n    keep(sequence)\n\n# In a variant calling pipeline\n#\n# Either condition being true is sufficient\nif mutation_frequency &gt; 0.01 or supporting_reads &gt;= 100:\n    report(variant)\n\n# In a data validation step\n#\n# Triggers if the condition is false\nif not sample_id.startswith('PROJ_'):\n    warn_user(sample_id)\nThink of these operators as the digital equivalent of the decision-making process you use in the lab: checking multiple criteria before proceeding with an experiment, or having alternative procedures based on different conditions.\n\nBehavior of logical operators\nLet’s explore how Python’s logical operators (and, or, not) work, using examples relevant to biological data analysis.\nThink of these operators as ways to check multiple conditions, similar to how you might design experimental criteria:\n\nand: Like requiring ALL criteria to be met (e.g., both proper staining AND correct cell count)\nor: Like accepting ANY of several criteria (e.g., either elevated temperature OR positive test result)\nnot: Like reversing a condition (e.g., NOT contaminated)\n\nHere’s a truth table showing all possible combinations.\n\n\n\nA\nB\nA and B\nA or B\nnot A\n\n\n\n\nTrue\nTrue\nTrue\nTrue\nFalse\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\nFalse\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\nHere are the rules:\n\nand only gives True if both conditions are True (like requiring all quality checks to pass)\nor gives True if at least one condition is True (like having multiple acceptable criteria)\nnot flips True to False and vice versa (like converting “passed QC” to “failed QC”)\n\nInterestingly, Python can also evaluate non-boolean values (values that aren’t strictly True or False) using these operators. We call values that Python treats as True “truthy” and values it treats as False “falsy”. This becomes important when working with different types of data in your programs and analysis pipelines.\n\n\nUnderstanding “Truthy” and “Falsy” Values\nIn Python, every value can be interpreted as either “true-like” (truthy) or “false-like” (falsy) when used in logical operations. This is similar to how in biology, we might categorize results as “positive” or “negative” even when the underlying data is more complex than a simple yes/no.\nThink of “falsy” values as representing empty, zero, or null states – essentially, the absence of meaningful data. Python considers the following values as “falsy”:\n\nFalse: The boolean False value\nNone: Python’s way of representing “nothing” or “no value” (like a blank entry in a spreadsheet)\nAny form of zero (like 0, 0.0)\nEmpty containers:\n\nEmpty string (\"\")\nEmpty list ([])\nEmpty set (set())\nEmpty dictionary ({})\n\n\nEverything else is considered “truthy” - meaning it represents the presence of some meaningful value or data.\nLet’s look at some practical examples. We can use Python’s bool() function to explicitly check whether Python considers a value truthy or falsy:\n\n# Examples from sample processing:\nsample_count = 0\n# False (no samples)\nprint(bool(sample_count))\n\nsample_ids = []\n# False (empty list of IDs)\nprint(bool(sample_ids))\n\npatient_data = {}\n # False (empty data table)\nprint(bool(patient_data))\n\n# Compare with:\nsample_count = 5\n# True (we have samples)\nprint(bool(sample_count))\n\nsample_ids = [\"A1\", \"B2\"]\n# True (we have some IDs)\nprint(bool(sample_ids))\n\npatient_data = {\"age\": 45}\n# True (we have some data)\nprint(bool(patient_data))\n\nFalse\nFalse\nFalse\nTrue\nTrue\nTrue\n\n\nUnderstanding truthy and falsy values becomes particularly useful when writing conditions in your code, like checking whether you have data before proceeding with analysis:\n# Sort of like saying: if there are some samples IDs,\n# then do something with them.\nif sample_ids:\n    process_samples(sample_ids)\nelse:\n    print(\"No samples to process\")\nWe’ll see more examples of how this concept is useful in practice as we work through more advanced topics.\n\n\nEven More Details About and and or\nNote: This section is a bit low-level, so don’t worry too much about it. It’s just here for your reference.\nOne kind of neat thing about the logical operators is that you can directly use them as a type of control flow.\n\nand\nGiven an expression a and b, the following steps are taken:\n\nFirst, evaluate a.\nIf a is “falsy”, then return the value of a.\nOtherwise, evaluate b and return its value.\n\nCheck it out:\n\na = \"apple\"\nb = \"banana\"\nresult = a and b\nprint(result)\n\nname = \"Maya\"\nage = 45\nresult = age &gt;= 18 and f\"{name} is an adult\"\nprint(result)\n\nname = \"Amira\"\nage = 15\nresult = age &gt;= 18 and f\"{name} is an adult\"\nprint(result)\n\nbanana\nMaya is an adult\nFalse\n\n\nWere the values assigned to result what you expected?\n\n\nor\nGiven an expression a or b, the following steps are taken:\n\nFirst, evaluate a.\nIf a is “truthy”, then return the value of a.\nOtherwise, evaluate b and return its value.\n\nLet’s return to the previous example, but this time we will use or instead of and.\n\na = \"apple\"\nb = \"banana\"\nresult = a or b\nprint(result)\n\nname = \"Maya\"\nage = 45\n# Observe that this code isn't really doing what we want it to do.\n# `result` will be True, rather than \"Maya is an adult\".\n# That's because it should be using `and`\n#   ...again, it's just for illustration.\nresult = age &gt;= 18 or f\"{name} is an adult\"\nprint(result)\n\nname = \"Amira\"\nage = 15\n# This code is a bit obscure, and you probably wouldn't\n# write it like this in practice.  But it illustrates the\n# point.\nresult = age &gt;= 18 or f\"{name} is not an adult\"\nprint(result)\n\napple\nTrue\nAmira is not an adult\n\n\nWere the values assigned to result what you expected?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#control-flow",
    "href": "basics.html#control-flow",
    "title": "1  Basics",
    "section": "Control Flow",
    "text": "Control Flow\nThink of control flow as the decision-making logic in your code - like following a lab protocol, but for data analysis. Just as you make decisions in the lab (“if the pH is too high, add buffer”), your code needs to make decisions about how to handle different situations.\nControl flow statements are the programming equivalent of those decision points in your protocols. They let your program take different paths depending on the conditions it encounters, much like how you might follow different steps in an experiment based on your observations.\nIn this section, we’ll cover several ways to build these decision points into your code:\n\nSimple if statements (like “if the sequence quality is low, skip it”)\nif-else statements (like “if the gene is expressed, mark it as active; otherwise, mark it as inactive”)\nif-elif-else chains (for handling multiple possibilities, like different ranges of p-values)\nNested conditions (for more complex decisions, like filtering sequences based on multiple quality metrics)\n\nControl flow is essential for writing programs that can:\n\nMake decisions based on data\nHandle different scenarios\nRespond to user input\nConditionally process data\n\nJust as following the right branch points in a protocol is crucial for experimental success, proper control flow is key to writing programs that correctly handle your data.\nLet’s explore the main types of control flow in Python:\n\nif Statements\nThink of these as your basic yes/no checkpoints, like checking if a sample meets quality control:\n\nquality_score = 35\nif quality_score &gt; 30:\n    print(\"Sample passes QC\")\n\nSample passes QC\n\n\n\n\nif-else Statements\nThese handle two alternative outcomes, like categorizing genes as expressed or not expressed:\n\nexpression_level = 1.5\nif expression_level &gt; 1.0:\n    print(\"Gene is upregulated\")\nelse:\n    print(\"Gene is not upregulated\")\n\nGene is upregulated\n\n\n\n\nif-elif-else Chains\nPerfect for handling multiple possibilities, like categorizing p-values or expression levels:\n\np_value = 0.03\nif p_value &lt; 0.01:\n    print(\"Highly significant\")\nelif p_value &lt; 0.05:\n    print(\"Significant\")\nelse:\n    print(\"Not significant\")\n\nSignificant\n\n\n\n\nMultiple Conditions\nSometimes you need to check multiple criteria, like filtering sequencing data:\n\nread_length = 100\ngc_content = 0.45\nquality_score = 35\n\nif read_length &gt;= 100 and quality_score &gt; 30 and 0.4 &lt;= gc_content &lt;= 0.6:\n    print(\"Read passes all quality filters\")\nelse:\n    print(\"Read filtered out\")\n\nRead passes all quality filters\n\n\n\n\nKey Points to Remember\n\nConditions are checked in order from top to bottom\nOnly the first matching condition’s code block will execute\nKeep your conditions clear and logical, like a well-designed experimental workflow\nTry to avoid deeply nested conditions as they can become confusing\n\nThink of control flow as building decision points into your data analysis pipeline. Just as you wouldn’t proceed with a PCR if your DNA quality was poor, your code can automatically make similar decisions about data processing.\n\n\nNested Conditional Statements\nConditional statements can also be nested. Here is some code that is checking if someone can go to the beach. If they are not at work, and the weather is sunny, then they can go to the beach.\n\nat_work = False\nweather = \"sunny\"\n\nif weather == \"sunny\" and not at_work:\n    print(\"It's sunny and you are not at work, let's go to the beach!\")\nelse:\n    print(\"We can't go to the beach today for some reason.\")\n\n# Let's move the check for at_work nested inside the if statement that checks\n# the weather.\n#\n# Note that this code isn't equivalent to the previous code, just an example\n# of nesting.\n\nif weather == \"sunny\":\n    if at_work:\n        print(\"You are at work and can't go to the beach.\")\n    else:\n        print(\"It's sunny and you are not at work, let's go to the beach!\")\nelse:\n    print(\"It's not sunny, so we can't go to the beach regardless.\")\n\n# Just to be clear, let's \"unnest\" that conditional.\nif weather == \"sunny\" and at_work:\n    print(\"You are at work and can't go to the beach.\")\nelif weather == \"sunny\":\n    print(\"It's sunny and you are not at work, let's go to the beach!\")\nelse:\n    print(\"It's not sunny, so we can't go to the beach regardless.\")\n\nIt's sunny and you are not at work, let's go to the beach!\nIt's sunny and you are not at work, let's go to the beach!\nIt's sunny and you are not at work, let's go to the beach!\n\n\n\n\nA Note on Keeping Things Simple\nJust as you want to keep your experimental protocols clear and straightforward, the same principle applies to writing conditional statements in your code. Think of deeply nested if-statements like trying to follow a complicated diagnostic flowchart - the more branches and decision points you add, the easier it is to lose track of where you are.\nFor example, imagine designing a PCR troubleshooting guide where each problem leads to three more questions, each with their own set of follow-up questions. While technically complete, it would be challenging for anyone to follow correctly. The same goes for code – when we stack too many decisions inside other decisions, we’re setting ourselves up for confusion.\nHere’s why keeping conditions simple matters:\n\nEach decision point is an opportunity for something to go wrong (like each step in a protocol)\nComplex nested conditions are harder to debug (like trying to figure out where a multi-step experiment went wrong)\nSimple, clear code is easier for colleagues to review and understand\n\nWhen you find yourself writing deeply nested conditions, it’s often a sign to step back and consider whether there’s a clearer way to structure your code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#basic-built-in-functions",
    "href": "basics.html#basic-built-in-functions",
    "title": "1  Basics",
    "section": "Basic Built-in Functions",
    "text": "Basic Built-in Functions\nThink of Python’s built-in functions as your basic laboratory toolkit - they’re always there when you need them, no special setup required. These functions will become your go-to tools for handling biological data, from DNA sequences to experimental measurements.\nHere are some of the most useful built-in functions you’ll use regularly:\n\nprint(): Displays your data or results\nlen(): Counts the length of something\nabs(): Gives you the absolute value\nround(): Tidies up decimal numbers\nmin() and max(): Find the lowest and highest values\nsum(): Adds up a collection of numbers\ntype(): Tells you what kind of data you’re working with (helpful for debugging)\n\nLet’s look at some examples:\n\n# Printing experimental results\nprint(\"Gene expression analysis complete!\")\n\n# Checking sequence length\ndna_sequence = \"ATCGATCGTAGCTAGCTAG\"\nlength = len(dna_sequence)\nprint(f\"This DNA sequence is {length} base pairs long.\")\n\n# Working with expression fold changes\nfold_change = -2.5\nabsolute_change = abs(fold_change)\nprint(f\"The absolute fold change is {absolute_change}x.\")\n\n# Cleaning up p-values\np_value = 0.0000234567\nrounded_p = round(p_value, 6)\nprint(f\"p-value = {rounded_p}\")\n\n# Analyzing multiple expression values\nexpression_levels = [10.2, 5.7, 8.9, 12.3, 6.8]\nlowest = min(expression_levels)\nhighest = max(expression_levels)\nprint(f\"Expression range: {lowest} to {highest}\")\n\n# Calculating average coverage\ncoverage_values = [15, 22, 18, 20, 17]\naverage_coverage = sum(coverage_values) / len(coverage_values)\nprint(f\"Average sequencing coverage: {average_coverage}x\")\n\n# Checking data types\ngene_name = \"nrdA\"\ndata_type = type(gene_name)\nprint(f\"The variable gene_name is of type: {data_type}\")\n\nGene expression analysis complete!\nThis DNA sequence is 19 base pairs long.\nThe absolute fold change is 2.5x.\np-value = 2.3e-05\nExpression range: 5.7 to 12.3\nAverage sequencing coverage: 18.4x\nThe variable gene_name is of type: &lt;class 'str'&gt;\n\n\nTo use these functions, just type the function name followed by parentheses containing your data (the “arguments”). Some functions, like min() and max(), can handle multiple inputs, which is handy when comparing several values at once.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#wrap-up",
    "href": "basics.html#wrap-up",
    "title": "1  Basics",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nIn this tutorial, we covered the fundamental building blocks of Python programming that you’ll use throughout your bioinformatics work:\n\nVariables help you store and manage data with meaningful names\nData types like numbers, strings, and booleans let you work with different kinds of biological data\nControl flow statements help your programs make decisions based on data\nBuilt-in functions provide essential tools for common programming tasks\n\nRemember:\n\nChoose clear, descriptive variable names\nBe mindful of data types when performing operations\nKeep conditional logic as simple as possible\nMake use of Python’s built-in functions for common tasks\n\nThese basics form the foundation for more advanced programming concepts we’ll explore in future tutorials. Practice working with these fundamentals – they’re the tools you’ll use to build more complex bioinformatics applications.\nDon’t worry if everything hasn’t clicked yet. Programming is a skill that develops with practice. Focus on understanding one concept at a time, and remember that you can always refer back to this tutorial as a reference.\nNext up, we’ll build on these basics to work with more complex data structures and write functions of our own!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "collections.html",
    "href": "collections.html",
    "title": "2  Collections",
    "section": "",
    "text": "Introduction to Python Collections\nIn this tutorial, we’ll explore Python’s fundamental data structures and collections – the building blocks that help organize and analyze biological data effectively. From strings for handling DNA sequences to dictionaries for mapping genes to functions, you’ll learn how to use these tools through practical examples. We’ll cover when and why to use each type, giving you the foundation needed to tackle real bioinformatics problems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#introduction-to-python-collections",
    "href": "collections.html#introduction-to-python-collections",
    "title": "2  Collections",
    "section": "",
    "text": "What are collections?\nCollections in Python are containers that can hold multiple items, and provide convenient ways to store, access, and manipulate groups of related values.\nThink of collections like different types of containers:\n\nA list is like a row of boxes where you can store items in order\nA tuple is similar but locked/sealed (immutable)\nA dictionary is like a filing cabinet with labeled folders (keys) containing items (values)\nA range represents a sequence of numbers stored in an efficient way\n\nCollections let us:\n\nGroup related data together\nProcess multiple items efficiently\nOrganize information in meaningful ways\nAccess data using consistent patterns\n\n\n\nWhy we need different data structures\nPython provides different collection types because different tasks require different tools. For example:\n\nIf you need to store multiple DNA sequences in order and have fast access to them, use a List\nIf you need to group various pieces of data together and ensure they don’t change, use a Tuple\nIf you need to look up protein functions by their names, use a Dictionary\nIf you need to generate sample numbers efficiently, use a Range\n\nUsing the right data structure for the job optimizes both speed and code clarity. As we progress through this tutorial, you’ll learn which data structures work best in different situations.\n\n\nCommon Python Data Structures at a Glance\nWe will break down the specifics of each type soon, but let’s look first at a quick example of each type:\nA list is a mutable, ordered collection of items:\n\nnucleotides = [\"A\", \"T\", \"C\", \"G\"]\nprint(nucleotides)\n\n# This is a for loop. We will talk more about them below.\nfor nucleotide in nucleotides:\n    print(nucleotide)\n\n['A', 'T', 'C', 'G']\nA\nT\nC\nG\n\n\nA tuple is an immutable, ordered collection of items:\n\n# (name, code, molecular_weight)\nalanine = (\"Alanine\", \"Ala\", 89.1)\nprint(alanine)\n\n('Alanine', 'Ala', 89.1)\n\n\nA dictionary is a mapping from keys to values:\n\n# Dictionary -- key-value pairs (gene id -&gt; function)\ngene_functions = {\n    \"TP53\": \"tumor suppression\",\n    \"BRCA1\": \"DNA repair\",\n    \"INS\": \"insulin production\"\n}\nprint(gene_functions)\n\nfor gene, function in gene_functions.items():\n    print(f\"{gene} =&gt; {function}\")\n\n{'TP53': 'tumor suppression', 'BRCA1': 'DNA repair', 'INS': 'insulin production'}\nTP53 =&gt; tumor suppression\nBRCA1 =&gt; DNA repair\nINS =&gt; insulin production\n\n\nA range is a representation of a sequence of numbers:\n\n# 96 well plate positions\nsample_ids = range(1, 96)\nprint(sample_ids)\n\nrange(1, 96)\n\n\nNotice that each collection has a dedicated syntax for creating it. This makes it easy to create collections and gives you a visual cue for which collection you’re working with.\n\nLists are formed using square brackets ([])\nTuples are created with parentheses (())\nDictionaries use curly brackets ({}) and colons (:)\nRanges are generated by the range() function\n\nBeing able to recognize these collection types and know when to use each is critical to both writing and reading code. Let’s explore them further.\nNote: Python contains other useful data structures, including sets, but we won’t cover them in this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#strings",
    "href": "collections.html#strings",
    "title": "2  Collections",
    "section": "Strings",
    "text": "Strings\nIn Python, strings are ordered collections of characters, meaning they are sequences that can be indexed, sliced, and iterated over just like other sequence types (such as lists and tuples), with each character being an individual element in the collection.\nThough we covered strings in Tutorial 1, let’s go over some basics again so that you have it here for easy reference.\n\nString Literals\nIn Python, text data is handled with str objects, or strings. You can build strings with string literals:\n\n# With single quotes\n'a string'\n\n# With double quotes\n\"another string\"\n\n# Triple quoted\n\"\"\"Here is a string.\"\"\"\n'''And here is another.'''\n\n'And here is another.'\n\n\nIf you need to embed quote marks within a string literal, you can do something like this:\n\n# Double quote in single quoted string\n'This course is \"fun\", right?'\n\n# Single quote in double quoted string\n\"Of course! It's my favorite class!\"\n\n\"Of course! It's my favorite class!\"\n\n\nThere are also escape sequences for including different kinds of text inside a string literal. Tabs and newlines are some of the more common escape sequences:\n\n# Tabs\nprint(\"name\\tage\")\n\n# Newlines\nprint(\"gene 1\\ngene 2\")\n\nname    age\ngene 1\ngene 2\n\n\n\n\nString Methods\nIn addition to common operations like indexing, slicing, and concatenation, strings have a rich set of functionality provided by string methods.\nA string method is essentially a function that is “attached” to a string. Some common string methods are:\n\nupper, lower – Case conversion\nstrip, lstrip, rstrip – Remove whitespace\nsplit – Convert string to list based on delimiter\njoin – Combine list elements into string\nreplace – Replace substring\nfind, index – Find substring position\nstartswith, endswith – Check string prefixes/suffixes\ncount – Count substring occurrences\n\nLet’s go through them now.\n\nCase Conversion\nThe upper and lower methods convert strings to uppercase or lowercase. This is useful for standardizing text or making case-insensitive comparisons.\n\ndna = \"ATCGatcg\"\n\nprint(dna.upper())\nprint(dna.lower())\n\nfragment_1 = \"ACTG\"\nfragment_2 = \"actg\"\n\n# You can convert both sequences to lower case before\n# comparing them for a case-insensitive comparison.\nprint(fragment_1.lower() == fragment_2.lower())\n\nATCGATCG\natcgatcg\nTrue\n\n\n\n\nRemove Whitespace\nThe strip method remove whitespace characters (spaces, tabs, newlines). strip removes from both ends, while lstrip and rstrip remove from left or right only. This is particularly useful when cleaning up input data.\n\ndna_sequence = \"  ATCG\\n\"\nprint(dna_sequence.strip())\n\ngene_name = \"nrdA    \"\nprint(gene_name.rstrip())\n\nATCG\nnrdA\n\n\n\n\nConvert String To List\nThe split method divides a string into a list of substrings based on a delimiter. By default, it splits on whitespace. This is useful for parsing formatted data.\n\nfasta_header = \"&gt;sp|P00452|RIR1_ECOLI Ribonucleoside-diphosphate reductase 1\"\nfields = fasta_header.split(\"|\")\nprint(fields)\n\n['&gt;sp', 'P00452', 'RIR1_ECOLI Ribonucleoside-diphosphate reductase 1']\n\n\nCheck out this neat trick where Python will let us put the different fields directly into named variables.\n\n_, uniprot_id, protein_info = fasta_header.split(\"|\")\n\nprint(f\"{uniprot_id} =&gt; {protein_info}\")\n\nP00452 =&gt; RIR1_ECOLI Ribonucleoside-diphosphate reductase 1\n\n\nPretty useful! (We will see more about this in the section on tuples.)\n\n\nCombine List Into String\nThe join method combines a list of strings into one, using the string it’s called on as a delimiter. This is useful for creating formatted output.\n\namino_acids = [\"Met\", \"Gly\", \"Val\"]\nprotein = \"-\".join(amino_acids)\nprint(protein)\n\nfields = [\"GeneName\", \"Length\", \"Count\"]\ntsv_line = \"\\t\".join(fields)\nprint(tsv_line)\n\nMet-Gly-Val\nGeneName    Length  Count\n\n\n\n\nReplace Substring\nThe replace method substitutes all occurrences of a substring with another. This is helpful for sequence modifications or text cleanup, like turning a DNA string into an RNA string.\n\ndna = \"ATCGTTA\"\nrna = dna.replace(\"T\", \"U\")\nprint(rna)\n\nAUCGUUA\n\n\n\n\nFind Substring Position\nThe find and index methods locate the position of a substring. find returns -1 if not found, while index raises an error. These are useful for sequence analysis.\n\nsequence = \"ATCGCTAGCT\"\nposition = sequence.find(\"GCT\")\nprint(position)\n\ntry:\n    position = sequence.index(\"NNN\")\n    print(position)\nexcept ValueError:\n    print(\"not found!\")\n\n3\nnot found!\n\n\nDon’t worry too much now about this try/except construction for now – we will cover it in a later tutorial! Basically, it is a way to tell Python that we think an error may occur here, and if it does, what we should do to recover.\n\n\nCheck String Prefix/Suffix\nThe startswith and endswith methods check if a string begins or ends with a given substring. These are helpful for parsing user input, or validating sequence patterns and file names.\n\ngene = \"ATGCCGTAA\"\nprint(gene.startswith(\"ATG\"))\nprint(gene.endswith(\"TAA\"))\n\nTrue\nTrue\n\n\n\n\nCount Substring Occurrences\nThe count method counts how many times a substring appears in a string. This is useful for sequence analysis and pattern counting.\n\ndna = \"ATAGATAGATAG\"\ntag_count = dna.count(\"TAG\")\nprint(tag_count)\n\n3\n\n\n\n\n\nString Summary\nIn Python, strings are immutable sequences of characters (including letters, numbers, symbols, and spaces) that are used to store and manipulate text data. They can be created using single quotes (''), double quotes (\"\"), or triple quotes (''' ''' or \"\"\" \"\"\") and support various built-in methods for operations like searching, replacing, splitting, and formatting text.\n(For more info about string indexing, slicing, etc., see Tutorial 1.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#lists",
    "href": "collections.html#lists",
    "title": "2  Collections",
    "section": "Lists",
    "text": "Lists\nLists are going to be one of your best friends in Python – they’re flexible, easy to modify, and good for handling biological sequences and experimental data.\n\nCreating Lists\nYou can create lists using square brackets [] and assign them to variables. As always, keep in mind best practices for naming variables!\n\n# A DNA sequence\ndna_sequence = [\"A\", \"T\", \"G\", \"C\", \"T\", \"A\", \"G\"]\n\n# Gene names in a pathway\npathway_genes = [\"TP53\", \"MDM2\", \"CDKN1A\", \"BAX\"]\n\n# Expression values\nexpression_levels = [0.0, 1.2, 3.4, 2.1, 0.8]\n\n# Mixed data types (though it may be best to avoid mixing types like this)\nsample_info = [\"SAMPLE001\", 37.5, \"positive\", True]\n\n# Empty list to fill later\nresults = []\n\nCreating an empty list might seem a bit weird, but is actually common practice in Python – create an empty list and then use a loop to store multiple things in it. We will see examples of this later in the tutorial.\n\n\nList Indexing and Slicing\nRemember that a list is like a row of boxes, each with something inside. The boxes are in a particular order and each has a number that you can use to access the data inside (the index).\nYou could imagine a list looking something like this:\n┌─────┬─────┬─────┬─────┬─────┐\n│ \"A\" │ \"T\" │ \"G\" │ \"A\" │ \"C\" │  (values in the list)\n└─────┴─────┴─────┴─────┴─────┘\n   0     1     2     3     4     (indices of the values)\nWhich corresponds to the following Python code:\n\nnucleotides = [\"A\", \"T\", \"G\", \"A\", \"C\"]\n# index         0    1    2    3    4\n\nDon’t forget that Python starts counting with 0 rather than with 1.\nNote: For now, don’t worry too much right now about how Python stores items in a list. Later in the tutorial, we will adjust our mental model for collections.\n\nIndexing\nSimilar to strings, you can get specific things out of a list with list_name[] syntax, which is sometimes called “indexing” the list. The most basic option is to grab items one at time:\n\n# Get single elements\ndna = \"ATGC\"\nfirst_base = dna[0]\nthird_base = dna[2]\n\nJust like with strings, you can also start indexing from the end of a list. Try to predict the outcome before uncommenting the print() statement.\n\nmystery_base = dna[-1]\n# print(mystery_base)\n\n\n\nSlicing\nIf you want to get chunks of a list, you can use “slicing”:\n\ndna = \"ACTGactgACTG\"\nfirst_four = dna[0:4]\nmiddle_section = dna[4:8]\n\nprint(first_four)\nprint(middle_section)\n\nACTG\nactg\n\n\nYou can leave off the beginning or the end of a slice as well:\n\ndna = \"ACTGactgGGGG\"\n\n# From index 4 to the end\nprint(dna[4:])\n\n# From the beginning up to index 4, but *excluding* 4.\nprint(dna[:4])\n\nactgGGGG\nACTG\n\n\nSlices can get pretty fancy. Check this out:\n\ndna = \"AaTtCcGg\"\n\n# Get every other base, starting from the beginning.\nevery_second = dna[::2]\nprint(every_second)\n\n# Get every other base starting from index 1\nevery_other_second = dna[1::2]\nprint(every_other_second)\n\nATCG\natcg\n\n\nThere are quite a few rules about slicing, which can get a bit complicated. For this reason, it’s generally best to keep your slicing operations as simple as possible.\n\n\n\nList Methods\nSimilar to strings, lists come with some methods that let you modify them or get information about them. Some of the most common are:\n\nappend\ninsert\npop\nsort\ncount\n\nLet’s take a look.\n\nAdding Items to Lists\n\ngenes = [\"TP53\"]\n\n# Adds to the end\ngenes.append(\"BRCA1\")\n\n# Adds at specific position\ngenes.insert(0, \"MDM2\")\n\n# Adds multiple items\ngenes.extend([\"ATM\", \"PTEN\"])\n\nBased on the information in the comments, what does our list look like now? Try to figure that out before running the next code block.\n\nprint(genes)\n\n['MDM2', 'TP53', 'BRCA1', 'ATM', 'PTEN']\n\n\n\n\nRemoving Items from Lists\nWe know how to add items now, but what about removing them? There are several ways to do that as well:\n\ngenes = [\"MDM2\", \"TP53\", \"BRCA1\", \"ATM\", \"PTEN\"]\n\n# Removes by value\ngenes.remove(\"BRCA1\")\nprint(f\"remaining genes: {genes}\")\n\n# Removes and returns last item\nlast_gene = genes.pop()\nprint(f\"last_gene: {last_gene}, remaining genes: {genes}\")\n\n# Removes and returns item at index\nspecific_gene = genes.pop(0)\nprint(f\"specific_gene: {specific_gene}, remaining genes: {genes}\")\n\nremaining genes: ['MDM2', 'TP53', 'ATM', 'PTEN']\nlast_gene: PTEN, remaining genes: ['MDM2', 'TP53', 'ATM']\nspecific_gene: MDM2, remaining genes: ['TP53', 'ATM']\n\n\nPay attention to pop in particular. While remove just takes a value out of our list, pop removes the item and returns it, which is what allows us to save it to a variable.\n\n\nOther Useful List Methods\nThere are many other cool list methods. Here are a few more. Try to guess what the output will be before running the code block.\n\ngenes = [\"MDM2\", \"TP53\", \"BRCA1\", \"ATM\", \"PTEN\", \"TP53\"]\n\ngenes.sort()\nprint(genes)\n\ngenes.reverse()\nprint(genes)\n\nprint(genes.count(\"TP53\"))\n\n['ATM', 'BRCA1', 'MDM2', 'PTEN', 'TP53', 'TP53']\n['TP53', 'TP53', 'PTEN', 'MDM2', 'BRCA1', 'ATM']\n2\n\n\n\n\n\nList Operations\nWe talked about operators in Tutorial 1. These operators can also be applied to lists in various ways. Let’s check it out.\nSimilar to strings, you can concatenate lists into a single list using +:\n\nforward_primers = [\"ATCG\", \"GCTA\"]\nreverse_primers = [\"TAGC\", \"CGAT\"]\nall_primers = forward_primers + reverse_primers\nprint(all_primers)\n\n['ATCG', 'GCTA', 'TAGC', 'CGAT']\n\n\nTake a small list a “multiply” its components to make a bigger list using *:\n\n# Creates a poly-A sequence\npoly_a = [\"A\"] * 20\nprint(poly_a)\n\n['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n\n\nCheck if something is in a list using in:\n\ngenes = [\"MDM2\", \"TP53\", \"BRCA1\", \"ATM\", \"PTEN\", \"TP53\"]\n\n# Checking membership\nif \"TP53\" in genes:\n    print(\"TP53 present in our pathway\")\n\nif \"POLA\" in genes:\n    print(\"POLA is not found!\")\n\nTP53 present in our pathway\n\n\nAnd get the length of a list using len:\n\nsamples = [\"Treatment_1\", \"Control_1\", \"Treatment_2\", \"Control_2\"]\ntotal_samples = len(samples)\nprint(total_samples)\n\n4\n\n\n\n\nNested Lists\nLists can contain other lists, useful for representing things like matrices, graph connections, and simple hierarchical data.\n\n# Matrix\nsequences = [\n    [\"A\", \"T\", \"G\", \"C\"],\n    [\"G\", \"C\", \"T\", \"A\"],\n    [\"T\", \"A\", \"G\", \"C\"]\n]\nprint(sequences)\n\n# Coordinates\ncoordinates =[\n    [1, 2],\n    [3, 4],\n    [5, 6]\n]\nprint(coordinates)\n\n# Simple hierarchical data: Experimental data with replicates\nexpression_data = [\n    [\"Gene1\", [1.1, 1.2, 1.0]],  # Gene name and replicate values\n    [\"Gene2\", [2.1, 2.3, 1.9]],\n    [\"Gene3\", [0.5, 0.4, 0.6]]\n]\nprint(expression_data)\n\n[['A', 'T', 'G', 'C'], ['G', 'C', 'T', 'A'], ['T', 'A', 'G', 'C']]\n[[1, 2], [3, 4], [5, 6]]\n[['Gene1', [1.1, 1.2, 1.0]], ['Gene2', [2.1, 2.3, 1.9]], ['Gene3', [0.5, 0.4, 0.6]]]\n\n\nMany times, there will be a better solution to your problem than nesting lists in this way, but it’s something that you should be aware of should the need arise.\nNested lists can be accessed just like regular lists, but there will be more “layers” to get through depending on what you want out of them.\n\n# Accessing nested data\n\nfirst_sequence = sequences[0]\nprint(first_sequence)\n\ngene2_rep2 = expression_data[1][1][1]\nprint(gene2_rep2)\n\n['A', 'T', 'G', 'C']\n2.3\n\n\nLists are very flexible in Python, and so can be complicated. However, it will be good for you to get comfortable with Lists as they are one of the most commonly used data structures!\n\nWhat Does Python Actually Store in the List?\nWhen working with lists and other collections in Python, there’s a crucial detail about how Python manages data that might seem counterintuitive at first. Let’s explore this through a simple example using 2D points.\nFirst, let’s create some points and store them in a list:\n\n# Represent points as [x, y] coordinates\npoint_a = [0, 3]\npoint_b = [1, 2]\n\n# Store points in a list\npoints = [point_a, point_b]\nprint(points)\n\n[[0, 3], [1, 2]]\n\n\nWe can access individual coordinates using nested indexing:\n\n# Get the y-coordinate of the first point\nprint(points[0][1])\n# Get the x-coordinate of the second point\nprint(points[1][0])\n\n3\n1\n\n\nNow here’s where things get interesting. Let’s modify some values:\n\n# Double the y-coordinate of the first point\npoints[0][1] *= 2\nprint(points)\n\n# Now modify the original point_b\npoint_b[0] *= 10\nprint(point_b)\n\n# What do you think our points list looks like now?\nprint(points)\n\n# Also, we modified the first point via the list.\n# What do you think `point_a` variable now contains?\nprint(point_a)\n\n[[0, 6], [1, 2]]\n[10, 2]\n[[0, 6], [10, 2]]\n[0, 6]\n\n\nDid the last result surprise you? When we modified point_b, the change was reflected in our points list too! This happens because Python doesn’t actually store the values directly in the list – instead, it stores references (think of them as pointers) to the data. It’s like having a directory of addresses rather than copies of the actual data.\nUnderstanding this behavior is important because it means changes to your data in one place can unexpectedly affect the same data being used elsewhere in your code.\nWith this in mind, we can now update our mental model and make it a bit more accurate. This time, the items in the lists are references that “point” to the actual items we care about.\n  \"A\"   \"T\"   \"G\"   \"A\"   \"C\"    (items \"in\" the list are objects)\n   ↑     ↑     ↑     ↑     ↑\n┌──┴──┬──┴──┬──┴──┬──┴──┬──┴──┐\n│  ✦  │  ✦  │  ✦  │  ✦  │  ✦  │  (values in the list are references)\n└─────┴─────┴─────┴─────┴─────┘\n   0     1     2     3     4     (indices of the references)\nThe diagram for the points example might look something like this:\n    0     3       1     2      (items \"in\" the list are numbers)\n    ↑     ↑       ↑     ↑\n ┌──┴──┬──┴──┐ ┌──┴──┬──┴──┐\n │  ✦  │  ✦  │ │  ✦  │  ✦  │   (each element in `points` is also a list)\n └──┬──┴──┬──┘ └──┬──┴──┬──┘\n    ↑     ↑       ↑     ↑\n    └──┬──┘       └──┬──┘\n┌──────┴──────┬──────┴──────┐\n│      ✦      │      ✦      │  (the first level is the `points` list)\n└─────────────┴─────────────┘\nFor now, don’t get too hung up on the lower-level details – just be aware of the practical implications mentioned above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#loops",
    "href": "collections.html#loops",
    "title": "2  Collections",
    "section": "Loops",
    "text": "Loops\nSo far, we’ve worked with two types of collections: lists and strings. But what if you want to work with each element in these collections one at a time? That’s where loops come in!\nLoops give you a way to automate repetitive tasks. Instead of copying and pasting the same code multiple times to process each item in a list (which would be both tedious and error-prone), loops let you write the instructions once and apply them to every item automatically.\nFor example, if you had a list of gene sequences and wanted to check each one for a particular pattern, you wouldn’t want to write separate code for each sequence. A loop would let you perform this check systematically across your entire dataset.\nPython offers several different types of loops, each suited for particular situations. In this section we will focus on for loops and while loops.\n\nFor Loops\nA for loop processes each item in a sequence, one at a time. Think of it like going through a list and looking at each item one at a time:\n\nfor letter in [\"D\", \"N\", \"A\"]:\n    print(letter)\n\nD\nN\nA\n\n\nLet’s break that down:\n\nfor – tells Python we want to start a loop\nletter – a variable that will hold each item\nin [\"D\", \"N\", \"A\"] – tells Python to loop through the list [\"D\", \"N\", \"A\"]\n: – marks the beginning of the code block to be executed\nThe indented code (print(letter)) runs once for each item\n\nNote that for and in are specifically required in for loop syntax. letter and [\"D\", \"N\", \"A\"] will change depending on the context.\nFor example, this loop has the same behavior as the previous loop:\n\nletters = [\"D\", \"N\", \"A\"]\nfor the_letter in letters:\n    print(the_letter)\n\nD\nN\nA\n\n\nThis time, we used a different variable name to store the items of the collection, and rather than putting the collection directly in the for ... in ... : part, we referred to the collection using a variable.\nIn addition to lists, for loops also work on strings:\n\nnucleotides = \"ATCG\"\nfor nucleotide in nucleotides:\n    print(f\"The nucleotide was '{nucleotide}'\")\n\nThe nucleotide was 'A'\nThe nucleotide was 'T'\nThe nucleotide was 'C'\nThe nucleotide was 'G'\n\n\nYou can actually use for loops on lots of different Python data structures: as long as it is iterable, then you can use a for loop with it.\nOften you will want to take some action multiple times. For this, we can use range:\n\nfor number in range(5):\n    print(number)\n\n0\n1\n2\n3\n4\n\n\nThis should have printed 5 numbers: 0, 1, 2, 3, 4. Here is Python counting from zero again!\nYou can also tell range where to start and stop:\n\n# Count from 1 to 5\nfor number in range(1, 6):\n    print(number)\n\n1\n2\n3\n4\n5\n\n\nHere is a neat thing you can do with ranges. Before running the code, could you guess what it might do?\n\nfor i in range(2, 10, 2):\n    print(i)\n\n2\n4\n6\n8\n\n\nLet’s break down what’s happening with range here. While we’ve seen range create simple sequences of numbers before, it can actually take up to three arguments: range(start, stop, step). The step tells Python how many numbers to count by each time.\nIt’s like counting: normally we count “1, 2, 3, 4…” (step of 1), but sometimes we count “2, 4, 6, 8…” (step of 2). In this example, we’re using a step of 2 to skip every other number.\nThe start and step arguments are optional – you can just use range(stop) if you want to count normally starting from zero. If you’re curious about more advanced uses, like counting backwards or working with negative numbers, check out the Python range docs for more details.\nRanges are memory efficient – they don’t store all the numbers in the range in memory. This is important when generating large batches of numbers.\n(This code shouldn’t be run. It’s just here to illustrate the point.)\nbig_range = range(1, 1000000)  # Takes very little memory\nbig_list = list(big_range)     # Takes much more memory!\n\nNested For Loops\nOne feature of for loops is that you can put one inside another – something we call “nesting”. Think of it like those Russian nesting dolls, where each doll contains a smaller one inside.\n\nfor i in range(2):\n    for j in range(3):\n        print(f\"i: {i}; j: {j}\")\n\ni: 0; j: 0\ni: 0; j: 1\ni: 0; j: 2\ni: 1; j: 0\ni: 1; j: 1\ni: 1; j: 2\n\n\nLet’s break down what’s happening here. The outer loop (using i) runs two times (0, 1), and for each of those times, the inner loop (using j) runs three times (0, 1, 2). It’s a bit like having a set of drawers where you check each drawer (outer loop), and within each drawer, you look at every item inside (inner loop).\nWhen you run the above code, you’ll see each combination of i and j printed out, showing how the loops work together. This pattern of nested loops is incredibly useful when you need to process data that has multiple levels or dimensions, for example, like comparing every gene in one dataset to every gene in another dataset.\nHere is a schematic view:\n┌──────────────────────────────────────────────┐\n│ i=0                                          │\n│ ┌────────────┐ ┌────────────┐ ┌────────────┐ │\n│ │ j=0        │ │ j=1        │ │ j=2        │ │\n│ │            │ │            │ │            │ │\n│ │ print(...) │ │ print(...) │ │ print(...) │ │\n│ └────────────┘ └────────────┘ └────────────┘ │\n└──────────────────────────────────────────────┘\n\n┌──────────────────────────────────────────────┐\n│ i=1                                          │\n│ ┌────────────┐ ┌────────────┐ ┌────────────┐ │\n│ │ j=0        │ │ j=1        │ │ j=2        │ │\n│ │            │ │            │ │            │ │\n│ │ print(...) │ │ print(...) │ │ print(...) │ │\n│ └────────────┘ └────────────┘ └────────────┘ │\n└──────────────────────────────────────────────┘\nYou can have more than two levels of nesting. For example:\n\nfor i in range(2):\n    for j in range(3):\n        for k in range(4):\n            print(f\"i: {i}; j: {j}; k: {k}\")\n\ni: 0; j: 0; k: 0\ni: 0; j: 0; k: 1\ni: 0; j: 0; k: 2\ni: 0; j: 0; k: 3\ni: 0; j: 1; k: 0\ni: 0; j: 1; k: 1\ni: 0; j: 1; k: 2\ni: 0; j: 1; k: 3\ni: 0; j: 2; k: 0\ni: 0; j: 2; k: 1\ni: 0; j: 2; k: 2\ni: 0; j: 2; k: 3\ni: 1; j: 0; k: 0\ni: 1; j: 0; k: 1\ni: 1; j: 0; k: 2\ni: 1; j: 0; k: 3\ni: 1; j: 1; k: 0\ni: 1; j: 1; k: 1\ni: 1; j: 1; k: 2\ni: 1; j: 1; k: 3\ni: 1; j: 2; k: 0\ni: 1; j: 2; k: 1\ni: 1; j: 2; k: 2\ni: 1; j: 2; k: 3\n\n\nThough I bet you know what’s going on with nested loops by now, let’s break it down anyway. The innermost loop (k) completes all its iterations before the middle loop (j) counts up once, and the middle loop completes all its iterations before the outer loop (i) counts up once. In this example, for each value of i, we’ll go through all values of j, and for each of those, we’ll go through all values of k.\nRemember that each additional level of nesting multiplies the number of iterations. In our example, we have 2 × 3 × 4 = 24 total iterations. Keep this in mind when working with larger datasets.\n\n\nEnumerated for Loops\nSometimes when you’re working with a sequence, you need to know not just what each item is, but also where it appears. That’s where Python’s handy enumerate function comes in. It lets you track both the position (index) and the value of each item as you loop through them.\nHere’s a simple example:\n\nfor index, letter in enumerate(\"ABCDE\"):\n    print(f\"index: {index}; letter: {letter}\")\n\nindex: 0; letter: A\nindex: 1; letter: B\nindex: 2; letter: C\nindex: 3; letter: D\nindex: 4; letter: E\n\n\nThis will show you each letter along with its position in the sequence, starting from 0 (remember, Python always starts counting at 0!).\nBy the way, you can also use enumerate outside of loops. For instance, if you have a list of nucleotides:\n\nnucleotides = [\"A\", \"C\", \"T\", \"G\"]\nenumerated_nucleotides = enumerate(nucleotides)\nprint(list(enumerated_nucleotides))\n\n[(0, 'A'), (1, 'C'), (2, 'T'), (3, 'G')]\n\n\nThis creates pairs of positions and values, which can be useful, say, when you need to track where certain elements appear in your sequence data.\n\n\n\nWhile Loops\nWhile loops keep repeating until the given condition is not true (or truthy). Let’s look at a simple example that counts from 1 to 5:\n\ncount = 1\nwhile count &lt;= 5:\n    print(count)\n    count += 1\n\n1\n2\n3\n4\n5\n\n\nTo understand what this loop does, imagine it following a simple set of instructions:\n\nCreate a variable called count and set it to 1.\nThen, keep doing these steps as long as count is less than or equal to 5:\n\nDisplay the current value of count\nAdd 1 to count.\n\n\nThe loop will keep running until count becomes 6, at which point the condition count &lt;= 5 becomes false, and the loop stops.\nJust to make it super clear, let’s write out the steps:\n\ncount = 1: is count &lt;= 5? Yes! prints 1, then adds 1\ncount = 2: is count &lt;= 5? Yes! prints 2, then adds 1\ncount = 3: is count &lt;= 5? Yes! prints 3, then adds 1\ncount = 4: is count &lt;= 5? Yes! prints 4, then adds 1\ncount = 5: is count &lt;= 5? Yes! prints 5, then adds 1\ncount = 6: is count &lt;= 5? No! stops because 6 is not &lt;= 5\n\n\nInfinite Loops and Other Problems\nWhen working with while loops, it’s crucial to ensure your loop has a way to end. Think of it like setting up an automated process – you need a clear stopping point, or the process will run forever!\nThere are two common pitfalls to watch out for:\n\nIf your condition is never true to begin with, the loop won’t run at all\nIf your condition can never become false, the loop will run forever (called an infinite loop)\n\nHere’s an example of the 2nd problem. Can you figure out why this code would run forever?\n# Infinite loop -- DO NOT RUN!!\ncount = 1\nwhile count &gt;= 0:\n    print(count)\n    count = count + 1\nLet’s think through what’s happening:\n\nWe start with count = 1\nThe loop continues as long as count is greater than or equal to 0\nEach time through the loop, we’re adding 1 to count\nSo count keeps getting bigger: 1, 2, 3, 4, 5…\nBut wait! A number that keeps getting bigger will always be greater than 0\nThis means our condition (count &gt;= 0) will always be true, and the loop will never end!\n\nWhen writing your own loops, always be sure that your condition will eventually become false – you need a clear endpoint!\n\n\n\nModifying a List While Looping\nOne tricky aspect of using loops in Python occurs if you try to modify a collection while looping over it.\nWith a while loop and the pop method, it’s not too weird – you run the while loop until the list is empty:\n\n# Starting with a list of tasks\ntodo_list = [\"task1\", \"task2\", \"task3\"]\n\nwhile todo_list:  # This is true as long as the list has items\n    current_task = todo_list.pop()  # removes and returns last item\n    print(f\"Doing task: {current_task}\")\n\nprint(\"All tasks complete!\")\nprint(todo_list)\n\nDoing task: task3\nDoing task: task2\nDoing task: task1\nAll tasks complete!\n[]\n\n\nHowever, things can get quite weird with for loops:\n\n# This is probably not what you want!\nnumbers = [1, 2, 3, 4, 5]\nfor number in numbers:\n    numbers.remove(number)  # Don't do this!\nprint(numbers)\n\n[2, 4]\n\n\nUnfortunately, that did not remove all the items from numbers like you may have expected.\nOne way to address this issue is to use [:] to create a copy of numbers and iterate over that collection. Meanwhile, you remove items from the original numbers.\n\nnumbers = [1, 2, 3, 4, 5]\nfor number in numbers[:]:  # The [:] creates a copy\n    numbers.remove(number)\n    print(f\"Removed {number}. List is now: {numbers}\")\nprint(f\"at the end: {numbers}\")\n\nRemoved 1. List is now: [2, 3, 4, 5]\nRemoved 2. List is now: [3, 4, 5]\nRemoved 3. List is now: [4, 5]\nRemoved 4. List is now: [5]\nRemoved 5. List is now: []\nat the end: []\n\n\nReally, this example is pretty artificial – you wouldn’t be trying to delete every item in a list with a for loop anyway. Just be aware that if you modify a collection during a loop, special care must be taken to ensure that you don’t mess things up.\nTake note of this for miniproject 1 – you will “probably” have to remove some items from a list to complete it! But don’t worry, you will see some more examples in the project description….\n\n\nComprehensions\nWhile we are on the topic of loops, let’s discuss one more thing: Comprehensions.\nComprehensions let you create new lists (and other collections) from existing lists (and other collections).\nLet’s say that you want to create a list of RNA bases and you’ve already made a list of DNA bases. One way to do this would be to take your existing list and convert any Thymines (T) to Uracils (U). We can do this with a traditional for loop:\n\n# Using traditional loop\ndna = [\"A\", \"T\", \"G\", \"C\"]\nrna = []\nfor base in dna:\n    if base != \"T\":\n        rna.append(base)\n    else:\n        rna.append(\"U\")\n\nprint(rna)\n\n['A', 'U', 'G', 'C']\n\n\nOr with a comprehension:\n\ndna = \"ATGC\"\nrna = [\"U\" if base == \"T\" else base for base in dna]\nprint(rna)\n\n['A', 'U', 'G', 'C']\n\n\nThe comprehension is much more concise! The list comprehension is doing everything that the traditional for loop is doing, but in a single line.\nThe basic structure of a comprehension can be broken down into these components:\nnew_list = [expression for item in iterable if condition]\nBreaking it down:\n\nnew_list: The resulting list\nexpression: What you want to do with each item (like transform it)\nfor item in iterable: The loop over some iterable object\nif condition: Optional filter (you can leave this out)\n\nNote that in our original example, the if condition actually came before the for loop part – that’s allowed!\nComprehensions are definitely weird at first! Let’s look at some more examples.\nHere is a basic example using range instead of an existing list:\n\nsquares = [x**2 for x in range(5)]\nprint(squares)\n\n# Same as:\nsquares = []\nfor x in range(5):\n    squares.append(x**2)\n\nprint(squares)\n\n[0, 1, 4, 9, 16]\n[0, 1, 4, 9, 16]\n\n\nThis example takes each number in the list produced by range(5), squares it, and adds it to the new list squares. In this case:\n\nsquares is the new_list\nx**2 is the expression\nx is the item and range(5) is the iterable\nThere is no if condition\n\nNotice that you don’t have to initialize an empty list for the comprehension to work – it makes the list itself, unlike with a for loop.\nLet’s look at an example with a condition:\n\n# Using comprehension\nexpressions = [1.2, 0.5, 3.4, 0.1, 2.2]\nhigh_expression = [x for x in expressions if x &gt; 2.0]\nprint(high_expression)\n\n# Using a for loop\nexpressions = [1.2, 0.5, 3.4, 0.1, 2.2]\nhigh_expression = []\nfor x in expressions:\n    if x &gt; 2.0:\n        high_expression.append(x)\nprint(high_expression)\n\n[3.4, 2.2]\n[3.4, 2.2]\n\n\nIn this example, we take an existing list, expressions, and make a new list, high_expressions, that contains only the expressions that are 2.0 or greater.\nNotice that in this example, there is nothing done to the existing items in the list before adding them to the new one, which is why the comprehension starts with x for x.\nComprehensions can also be used to create dictionaries. Check this out:\n\nsquares = {x: x**2 for x in range(5)}\nprint(squares)\n\neven_squares = {x: x**2 for x in range(5) if x % 2 == 0}\nprint(even_squares)\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n{0: 0, 2: 4, 4: 16}\n\n\nThat is pretty neat right?\nWhile comprehensions are compact, whether or not you think this conciseness leads to better code is a different story. As you gain more experience, you will get a better feel for such things. Whether you use them a lot or a little, you should be aware of them as they are quite common in Python codebases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#tuples",
    "href": "collections.html#tuples",
    "title": "2  Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are like lists that can’t be changed – perfect for storing fixed information. We often use tuples when we want to ensure data integrity or represent relationships that shouldn’t change.\n\nCreating Tuples\nTuples have a dedicated syntax used for construction:\n\nletters = (\"a\", \"b\", \"c\")\n\n# Single item tuples still need a comma!\nnumber = (1, )\n\nThe syntax for creating a tuple is not that different from creating a list, but you’ll notice differences when trying to alter their components. For example, the following code would raise an error if we didn’t put the try/except around it:\n\nletters = (\"a\", \"b\", \"c\")\n\ntry:\n    letters[0] = \"d\"\nexcept TypeError:\n    print(\"you can't assign to a tuple\")\n\nyou can't assign to a tuple\n\n\nIf letters was a list, the above code would change the list to start with d instead of b. Instead, we got an error. This is because tuples are immutable.\nWhether some data is mutable or immutable determines whether you can modify it or not. Here is a silly metaphor to illustrate what I mean:\n\nMutable collections (like lists and dictionaries) are like erasable whiteboards – you can add, remove, or change items whenever you need to\nImmutable collections (like tuples) are more like carved stone tablets – once created, their contents are “set in stone”\n\nWhy does this matter? Here are two practical implications:\n\nData Safety: Immutable collections help prevent accidental changes to important data\n\nRemember how we could modify individual coordinates in our list earlier? If we had used tuples instead, Python would have prevented any accidental modifications\n\nTechnical Requirements: Some Python features, like using values as dictionary keys (which we’ll explore soon), only work with immutable data types\n\nTuples excel at representing fixed relationships between values that logically belong together. Think of them as a way to package related information that you know shouldn’t change during your program’s execution.\nE.g., our coordinates example from above could be better written with a tuple:\n\n# (x, y)\npoint = (1, 2)\nprint(point)\n\n(1, 2)\n\n\nOr, you could represent facts about a codon as a tuple:\n\nmethionine = (\"Methionine\", \"Met\", \"M\", \"ATG\")\nprint(methionine)\n\n('Methionine', 'Met', 'M', 'ATG')\n\n\nOr, you could represent related gene information:\n\ngene_info = (\"BRCA1\",     # gene name\n             \"chr17\",     # chromosome\n             43044295,    # start position\n             43125364,    # end position\n             \"plus\")      # strand\nprint(gene_info)\n\n('BRCA1', 'chr17', 43044295, 43125364, 'plus')\n\n\n\n\nTuple Packing and Unpacking\nLet’s look at two really useful Python features that make working with multiple values easier: tuple packing and unpacking.\nTuple packing is pretty straightforward – Python can automatically bundle multiple values into a tuple for you. Here’s an example using a codon and its properties:\n\n# Packing values into a tuple\ncodon = \"AUG\", \"Methionine\", \"Start\"\nprint(codon)\n\n('AUG', 'Methionine', 'Start')\n\n\nThe opposite operation, tuple unpacking, lets you smoothly assign tuple elements to separate variables:\n\n# Unpacking a tuple into individual variables\ncodon = (\"AUG\", \"Methionine\", \"Start\")\nsequence, amino_acid, role = codon\n\nprint(f\"Codon: {sequence}; Amino Acid: {amino_acid}; Role: {role}\")\n\nCodon: AUG; Amino Acid: Methionine; Role: Start\n\n\nOne of the coolest applications of packing and unpacking is swapping values between variables. Check this out:\n\n# Set initial values\nx, y = 1, 2\n\n# Print the original values\nprint(f\"x: {x}; y: {y}\")\n\n# Swap values in one clean line\nx, y = y, x\n\n# Print the swapped values\nprint(f\"x: {x}; y: {y}\")\n\nx: 1; y: 2\nx: 2; y: 1\n\n\nTo appreciate how nice this is, here’s how you’d typically swap values in many other programming languages:\n\nx = 1\ny = 2\n\n# Print the original values\nprint(f\"x: {x}; y: {y}\")\n\n# The traditional way requires a temporary variable\ntmp = y\ny = x\nx = tmp\n\n# Print the swapped values\nprint(f\"x: {x}; y: {y}\")\n\nx: 1; y: 2\nx: 2; y: 1\n\n\nPython’s packing and unpacking syntax makes this common operation more intuitive and readable. Instead of juggling a temporary variable, you can swap values in a single, clear line of code. This is just one example of how Python’s design choices can make your code both simpler to write and easier to understand.\n\n\nNamed Tuples\nYou may be thinking that it could get tricky to remember which field of a tuple is which. Named tuples provide a great way to address this. They’re like regular tuples, but with the added benefit of letting you create them and access data using descriptive names instead of index numbers.\nLet’s see how they work:\n\n# We need to import namedtuple from the collections module\nfrom collections import namedtuple\n\n# Create a Gene type with labeled fields\n# (note the name is Gene and not gene)\nGene = namedtuple(\"Gene\", \"name chromosome start stop\")\n\n# Create a specific gene entry\n#\n# Using named arguments can keep you from mixing up the arguments!\ntp53 = Gene(\n    name=\"TP53\",\n    chromosome=\"chr17\",\n    start=7_571_720,\n    stop=7_590_868,\n)\n\n# Access the data using meaningful names\nprint(tp53.name)\nprint(tp53.chromosome)\n\n# You can still unpack it like a regular tuple if you want\nname, chromosome, start, stop = tp53\nprint(name, chromosome, start, stop)\n\nTP53\nchr17\nTP53 chr17 7571720 7590868\n\n\nWhat makes named tuples great?\n\nThey’re clear and self-documenting – the labels tell you exactly what each value means\nThey’re less prone to errors – no more mixing up whether position 2 was start or stop\nThey’re efficient and unchangeable (immutable), just like regular tuples\n\nFor example, you can’t change values after creation:\n\ntry:\n    tp53.start = 1300  # This will raise an error\nexcept AttributeError:\n    print(\"you can't do this!\")\n\nyou can't do this!\n\n\nNamed tuples are perfect for representing any kind of structured data. Here’s another example using DNA sequences:\n\nSequence = namedtuple(\"Sequence\", \"id dna length gc_content\")\n\n# Create some sequence records\nseq1 = Sequence(\"SEQ1\", \"GGCTAA\", length=6, gc_content=0.5)\nseq2 = Sequence(\"SEQ2\", \"GGTTAA\", length=6, gc_content=0.33)\n\n# Named tuples print out nicely too\nprint(seq1)  # Shows all fields with their values\nprint(seq2)\n\nSequence(id='SEQ1', dna='GGCTAA', length=6, gc_content=0.5)\nSequence(id='SEQ2', dna='GGTTAA', length=6, gc_content=0.33)\n\n\nI have mentioned a few times now that tuples are immutable, and named tuples are as well. There is a way to get an modified copy of a named tuple however:\n\nseq1 = Sequence(\"SEQ1\", \"GGCTAA\", length=6, gc_content=0.5)\n\nseq1_with_new_id = seq1._replace(id=\"sequence 1\")\n\n# The original seq1 is unchanged:\nprint(seq1)\n\n# The new one has the same values as the original other than the id\nprint(seq1_with_new_id)\n\nSequence(id='SEQ1', dna='GGCTAA', length=6, gc_content=0.5)\nSequence(id='sequence 1', dna='GGCTAA', length=6, gc_content=0.5)\n\n\nThe bottom line: When you need to bundle related data together, named tuples are often a great choice. They’re essentially as lightweight as regular tuples, but they make your code much easier to read and maintain. Think of them as regular tuples with the added bonus of built-in documentation!\n\n\nWhen to Use Tuples vs. Lists\nIt may still be unclear when to choose tuples rather than lists. While you will get a feel for it over time, here are some guidelines that can help you choose:\nChoose a Tuple When:\n\nYour data represents an inherent relationship that won’t change (like a DNA sequence’s start and end coordinates)\nYou want to make sure your data stays protected from accidental modifications\nYou need to use the data as a dictionary key (we’ll explore this more soon)\nYou’re returning multiple related values from a function\n\nChoose a List When:\n\nYou’ll need to add or remove items as your program runs\nYour data needs to be flexible and modifiable\nYou’re accumulating or building up data throughout your program\n\nOne way to think of it is: if you’re working with data that should remain constant, reach for a tuple. If you need something more flexible that can grow or change (like collecting results), a list is your better choice.\nHere is a nice section of the Python docs if you want to dive deeper: Why are there separate tuple and list data types?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#dictionaries",
    "href": "collections.html#dictionaries",
    "title": "2  Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries in Python are a bit like address books. Just as you can look up someone’s phone number using their name, dictionaries let you pair up pieces of information so you can easily find one when you know the other. The first part (like the person’s name) is called the key, and it leads you to the second part (like their phone number), which is called the value.\nLet’s say you want to keep track of gene names and their functions. Instead of scanning through a long list every time, a dictionary lets you jump straight to the function just by knowing the gene name. They are a great way to organize and retrieve your data quickly.\n\nCreating Dictionaries\n\nDictionary Literals ({})\nThe most straightforward way to create dictionaries is using curly brackets {} with key: value pairs:\n\ncodon_table = {\n    \"AUG\": \"Met\",\n    \"UAA\": \"Stop\",\n    \"UAG\": \"Stop\",\n    \"UGA\": \"Stop\"\n}\n\nprint(codon_table)\n\n{'AUG': 'Met', 'UAA': 'Stop', 'UAG': 'Stop', 'UGA': 'Stop'}\n\n\n\n\ndict Function\nYou can also create dictionaries using the dict() function, which is particularly nice when you have simple string keys:\n\ngene = dict(gene=\"nrdA\", product=\"ribonucleotide reductase\")\nprint(gene)\n\n{'gene': 'nrdA', 'product': 'ribonucleotide reductase'}\n\n\n\n\ndict + zip\nHere’s a handy trick: if you have two separate lists that you want to pair up into a dictionary, you can use zip with dict:\n\ngenes = [\"TP53\", \"BRCA1\", \"KRAS\"]\nfunctions = [\"tumor suppressor\", \"DNA repair\", \"signal transduction\"]\n\ngene_functions = dict(zip(genes, functions))\n\nprint(gene_functions)\n\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'KRAS': 'signal transduction'}\n\n\nThe order matters when using zip – the first list provides the keys, and the second list provides the values:\n\n# Switching the order gives us a different dictionary\nmysterious_dictionary = dict(zip(functions, genes))\nprint(mysterious_dictionary)\n\n{'tumor suppressor': 'TP53', 'DNA repair': 'BRCA1', 'signal transduction': 'KRAS'}\n\n\n\n\nOne Entry at a Time\nYou can also built up dictionaries one value at a time. Here’s a common real-world scenario: you’re reading data from a file and need to build a dictionary as you go.\nFor this example, imagine that lines came from parsing a file rather than being hardcoded.\n\n# This could be data from a file\nlines = [\n    [\"TP53\", \"tumor suppressor\"],\n    [\"BRCA1\", \"DNA repair\"],\n    [\"KRAS\", \"signal transduction\"],\n]\n\n# Start with an empty dictionary\ngene_functions = {}\n\n# Add each item to the dictionary\nfor gene_name, function in lines:\n    gene_functions[gene_name] = function\n\nprint(gene_functions)\n\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'KRAS': 'signal transduction'}\n\n\nThis pattern of building a dictionary piece by piece is something you’ll use frequently when working with real data. It’s especially useful when processing files or API responses where you don’t know the contents ahead of time.\n\n\nDuplicate Keys & Values\nA few important things to know about dictionaries:\n\nValues can be repeated (the same value can appear multiple times)\nKeys must be unique (if you try to use the same key twice, only the last value will be kept)\n\nHere’s an example showing both of these properties:\n\n# Values can be repeated\nprint(dict(a=\"apple\", b=\"banana\", c=\"apple\"))\n\n# Only the last value for a repeated key is kept\ncodons = {\n    \"AUG\": \"Met\",\n    \"UAA\": \"Stop\",\n    \"UAG\": \"Stop\",\n    \"UGA\": \"Stop\",\n    \"AUG\": \"Methionine\",  # This will override the first AUG entry\n}\nprint(codons)\n\n{'a': 'apple', 'b': 'banana', 'c': 'apple'}\n{'AUG': 'Methionine', 'UAA': 'Stop', 'UAG': 'Stop', 'UGA': 'Stop'}\n\n\n\n\n\nWorking with Dictionaries: Getting, Adding, and Removing Items\nLet’s see the basics of working with dictionaries in Python. We’ll continue with our gene_functions dictionary from earlier:\n\ngenes = [\"TP53\", \"BRCA1\", \"KRAS\"]\nfunctions = [\"tumor suppressor\", \"DNA repair\", \"signal transduction\"]\ngene_functions = dict(zip(genes, functions))\nprint(gene_functions)\n\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'KRAS': 'signal transduction'}\n\n\n\nGetting Items from a Dictionary\nThe most basic way to look up information in a dictionary is similar to how you’d look up a word in a real dictionary: you use the key to find the value. In Python, this means using square brackets:\n\n# Looking up a value\np53_function = gene_functions[\"TP53\"]\nprint(p53_function)\n\ntumor suppressor\n\n\nTrying to find a key that doesn’t exist will cause an error. (Again, we wrap the code that will cause an error in a try/except block so that it doesn’t break our notebook code.)\n\ntry:\n    gene_functions[\"apple pie\"]\nexcept KeyError:\n    print(\"there is no gene called 'apple pie'\")\n\nthere is no gene called 'apple pie'\n\n\nThere is an alternative way to get info from a dictionary that will not raise an error if the key you’re searching for is not found: get.\n\n# This will return `None` rather than raise an error\n# if the key is not found\nresult = gene_functions.get(\"BRCA2\")\nprint(result)\n\n# This will return the value \"Unknown\"\n# if the key is not found\nresult = gene_functions.get(\"BRCA2\", \"Unknown\")\nprint(result)\n\nNone\nUnknown\n\n\n\n\nAdding Items to a Dictionary\nWe mentioned that dictionaries are mutable. Let’s see how to add items to our dictionary. You can either add items one at a time or several at once:\n\n# Adding a single new entry\ngene_functions[\"EGFR\"] = \"growth signaling\"\nprint(gene_functions)\n\n# Adding multiple entries at once\ngene_functions.update({\n    \"MDM2\": \"p53 regulation\",\n    \"BCL2\": \"apoptosis regulation\"\n})\nprint(gene_functions)\n\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'KRAS': 'signal transduction', 'EGFR': 'growth signaling'}\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'KRAS': 'signal transduction', 'EGFR': 'growth signaling', 'MDM2': 'p53 regulation', 'BCL2': 'apoptosis regulation'}\n\n\nYou can get a bit fancy with updating dictionaries if you want by using operators:\n\nletters_and_numbers = dict(a=1, b=2) | dict(a=10, c=30)\nprint(letters_and_numbers)\n\nletters_and_numbers |= dict(d=400, e=500)\nprint(letters_and_numbers)\n\n{'a': 10, 'b': 2, 'c': 30}\n{'a': 10, 'b': 2, 'c': 30, 'd': 400, 'e': 500}\n\n\nWhen you’re learning to code, it’s best to stick with straightforward, easy-to-read solutions. While Python offers some fancy shortcuts (like complex operators), you’ll usually want to write code that you and others can easily understand later. Simple and longer is often better than shorter and clever!\nHere’s an interesting feature of Python dictionaries that you might have noticed: when you print out a dictionary, the items appear in the exact order you added them. This wasn’t always true in older versions of Python, but now dictionaries automatically keep track of the order of your entries.\nOne final thing to mention. You can’t use every Python type as a dictionary key, only immutable types. E.g., you couldn’t use a list as a key for a dictionary. The specific reason for that is beyond the scope of this tutorial, but you may be interested in reading more about it here: Why must dictionary keys be immutable?\n\n\nRemoving Items from a Dictionary\nNeed to remove something from your dictionary? Here are two options:\n\n# Remove an entry with del.\n#\n# del will raise an error if the key is not present\ntry:\n    del gene_functions[\"KRAS\"]\nexcept KeyError:\n    print(\"KRAS was not present in the dictionary\")\nprint(gene_functions)\n\n# Remove and save the value with pop()\n#\n# We add the \"Unknown\" to the call to pop so that our program\n# will still run if the key is not present.\nremoved_gene = gene_functions.pop(\"EGFR\", \"Unknown\")\nprint(f\"Removed function: {removed_gene}\")\nprint(gene_functions)\n\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'EGFR': 'growth signaling', 'MDM2': 'p53 regulation', 'BCL2': 'apoptosis regulation'}\nRemoved function: growth signaling\n{'TP53': 'tumor suppressor', 'BRCA1': 'DNA repair', 'MDM2': 'p53 regulation', 'BCL2': 'apoptosis regulation'}\n\n\nThe del statement is probably the more common way to remove an item from a dictionary.\nNote that if you run that code block more than one time, you will get different outputs. Can you think of why that would be?\nBy the way…before working with a key, it’s often wise to first check if it exists:\n\nif \"TP53\" in gene_functions:\n    print(\"Found TP53's function!\")\n    function = gene_functions[\"TP53\"]\nelse:\n    print(\"TP53 not found in our dictionary\")\n\nFound TP53's function!\n\n\nThis same technique is a good idea before using del as well, since del will give you an error if you try to delete the value of a key that is not present in the dictionary.\n\nif \"TP53\" in gene_functions:\n    del gene_functions[\"TP53\"]\n    print(gene_functions)\nelse:\n    print(\"TP53 not found in our dictionary\")\n\n{'BRCA1': 'DNA repair', 'MDM2': 'p53 regulation', 'BCL2': 'apoptosis regulation'}\n\n\nNote the use of the in operator. It is for membership testing and also works with dictionaries.\n\n\nExample: Creating the Reverse Complement of a DNA Sequence\nLet’s tackle a common task in DNA sequence analysis: generating a reverse complement. If you’ve worked with DNA before, you know that A pairs with T, and C pairs with G.\nFirst, we’ll create a dictionary that maps each nucleotide to its complement:\n\ncomplement = {\"A\": \"T\", \"T\": \"A\", \"G\": \"C\", \"C\": \"G\"}\nprint(complement)\n\n{'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G'}\n\n\nThen, we’ll take a simple DNA sequence to demonstrate:\n\ndna_sequence = \"AACCTTGG\"\n\nFinally, we’ll loop through the sequence backwards (that’s what reversed(...) does) and look the complement of each nucleotide:\n\nfor nucleotide in reversed(dna_sequence):\n    print(complement[nucleotide], end=\"\")\n\nCCAAGGTT\n\n\n(The end=\"\" parameter tells Python not to add newlines between letters, giving us one continuous sequence.)\n\n\n\nNested Dictionaries: Organizing Complex Data\nWhile simple dictionaries work well for simple mappings like mapping the name of a gene to its function, biological data often has multiple layers of related information.\nLet’s look at one way we can organize this richer data using nested dictionaries – dictionaries that themselves contain other dictionaries or lists. (Remember how we could nest lists in other lists? This is similar!)\nHere’s an example showing how we might store information about the TP53 gene:\n\n# Gene information database\n#\n# Imagine there are more genes in here too....\ngene_database = {\n    \"TP53\": {\n        \"full_name\": \"Tumor Protein P53\",\n        \"chromosome\": \"17\",\n        \"position\": {\"start\": 7_571_720, \"end\": 7_590_868},\n        \"aliases\": [\"p53\", \"TRP53\"],\n    }\n}\nprint(gene_database)\n\n{'TP53': {'full_name': 'Tumor Protein P53', 'chromosome': '17', 'position': {'start': 7571720, 'end': 7590868}, 'aliases': ['p53', 'TRP53']}}\n\n\nLet’s use the filing cabinet metaphor again: the main drawer is labeled “TP53”, and inside that drawer are several folders containing different types of information. Some of these folders (like “position”) contain their own sub-folders! (Alright, it’s not the greatest metaphor…but hopefully you get the idea!)\nLet’s break down what we’re storing:\n\nBasic information: The full name and chromosome location\nPosition data: Both start and end coordinates on the chromosome\nAlternative names: A list of other common names for the gene\n\nTo access this information, we use square brackets to “drill down” through the layers. Each set of brackets takes us one level deeper:\n\n# Get the full name\ngene_name = gene_database[\"TP53\"][\"full_name\"]\nprint(gene_name)\n\n# Get the start position\nstart_position = gene_database[\"TP53\"][\"position\"][\"start\"]\nprint(start_position)\n\n# Get the first alias\nfirst_alias = gene_database[\"TP53\"][\"aliases\"][0]\nprint(first_alias)\n\nTumor Protein P53\n7571720\np53\n\n\nIt’s pretty similar to nested lists, right?\n\nHandling Missing Data in Nested Dictionaries\nWith nested dictionaries, accessing missing data requires extra care to avoid errors. Let’s see why:\n\n# Trying to access data that doesn't exist\ntry:\n    # Attempting to access methylation data that isn't stored\n    methylation = gene_database[\"TP53\"][\"methylation\"][\"site\"]\nexcept KeyError as error:\n    print(f\"Oops! That data isn't available: {error}\")\n\nOops! That data isn't available: 'methylation'\n\n\nThis code will raise a KeyError because we’re trying to access a key (“methylation”) that doesn’t exist. When dealing with nested structures, it’s particularly important to handle these cases because an error could occur at any level of nesting.\nHere is what happens if we try and access a key that doesn’t exist in the position map:\n\ntry:\n    middle_position = gene_database[\"TP53\"][\"position\"][\"middle\"]\nexcept KeyError as error:\n    print(f\"Oops! That data isn't available: {error}\")\n\nOops! That data isn't available: 'middle'\n\n\nAs you see, this approach will work for missing keys at different levels of nesting.\nOne thing to be aware of if you are mixing lists and dictionaries is that while “drilling down” into the data structure you could potentially get errors other than KeyError:\n\ntry:\n    an_alias = gene_database[\"TP53\"][\"aliases\"][10]\nexcept IndexError as error:\n    print(f\"Oops! That data isn't available: {error}\")\n\nOops! That data isn't available: list index out of range\n\n\nIn this case, we need to handle the IndexError because the data that the aliases key points to is a list, but that list doesn’t have enough items to handle our request for the item at index 10. Don’t worry too much right now on handling specific errors. We will discuss error handling in greater depth in a future tutorial.\nWhile there are quite a few other ways to handle missing data when “drilling down” through nested data structures in Python, for now, we will just use the try/except approach similar to the one shown above.\n\n\n\nDefault Dictionaries: A Nice Way to Handle Missing Keys\nWe mentioned earlier that you should check for key presence in a dictionary before doing something interesting with that key to avoid key errors. Default dictionaries solve this problem elegantly by automatically creating new entries with preset values when you access a key that doesn’t exist yet.\nA default dictionary is sort of like a self-initializing storage system. Instead of having to check if a key exists before using it, the dictionary takes care of that for you. It’s particularly useful when you’re counting occurrences or building categorized lists.\nYou can create default dictionaries with three common starting values:\n\nint: starts new entries at zero (perfect for counting)\nlist: starts new entries with an empty list [] (great for categorizing or grouping)\nstr: starts new entries with an empty string \"\"\n\nHere is an example showing how to initialize default dictionaries:\n\nfrom collections import defaultdict\n\n# For counting things (starts at 0)\nnucleotide_counts = defaultdict(int)\n\n# For grouping things (starts with empty list)\ngenes_chromosomes = defaultdict(list)\n\nLet’s look at some practical examples.\n\nCounting Items with defaultdict\nSay we want to count nucleotides in a DNA sequence. It is pretty straightforward with a default dictionary:\n\nnucleotide_counts = defaultdict(int)\ndna_sequence = \"ATGCATTAG\"\n\nfor base in dna_sequence:\n    nucleotide_counts[base] += 1\n\nfor nucleotide, count in nucleotide_counts.items():\n    print(f\"{nucleotide} =&gt; {count}\")\n\nA =&gt; 3\nT =&gt; 3\nG =&gt; 2\nC =&gt; 1\n\n\nWhat’s happening here? Each time we see a nucleotide:\n\nIf we haven’t seen it before, defaultdict automatically creates a counter starting at 0\nWe add 1 to the counter\n\nWithout defaultdict, we’d need this more complicated code:\n\nnucleotide_counts = {}\ndna_sequence = \"ATGCATTAG\"\n\nfor base in dna_sequence:\n    if base in nucleotide_counts:\n        nucleotide_counts[base] += 1\n    else:\n        nucleotide_counts[base] = 1\n\nfor nucleotide, count in nucleotide_counts.items():\n    print(f\"{nucleotide} =&gt; {count}\")\n\nA =&gt; 3\nT =&gt; 3\nG =&gt; 2\nC =&gt; 1\n\n\nYuck!\n\n\nGrouping Items with defaultdict\nDefault dictionaries are also great for grouping related items. Let’s organize some genes by their chromosomes:\n\nchromosomes = defaultdict(list)\n\nchromosomes[\"chr17\"].append(\"TP53\")\nchromosomes[\"chr13\"].append(\"BRCA2\")\nchromosomes[\"chr17\"].append(\"BRCA1\")\n\nfor chromosome, genes in chromosomes.items():\n    for gene in genes:\n        print(f\"{chromosome}, {gene}\")\n\nchr17, TP53\nchr17, BRCA1\nchr13, BRCA2\n\n\nNotice how we didn’t need to create empty lists for each chromosome first? The defaultdict does it for us. Each time we reference a new chromosome, it automatically creates an empty list ready to store genes.\n\n\ndefaultdict Summary\nThe default dictionary approach is particularly useful when you’re:\n\nCounting frequencies of any kind\nGrouping items by categories\nBuilding collections of related items\n\nDefault dictionaries combine the power of regular dictionaries with automatic handling of new keys, making your code both simpler and more robust.\n\n\n\nCounters\nPython has another type of dictionary called a counter. Counters provide a convenient way to tally hashable items.\nLet’s return to our example from above, but this time, we will use a Counter.\n\nfrom collections import Counter\n\n# This is all you need to tally the nucleotides!\nnucleotide_counts = Counter(\"ATGCATTAG\")\n\n# You can loop through the Counter like a dictionary\nfor nucleotide, count in nucleotide_counts.items():\n    print(f\"{nucleotide} =&gt; {count}\")\n\nA =&gt; 3\nT =&gt; 3\nG =&gt; 2\nC =&gt; 1\n\n\nWe can find the N most common items using most_common:\n\nprint(nucleotide_counts.most_common(2))\n\n[('A', 3), ('T', 3)]\n\n\nVery nice!\nWhat if we wanted to calculate the ratio of nucleotides rather than the raw counts? A counter can help us here too:\n\nnucleotide_counts = Counter(\"ATGCATTAG\")\n\ntotal = nucleotide_counts.total()\n\nfor nucleotide, count in nucleotide_counts.items():\n    ratio = count / total\n    print(f\"{nucleotide} =&gt; {ratio:.3f}\")\n\nA =&gt; 0.333\nT =&gt; 0.333\nG =&gt; 0.222\nC =&gt; 0.111\n\n\nPretty cool, right?\nCounters have lots of other neat methods and operator support that you may want to check out and use in your own programs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#control-flow-with-collections",
    "href": "collections.html#control-flow-with-collections",
    "title": "2  Collections",
    "section": "Control Flow with Collections",
    "text": "Control Flow with Collections\nNow that we have covered some of Python’s data structures and collections, and gone over the different type of loops, let’s dive a little deeper into how you can combine collections, loops, and control flow into more realistic programs.\n\nOverview\nYou have already seen how to loop over collections and sequences. But it never hurts to have a few more examples. Here is the for loop on a couple of different type of sequences:\n\nphrase = \"Hello, Python!\"\nfor letter in phrase:\n    print(letter)\n\nfoods = [\"apple\", \"pie\", \"grape\", \"cookie\"]\nfor food in foods:\n    print(food)\n\nfor number in range(2, 10, 2):\n    print(number)\n\nprices = {\"book\": 19.99, \"pencil\": 0.55}\n\n# By default, we only get the keys of a dictionary\n# in the for loop\nfor item in prices:\n    print(item)\n\n# Use .items() to get the key and value\nfor item, price in prices.items():\n    print(f\"{item} =&gt; ${price}\")\n\n# Use .values() to get just the values\nfor price in prices.values():\n    print(price)\n\nH\ne\nl\nl\no\n,\n \nP\ny\nt\nh\no\nn\n!\napple\npie\ngrape\ncookie\n2\n4\n6\n8\nbook\npencil\nbook =&gt; $19.99\npencil =&gt; $0.55\n19.99\n0.55\n\n\nAs we mentioned earlier, you can use the for loop on anything that is iterable.\nRecall that if you want to get the position of the item in the sequence over which you are looping, use enumerate.\n\nphrase = \"Hello, Python!\"\nfor index, letter in enumerate(phrase):\n    print(f\"{index}: {letter}\")\n\nfoods = [\"apple\", \"pie\", \"grape\", \"cookie\"]\nfor index, food in enumerate(foods):\n    print(f\"{index}: {food}\")\n\nfor index, number in enumerate(range(2, 10, 2)):\n    print(f\"{index}: {number}\")\n\n0: H\n1: e\n2: l\n3: l\n4: o\n5: ,\n6:  \n7: P\n8: y\n9: t\n10: h\n11: o\n12: n\n13: !\n0: apple\n1: pie\n2: grape\n3: cookie\n0: 2\n1: 4\n2: 6\n3: 8\n\n\nYou can use enumerate with dictionaries as well, but it is a bit less common, as many times when you are using a dictionary you don’t really care about the order anyway.\n\n\nControlling the Flow of Loops\nWhen you’re working with loops, sometimes you need more than just going through items one by one. You might want to skip certain items, stop the loop early, or take different actions based on what you find. Let’s explore some techniques that will give you more control over how your loops behave.\n\nMaking Decisions in Loops\nWe can use boolean expressions and conditional statements to make decisions inside of loops. This allows us to take different actions depending on characteristics of the data.\n\nfor n in range(10):\n    if n &gt; 5:\n        print(n)\n\n6\n7\n8\n9\n\n\nHere, we are looping through the numbers from 0 to 9, and if the number is 6 or more, then we print it, otherwise, we just go on to the next number.\nIn this example, we want to keep DNA sequences that start with the start codon ATG:\n\nstart_codon = \"ATG\"\nsequences = ['ATGCGC', 'AATTAA', 'GCGCGC', 'TATATA']\n\nwith_start_codons = []\n\nfor sequence in sequences:\n    if sequence.startswith(start_codon):\n        with_start_codons.append(sequence)\n\nprint(with_start_codons)\n\n['ATGCGC']\n\n\nThis example is actually a decent one for a comprehension:\n\nstart_codon = \"ATG\"\nsequences = ['ATGCGC', 'AATTAA', 'GCGCGC', 'TATATA']\n\nwith_start_codons = [\n    sequence for sequence in sequences if sequence.startswith(start_codon)\n]\n\nprint(with_start_codons)\n\n['ATGCGC']\n\n\nComprehensions can be nice for simple filtering and transformations, like in this example. However, you should be cautious about making them too complex. As a rule of thumb:\nGood for comprehensions:\n\nSimple filters (like checking if something starts with “ATG”)\nBasic transformations (like converting strings to uppercase)\nWhen the logic fits naturally on one line\n\nAvoid comprehensions when:\n\nThe logic gets nested or complicated\nMultiple operations are involved\nThe line becomes hard to read at a glance\n\nIn this case, the comprehension is kind of nice because it’s doing a single, straightforward filter operation. But remember: code readability is more important than being clever. If you find yourself writing a complex comprehension, consider using a regular for loop instead.\n\n\nbreak\nSometimes you find what you’re looking for before going through the entire sequence. The break statement is like having an “early exit” button – it lets you stop the loop immediately when certain conditions are met. Sometimes this can make your code more efficient by preventing unnecessary iterations.\nIn this example, we are interested in seeing if a collection of DNA sequences contains at least one sequence with an ambiguous base (N), and if so, save that DNA fragment and stop looking:\n\nsequences = ['ATGCGC', 'AATTAGA', 'GCNGCGC', 'TCATATA']\n\nfor i, sequence in enumerate(sequences):\n    print(f\"checking sequence {i+1}\")\n    # Recall that we can use `in` to check if a\n    # letter is in a word.\n    if \"N\" in sequence:\n        print(f\"sequence {i+1} had an N!\\n\")\n        sequence_with_n = sequence\n        break\n\nprint(sequence_with_n)\n\nchecking sequence 1\nchecking sequence 2\nchecking sequence 3\nsequence 3 had an N!\n\nGCNGCGC\n\n\nNotice how the loop stops after the 3rd sequence and doesn’t continue all the way until the end. This is thanks to the break keyword.\n\n\ncontinue\nThink of continue as a “skip to the next item” command. When you hit a continue statement, the loop immediately jumps to the next iteration. This is perfect for when you want to skip over certain items without stopping the entire loop, like focusing only on the data points that meet your criteria.\nIn this example, we only want to process protein fragments that start with Methionine (M) and skip the others. While there are multiple ways to approach this, let’s use continue:\n\nproteins = [\"MVQIPQNPL\", \"ILVDGSSYLYR\", \"MAYHAFPPLTNSA\", \"GEPTGA\"]\n\nfor protein in proteins:\n    if not protein.startswith(\"M\"):\n        continue\n\n    print(f\"we will process {protein}\")\n\nwe will process MVQIPQNPL\nwe will process MAYHAFPPLTNSA\n\n\nThis example is a little bit contrived. I actually think writing it without the continue is clearer:\n\nproteins = [\"MVQIPQNPL\", \"ILVDGSSYLYR\", \"MAYHAFPPLTNSA\", \"GEPTGA\"]\n\nfor protein in proteins:\n    if protein.startswith(\"M\"):\n        print(f\"we will process {protein}\")\n\nwe will process MVQIPQNPL\nwe will process MAYHAFPPLTNSA\n\n\n\n\n\nA Practical Example: Simulating Bacterial Growth\nLet’s look at something more interesting – simulating how bacteria might grow over time. We’ll create a simple model where each bacterium can grow, shrink, or stay the same size each day.\nPay particular attention to this exmaple. It will be useful for Miniproject 1!\n\nimport random\n\ntotal_bacteria = 15\n\n# Make 15 bacteria all starting with size 10\nbacteria = [10] * total_bacteria\n\n# Simple \"growth\" rules:\n#\n# - 50% chance to grow\n# - 25% chance to shrink\n# - 25% chance to stay the same\n\n# The outer loop tracks days in the experiment\nfor day in range(20):\n\n    # The inner loop tracks each individual bateria\n    for i in range(total_bacteria):\n        chance = random.random()\n\n        # First we check if this bacterium will grow today\n        if chance &lt; 0.5:\n            bacteria[i] += 1\n        # If it will not grow, we need to check if it will shrink\n        elif chance &lt; 0.75:\n            bacteria[i] -= 1\n\n        # We don't need the `else` here because if the bacterium\n        # won't grow AND it won't shrink, then no action is required.\n\n# Finally, we print out the sizes of all the bacteria\n# at the end of the experiment\nfor id, size in enumerate(bacteria):\n    print(f\"bacterium {id+1}, size: {size}\")\n\nbacterium 1, size: 12\nbacterium 2, size: 15\nbacterium 3, size: 10\nbacterium 4, size: 14\nbacterium 5, size: 14\nbacterium 6, size: 17\nbacterium 7, size: 11\nbacterium 8, size: 14\nbacterium 9, size: 9\nbacterium 10, size: 11\nbacterium 11, size: 12\nbacterium 12, size: 13\nbacterium 13, size: 13\nbacterium 14, size: 18\nbacterium 15, size: 18\n\n\nHere is what is happening:\n\nIn the outer loop, we run the simulation for 20 days, with each iteration representing one day of bacterial growth.\nIn the inner loop, we check each bacterium in our population and apply the growth rules using random chances.\nThen we loop through the bacteria sizes and print out the final size of each bacterium. (We treat the bacterium’s location in the array (plus one) as its ID.)\n\n\nHow the Random Choices Work\nThe clever part here is how we use a single random number to make weighted choices. Think of it like a number line from 0 to 1, divided into three sections:\n┌────────────────────┬──────────┬──────────┐\n│ 50%                │ 25%      │ 25%      │\n└────────────────────┴──────────┴──────────┘\n↑                    ↑          ↑          ↑\n0.0                  0.5        0.75       1.0\nWhen we generate a random number between 0 and 1:\n\nIf it falls in the first half (0.0-0.5), the bacterium grows\nIf it falls in the next quarter (0.5-0.75), the bacterium shrinks\nIf it falls in the last quarter (0.75-1.0), the bacterium stays the same size\n\nThis is one way to implement different probabilities for different outcomes. While this example uses bacterial growth, you could adapt this pattern for any situation where you need to simulate random events with different probabilities – like mutation rates, drug responses, or population changes.\nIf you are curious, Python has a method that simplifies this random choice logic. Check it out if you’re curious! You might want to use it for your first miniproject….",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#common-sequence-operations",
    "href": "collections.html#common-sequence-operations",
    "title": "2  Collections",
    "section": "Common Sequence Operations",
    "text": "Common Sequence Operations\nYou may have noticed that we can treat many of Python’s collection types in a similar way.\nOne of Python’s most helpful features is that many collection types (like lists, strings, and tuples) share the same basic operations. This means once you learn how to work with one type of sequence, you can apply that knowledge to others – you can find the length of any sequence using len(), check if something exists in a sequence using in, or grab a specific element using square bracket notation [].\nFor instance, whether you’re working with a DNA sequence as a string or a list of gene names, you can use the same syntax: len(\"ATCG\") and len([\"nrdA\", \"nrdJ\"]) both work the same way!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#choosing-the-right-collection",
    "href": "collections.html#choosing-the-right-collection",
    "title": "2  Collections",
    "section": "Choosing the Right Collection",
    "text": "Choosing the Right Collection\nWhen deciding which type of collection to use, consider these three key questions:\n\n“How will I create or receive this data initially?”\n“How will I need to access this data later?”\n“How will I need to modify this data?”\n\nHere’s a practical guide to help you choose:\n\nUse a list when…\n\nYour data has a meaningful order (e.g., lines from a file, time series)\nYou need to access items by position (index) or slices\nYou need ordered operations (iteration in sequence, sorting, reversing)\nYou want efficient operations at the end of the collection (append/pop)\nYou need to maintain duplicates\nYou need to modify items in place\n\n\n\nUse a dictionary when…\n\nYour data naturally comes as key-value pairs\nYou need to look up values by a unique identifier (key)\nYou need to efficiently find, add, or update specific items without linear searching\nYou want to map one piece of data to another\nYou need to combine data from multiple sources using a common key\n\n\n\nUse a set when…\n\nYou only care about uniqueness, not order or association\nYou need automatic elimination of duplicates\nYou’re only concerned with presence/absence of items\nYou need to perform set operations (unions, intersections, differences)\nYou need fast membership testing\n\n\n\nExamples\nFor instance, when processing a FASTA file, you’ll encounter ID-sequence pairs. If you need to access sequences by their identifiers later, a dictionary is the natural choice. However, if you’re only interested in the sequences themselves and won’t need to reference them by ID, storing just the sequences in a list would be more appropriate.\nAs another example, consider analyzing homology search results where you need to organize multiple hits that correspond to each query sequence. If you’ll need to retrieve all hits for a specific query using its identifier, a dictionary is ideal. You could structure it with query IDs as keys and lists of corresponding hits as values, allowing efficient lookup of results for any particular query of interest:\n\n# Tuples of query-target-bitscore -- imagine these come directly from a BLAST\n# output file or something similar.\nhomology_search_results = [\n    (\"query_1\", \"target_1\", 95),\n    (\"query_1\", \"target_2\", 32),\n    (\"query_2\", \"target_1\", 112)\n]\n\nquery_hits = {}\n\nfor query, target, bitscore in homology_search_results:\n    hit_info = (target, bitscore)\n\n    if query in query_hits:\n        query_hits[query].append(hit_info)\n    else:\n        query_hits[query] = [hit_info]\n\nprint(query_hits[\"query_2\"])\n\n[('target_1', 112)]\n\n\n\n\nSummary\nTo summarize, select the collection type that both enhances code readability and aligns with your specific patterns of data creation, access, and modification throughout your program’s workflow.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "collections.html#key-takeaways",
    "href": "collections.html#key-takeaways",
    "title": "2  Collections",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nWe’ve covered a lot of material about some of Python’s most commonly used data structures. Here are some key takeaways.\n\nGeneral Suggestions\n\nGenerally keep data types consistent within collections\nUse clear, descriptive names\nChoose the simplest structure that works\nUse list comprehensions for simple transformations\nHandle missing dictionary keys with get\nConsider memory usage with large datasets\n\n\n\nWatch Out For\n\nModifying lists while iterating\nForgetting tuple immutability\nMissing dictionary keys\nInfinite loops\nUsing lists when dictionaries would be more appropriate",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Collections</span>"
    ]
  },
  {
    "objectID": "algorithms.html",
    "href": "algorithms.html",
    "title": "3  Algorithmic Thinking",
    "section": "",
    "text": "Overview\nAn algorithm is a step-by-step procedure for solving a problem or accomplishing a task. It as a detailed set of instructions that takes some input, follows a clear sequence of steps, and produces a desired output.\nAlgorithms can vary greatly in their level of complexity, from simple operations like finding the larger of two numbers to complex tasks such as generating a phylogenetic tree from a sequence alignment. It’s worth noting that the same problem might have multiple algorithmic solutions, each with its own advantages and trade-offs in terms of simplicity and efficiency.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#overview",
    "href": "algorithms.html#overview",
    "title": "3  Algorithmic Thinking",
    "section": "",
    "text": "Key Characteristics of Algorithms\n(Adapted from The Art of Computer Programming by Donald E. Knuth)\nAll algorithms share some important properties:\n\nDefined inputs & outputs\n\nAlgorithms must have clearly defined inputs and outputs.\nExample: PCR protocol\n\nInput: Template DNA, primers, nucleotides, polymerase\nOutput: Amplified DNA fragments\n\n\nDefiniteness\n\nSteps must be clear and unambiguous\nEach step must be understood exactly the same way by anyone following it\nExamples:\n\nGood example: “Heat the sample at 95°C for 5 minutes”\nBad example: “Heat the sample for a while”\n\n\nFiniteness\n\nThe algorithm must terminate after a finite number of steps\nI.e., it cannot run indefinitely\nExamples:\n\nGood example: A PCR reaction has a specific number of cycles\nBad example: “Keep checking gel until bands appear” (no clear end point)\n\n\nEffectiveness\n\nEach step must be basic enough to be executed\nMust be doable with available resources (ideally by a person using a pen and paper, but not always practical)\nExamples:\n\nGood example: “Pipette 100 µL”\nBad example: “Separate molecules instantly”\n\n\n\nThere are a few more important properties of algorithms. Generally an algorithm should produce the same output given the same input. For example, if your algorithm is supposed to triple a number, an input of 5 should always produce an output of 15. Additionally, an algorithm should ideally be general enough to solve similar problems in a category. Your tripling algorithm would be more useful (and general) if a user could supply both the number to be multiplied (multiplicand) and the multiplier. This way, the same algorithm could be used for doubling, tripling, quadrupling, etc.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#algorithmic-thinking-process",
    "href": "algorithms.html#algorithmic-thinking-process",
    "title": "3  Algorithmic Thinking",
    "section": "Algorithmic Thinking Process",
    "text": "Algorithmic Thinking Process\nBeing able to think algorithmically is essential for success in programming. Algorithmic thinking is the ability to break down problems into clear, logical steps that a computer can follow – like writing a very detailed recipe where nothing can be assumed or left to interpretation. This skill helps you break down complex problems and translate them into effective code solutions.\nLet’s go through the various aspects of algorithms.\n\nBasic Components\nEvery algorithm consists of three fundamental parts:\n\nInput\n\nThe data or information that your algorithm needs to work with.\nThis could be numbers, text, DNA sequences, or any other form of data.\n\nProcessing\n\nThe step-by-step instructions that transform your input into the desired result.\nThis is essentially your recipe or procedure.\n\nOutput\n\nThe final result or solution that your algorithm produces.\nThis should match what you need to solve your problem.\n\n\nBefore you can write an algorithm, you need to understand what problem you’re trying to solve:\n\nDefining the problem scope\n\nClearly state what your algorithm should and shouldn’t do.\nFor example, “Find all prime numbers under 100” is clearer than “Find prime numbers.”\n\nUnderstanding requirements\n\nList everything your solution needs to handle.\nWhat kinds of input should it work with?\nWhat should it do with invalid input?\n\n\nYou can think of these as “behind-the-scenes” components. They are critical to algorithmic thinking and construction, but not always explicitly part of the algorithm itself.\n\n\nBreaking Problems Into Steps\nOnce you have these components in mind, you can break large problems into smaller pieces, which are much more manageable:\n\nOne big problem -&gt; Mutliple sub-problems\n\nBreak your main problem into smaller, manageable tasks.\nInstead of “Analyze DNA sequence,” think about\n\nRead sequence\nCheck validity\nFind patterns\n\nThese steps can get as granular as necessary for you to solve the problem at hand.\n\nDetermine the essential operations needed for each sub-problem.\nCreate a logical sequence of operations\n\nArrange your sub-problems in a logical order.\nWhat needs to happen first?\nWhich steps depend on other steps?\n\n\n\n\nPseudocode Development\nDepending on the complexity of your problem, it can be helpful to sketch out your solution in plain language or pseudocode:\n\nWriting abstract steps\n\nWrite out your algorithm in everyday language.\nUse simple statements like “For each number in the list” or “If sequence is valid then…”\n\nPlanning program flow\n\nMap out how your program will move from start to finish.\nWhat decisions need to be made?\nWhat steps might need to repeat?\n\nOutlining solution structure\n\nOrganize your steps into a clear structure, showing where loops and decisions occur.\n\n\nPlanning your code’s structure and components before diving into actual programming makes the whole process much smoother. When you tackle problems this way, you can focus on one aspect at a time, first mapping out the logic and flow, then implementing the code itself. This approach prevents you from getting overwhelmed by trying to solve multiple challenges simultaneously.\nWhile thorough planning is essential when you’re learning, you’ll likely develop a more streamlined approach as you gain experience. For simpler problems, you may find yourself able to start coding directly, having internalized the planning process. However, for complex projects, taking time to sketch out your approach first remains valuable regardless of your skill level.\n\n\nImplementation\nNow that you have a solid plan, it’s time to translate it into working code.\n\nConvert each step from your pseudocode into actual Python code, one piece at a time\nBuild your code following the structure you mapped out earlier\nKeep your code clean and maintainable by:\n\nUsing descriptive variable names that make sense\nAdding helpful comments to explain what your code does\nFollowing consistent formatting and organization\n\n\nThis stage is a bit like assembling the pieces of a puzzle where you already know what the final picture should look like. Take your time with each component – rushing through implementation often leads to mistakes that can be challenging to fix later.\n\n\nTesting and Validation\nAfter your implementation is complete, be sure to test that your algorithm works correctly:\n\nTesting with simple examples\n\nStart with basic test cases where you know the correct answer.\nCheck that your algorithm produces correct results for all expected inputs.\n\nIterative refinement\n\nImprove your solution based on test results.\nFix errors and handle edge cases.\n\n\nWe will discuss more testing strategies in a later tutorial.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#real-world-algorithm-example-making-coffee",
    "href": "algorithms.html#real-world-algorithm-example-making-coffee",
    "title": "3  Algorithmic Thinking",
    "section": "Real-World Algorithm Example – Making Coffee",
    "text": "Real-World Algorithm Example – Making Coffee\nLet’s take an everyday activity, making coffee, and practice turning it into clear instructions that could work as an algorithm. We will start with some pretty bad instructions, identify problems with them, and then refine them.\n(Apologies to all the tea lovers reading this!)\n\nTake 1\nHere is a silly set of instructions for making coffee:\n\nYou’ll want to put some liquid in there first, then put the paper thing in. Get the coffee ready – not too much, not too little. Make sure everything is closed tight before you get things going. Now, just give it a tap and wait a while. If all goes well, you should end up with something drinkable!\n\nIf you have ever made coffee before, you could probably figure out how to follow these instructions. However, it doesn’t really work as an algorithm:\n\n“liquid”: should I add water or milk?\n“in there”: in where?\n“paper thing” instead of filter\n“coffee”: should I add coffee beans, ground coffee, instant coffee?\n“give it a tap” instead of pressing the start button\n“not too much, not too little”: no specific measurements or timing\nUses subjective phrases like “nice and tight” and “you know how it goes”\nUncertain outcome (“if all goes well”)\n\nLet’s address some of these issues and try again.\n\n\nTake 2\n\nPour fresh water into the top part until it looks full enough. Insert a clean paper filter (any size should work) into the basket area. Measure some coffee grounds – about a few spoonfuls should do it, depending on how strong you like it. Make sure to close the top properly until you hear a click or something. Find the power button (it might be on the front or side) and push it down. After a few minutes when you stop hearing the machine make noise, your coffee should be done!\n\nThough this version is definitely better than the last one, it still has a few issues:\n\nImprecise measurements (“looks full enough”, “a few spoonfuls”)\nAmbiguous specifications (“any size should work”)\nSubjective criteria (“how strong you like it”)\nUncertain timing (“a few minutes”)\nVague sensory cues (“hear a click or something”, “stop hearing the machine”)\nOptional or unclear elements (“or something”, “might be on the front”)\n\nAgain, if you have ever used a coffee machine, you could probably understand the instructions and adapt them to your taste to make a good cup of coffee. But to be a good algorithm, it still needs more precision, less ambiguity, and it shouldn’t leave so much up to your own taste.\nLet’s address some more of those ambiguities.\n\n\nTake 3\n\nPour 8 cups of fresh water into the top reservoir, filling to the marked water line. Insert a #4 cone paper filter into the filter basket. Measure 2 level tablespoons of ground coffee per cup of water (16 tablespoons total for a full pot). Press down firmly on the lid until you hear a distinct click indicating it’s fully closed. Locate the power switch on the front panel and press it to the “ON” position. The brewing process will take approximately 5-7 minutes, and is complete when the gurgling sound stops and the brewing indicator light turns off. Your coffee is now ready to serve.\n\nHere are some specific improvements as compared to the last version:\n\nUsing specific measurements (8 cups, #4 filter, 2 tablespoons per cup)\nProviding clear indicators (marked water line, distinct click)\nGiving a defined time range (5-7 minutes)\nIncluding concrete completion signals (gurgling stops, indicator light)\nSpecifying exact locations (front panel)\n\n\n\nBeyond the Basic Steps\nThough we could keep refining these instructions, it’s not a bad description of making coffee now!\nIf this were a “real algorithm” that we needed to program in Python, there are some more things we should think about. When writing instructions for a computer (or a coffee maker!), it’s easy to focus on the happy path – the sequence of actions that work perfectly. However, robust algorithms must consider various other factors to effectively handle real-world situations where things can go wrong.\nHere are some things to think about that are “beyond the basic steps”.\n\nSetup and Requirements\nBefore starting any process, we need to ensure everything is in place. This includes checking equipment, materials, and the system’s readiness. (You might see the term “preconditions” if you are reading about algorithms online.)\n\nGeneral Questions:\n\nHave we verified all equipment is functional?\nAre all necessary materials available?\nIs the system in a ready state?\n\nCoffee Maker Example:\n\nIs the coffee maker plugged in and working?\nIs it clean/ready to use?\nDo you have all needed supplies on hand?\n\n\n\n\nHandling Problems\nThings can go wrong. A good algorithm anticipates potential problems and provides solutions.\n\nGeneral Questions:\n\nHow do we handle insufficient resources?\nWhat happens if components fail?\nHow do we respond to interruptions?\nWhat backup procedures are needed?\n\nCoffee Maker Example:\n\nWhat if there’s not enough water?\nWhat if the filter is inserted incorrectly?\nWhat if the machine doesn’t turn on?\nWhat if the brewing stops midway?\n\n\nWhile you can’t generally anticipate everything that may go wrong, it’s a good idea to put some thought into it, and try to handle any likely errors.\n\n\nSequential Dependencies\nGenerally, certain steps will rely on others. We need to define the correct order of operations.\n\nGeneral Questions:\n\nWhich steps must happen in a specific order?\nWhat are the critical timing requirements?\nWhich steps block others from proceeding?\n\nCoffee Maker Example:\n\nThe filter must be inserted before adding coffee grounds\nWater must be added before turning on the machine\nThe lid must be closed before powering on\n\n\n\n\nConditional Pathways\nAlgorithms often need to handle different scenarios based on input or conditions.\n\nGeneral Questions:\n\nHow do varying inputs affect the process?\nWhat alternative routes exist?\nHow do we handle different scenarios?\n\nCoffee Maker Example:\n\nIf making less than a full pot, adjust measurements accordingly\nIf using different grind sizes, adjust portions\n\n\n\n\nValidation\nValidation and verification of any post-conditions is essential to ensure each step is completed successfully and the final result is correct.\n\nGeneral Questions:\n\nHow do we verify each step succeeded?\nWhat indicates proper operation?\nHow do we confirm the final result?\n\nCoffee Maker Example:\n\nHow to check if the filter is seated properly\nHow to verify the water is actually flowing/brewing\nHow to confirm the coffee is properly brewed\n\n\n\n\nSummary\nWhile we don’t need this level of detail for every example, it’s valuable to understand how simple procedures evolve into robust algorithms through careful consideration of edge cases, error handling, and validation steps.\nThis methodology applies broadly to software development: start simple, then systematically address complexities and potential problems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#building-blocks-for-solving-programming-problems",
    "href": "algorithms.html#building-blocks-for-solving-programming-problems",
    "title": "3  Algorithmic Thinking",
    "section": "Building Blocks for Solving Programming Problems",
    "text": "Building Blocks for Solving Programming Problems\nWhen you’re learning to program, it helps to recognize that many solutions are built from common, reusable patterns. These patterns are basic building blocks that you can combine and adapt to solve more complex problems.\nWhile there are often many ways to solve a programming challenge, we’ll focus on straightforward approaches that are easy to understand and implement. These might not always be the most efficient solutions, but they’re good learning tools that will help you:\n\nBreak big problems into manageable pieces\nLearn reliable approaches to common challenges\nDevelop your problem-solving skills\n\nAs you gain experience, you’ll learn more sophisticated methods, or ways that are built-in to the language itself, but mastering these fundamental patterns first will give you a solid foundation. Let’s look at some practical examples that demonstrate these basic patterns in action.\nIn this tutorial, we will mostly focus on strategies that involve looking at one element at a time from a sequence. Often, this sequential processing will also involve tracking or accumulating values.\n\nCharacter Processing\nTutorial 2 showed many examples of using for loops to process characters of a string one-by-one. We will repeat some of them here so that you can get more practice seeing the common patterns.\n\nPrinting Each Character\nPrinting each letter of a string:\n\nword = \"HELLO\"\nfor character in word:\n    # Do something interesting with each character...\n    print(character)\n\nH\nE\nL\nL\nO\n\n\n\n\nIterating with an Index\nAccessing the index of the character during iteration:\n\n# Print with position\nword = \"hello\"\nfor index, letter in enumerate(word):\n    print(f\"Position {index}: {letter}\")\n\nPosition 0: h\nPosition 1: e\nPosition 2: l\nPosition 3: l\nPosition 4: o\n\n\n\n\nIterating in Reverse\nProcessing a string in reverse order:\n\n# Process in reverse\nfor letter in reversed(word):\n    print(letter)\n\no\nl\nl\ne\nh\n\n\n\n\nFrequencies\nCounting the frequency of individual letters:\n\nletter_counts = {}\n\nfor letter in word:\n    current_count = letter_counts.get(letter, 0)\n    letter_counts[letter] = current_count + 1\n\nprint(letter_counts)\n\n{'h': 1, 'e': 1, 'l': 2, 'o': 1}\n\n\nNote: This is a good example of what I mentioned above regarding these solutions not always being the best way to do something. In Tutorial 2, we discussed a better way to approach this particular counting problem. Can you remember it?\n\n\n\nNumber Processing\n\nRunning Sum\nTracking a running sum:\n\nnumbers = [2, 5, 3, 1]\ntotal = 0\n\nfor number in numbers:\n    total += number\n\nprint(f\"Total: {total}\")\n\nTotal: 11\n\n\nWhile Python provides built-in functions like sum() for this specific case, understanding the basic pattern helps with more complex variations.\n\n\nSumming Positive Numbers\nSum of positive numbers:\n\nnumbers = [-1, 2, -5, 3, -8, 1]\n\npositive_sum = sum(num for num in numbers if num &gt; 0)\n\nprint(f\"Sum of positive numbers: {positive_sum}\")\n\nSum of positive numbers: 6\n\n\n\n\nAverages\nCalculating the average of a list of numbers:\n\n# Calculate average\nnumbers = [2, 5, 3, 8, 1]\naverage = sum(numbers) / len(numbers)\nprint(f\"Average: {average}\")\n\nAverage: 3.8\n\n\n\n\n\nFinding Maximum/Minimum\nFinding the largest number in a list without using Python’s built-in max function:\n\nnumbers = [5, 3, 0, -1, 8]\nlargest_number = numbers[0]\n\nfor number in numbers[1:]:\n    if number &gt; largest_number:\n        largest_number = number\n\nprint(largest_number)\n\n8\n\n\nFinding the shortest string in a list:\n\nwords = [\"i\", \"like\", \"apple\", \"pie\"]\nshortest_word = words[0]\n\nfor word in words[1:]:\n    if len(word) &lt; len(shortest_word):\n        shortest_word = word\n\nprint(shortest_word)\n\ni\n\n\n\n\nSimple Search/Validation\nAnother common task is finding an item in a collection or validating some condition of a collection.\n\nFinding a Number in a List\nFinding a specific number in a list:\n\ntarget = 5\nnumbers = list(range(10))\nis_found = False\n\nfor number in numbers:\n    if number == target:\n        is_found = True\n        print(\"we found the number!\")\n\nif not is_found:\n    print(\"we didn't find the number!\")\n\nwe found the number!\n\n\n\n\nIs a List Sorted?\nChecking if a list is sorted. (Before reading the code, try and think of how the solution might look.)\n\nnumbers = [1, 2, 4, 3, 5]\nprevious_number = numbers[0]\n\nis_sorted = True\n\nfor current_number in numbers[1:]:\n    if current_number &lt; previous_number:\n        is_sorted = False\n        break\n    previous_number = current_number\n\nif is_sorted:\n    print(\"the list is sorted!\")\nelse:\n    print(\"the list is not sorted!\")\n\nthe list is not sorted!\n\n\nThis example has a couple of interesting things to focus on:\n\nWe start the iteration at index 1 in the array\nAs soon as we see a number that is not sorted, we break since that is enough to say the array as a whole is unsorted.\n\n\n\n\nNested Loops\nProblems often require nested loops, such as cases where for every item in one list, you need to process every item in another list. Note that these nested loop problems can often be solved in clever ways that help you avoid a having to look at every combination. There’s a good chance you will see some of these clever solutions as you are exposed to more code in the future.\n\nDistance Between Points\nCalculating distances between points. Here we are using 1-dimensional points. The distance between two points in 1D (on a line) is the absolute value of their difference. E.g., if you have two points x₁ and x₂, the distance between them is |x₁ - x₂|.\n\npoints = [8, 3, 4]\ndistances = []\n\nfor x in points:\n    for y in points:\n        distance = abs(x - y)\n        distances.append((x, y, distance))\n\nfor x, y, distance in distances:\n    print(f\"({x}, {y}) =&gt; {distance}\")\n\n(8, 8) =&gt; 0\n(8, 3) =&gt; 5\n(8, 4) =&gt; 4\n(3, 8) =&gt; 5\n(3, 3) =&gt; 0\n(3, 4) =&gt; 1\n(4, 8) =&gt; 4\n(4, 3) =&gt; 1\n(4, 4) =&gt; 0\n\n\nYou could imagine instead of distances of 1D points, this pattern could work for calculating all-vs-all homology scores from BLAST output, or comparing some aspect of each sample vs. every other sample.\n\n\nDistance Between Samples\nHere’s a slightly different example. In this case, say we have ecological distances between all sampling locations stored in a dictionary. Here is one way that you might loop through them:\n\nsample_distances = {\n    \"S1\": {\"S1\": 0, \"S2\": 3, \"S3\": 5},\n    \"S2\": {\"S1\": 2, \"S2\": 0, \"S3\": 1},\n    \"S3\": {\"S1\": 6, \"S2\": 2, \"S3\": 0},\n}\n\nfor sample_a, other_samples in sample_distances.items():\n    for sample_b, distance in other_samples.items():\n        print(f\"{sample_a} -&gt; {sample_b} =&gt; {distance}\")\n\nS1 -&gt; S1 =&gt; 0\nS1 -&gt; S2 =&gt; 3\nS1 -&gt; S3 =&gt; 5\nS2 -&gt; S1 =&gt; 2\nS2 -&gt; S2 =&gt; 0\nS2 -&gt; S3 =&gt; 1\nS3 -&gt; S1 =&gt; 6\nS3 -&gt; S2 =&gt; 2\nS3 -&gt; S3 =&gt; 0\n\n\nWhile there are often clever ways to avoid these type of all-vs-all comparisons, they still come up pretty frequently, so it’s a good idea to get familiar with them!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#introduction-to-algorithm-analysis",
    "href": "algorithms.html#introduction-to-algorithm-analysis",
    "title": "3  Algorithmic Thinking",
    "section": "Introduction to Algorithm Analysis",
    "text": "Introduction to Algorithm Analysis\nWhen we write code to solve a problem, there’s usually more than one way to do it. It’s a bit like how you may have different routes to get to work – some are faster, some are more efficient, and some are just easier to remember. The same applies to our code solutions.\nWhen evaluating different approaches to solving a problem, we typically look at three main things:\n\nDoes it actually solve the problem correctly?\nHow efficiently does it run (in terms of time and computer memory)?\nIs it clear and maintainable?\n\n\nTime Complexity\nLet’s focus on efficiency in terms of time for a moment. Imagine you have a list of genes to search through. You could check each gene one by one (we call this linear time), or you might have a clever way to eliminate half the possibilities with each step (logarithmic time). As your gene list grows from hundreds to millions of entries, these different approaches can mean the difference between waiting seconds versus hours for your results.\nComputer scientists use something called “Big O notation” to describe how an algorithm’s performance changes as the input gets larger. Here are some common patterns you’ll encounter.\n\nConstant time (O(1)): The operation always takes the same amount of time\nLinear time (O(n)): The time increases directly with the size of the input\nQuadratic time (O(n²)): The time increases with the square of the input size\n\nThe key takeaway is that some solutions scale better than others when working with larger datasets. As you write code, keeping these basic patterns in mind will help you make better choices about how to approach problems.\nHere are some simple examples to illustrate these three time complexities.\n\nConstant Time – O(1)\nConstant time operations like dictionary lookups:\n\ngene_info = {\"nrdA\": \"ribonucleotide reductase\"}\nresult = gene_info[\"nrdA\"]\nprint(result)\n\nribonucleotide reductase\n\n\n\n\nLinear Time – O(n)\nLinear time operations like checking each item in a list once:\n\n# Counting mutations\ndna_sequence = \"ACTACTGTACTACTGTCACACTAGAGTAT\"\nt_count = 0\n\nfor base in dna_sequence:\n    if base == \"T\":\n        t_count += 1\n\nprint(t_count)\n\n9\n\n\n\n\nQuadratic Time – O(n²)\nQuadratic time operations like comparing every item with every other item:\n\n# Finding equivalent sequences\nsequences = [\"ACTG\", \"ATGAC\", \"ACTGGT\", \"ACTG\"]\nsequence_count = len(sequences)\n\nfor i in range(sequence_count):\n    for j in range(sequence_count):\n        if i != j and sequences[i] == sequences[j]:\n            print(f\"Match found: {sequences[i]}\")\n\nMatch found: ACTG\nMatch found: ACTG\n\n\n\n\n\nSpace Complexity\nIn addition to thinking about how long our code takes to run, sometimes we also need to consider how much memory it uses. Space complexity describes how memory usage grows with input size. The two most common patterns you’ll encounter are:\n\nO(1) space: Uses a fixed amount of extra memory regardless of input size\nO(n) space: Uses extra memory that grows with the input size\n\nHere are some examples.\n\nConstant Space – O(1)\nIn a constant space solution, the same few variables are used regardless of the input size:\n\ng_count = 0\n\nfor base in dna_sequence:\n    if base == \"G\":\n        g_count += 1\n\nprint(g_count)\n\n4\n\n\nIn this example, we’re just counting, so we only need one variable no matter how long the DNA sequence is.\n\n\nLinear Space – O(n)\nIn a linear space solution, the space needed to calculate the result grows linearly with the size of the input.\n\ng_positions = []\n\nfor i in range(len(dna_sequence)):\n    if dna_sequence[i] == \"G\":\n        g_positions.append(i)\n\nprint(g_positions)\n\n[6, 14, 23, 25]\n\n\nIn this example, we’re storing positions, so we need more space for longer sequences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#algorithmic-puzzles",
    "href": "algorithms.html#algorithmic-puzzles",
    "title": "3  Algorithmic Thinking",
    "section": "Algorithmic Puzzles",
    "text": "Algorithmic Puzzles\nLet’s finish off this tutorial by looking at a common, beginner-level algorithmic puzzle: checking if a string is a palindrome.\n\nA word is a palindrome if it reads the same forward and backward.\n\nNote: This problem description is adapted from LeetCode problem 125. Valid Palindrome.\n\nStarting with the Problem\nFirst, let’s understand what we’re trying to do in plain English:\n\nWe need to check if a word reads the same forwards and backwards\nExamples\n\n“racecar” → Yes!\n“hello” → No!\n\n\n(For this version of the problem, we can assume that the strings we need to check are all a single word with all lowercase letters.)\n\n\nSolution 1: The Obvious Way\nLet’s start with the most obvious solution that uses Python string slicing to check the definition of a palindrome.\n\nstring = \"racecar\"\nis_palindrome = string == string[::-1]\nprint(string, is_palindrome)\n\nstring = \"apple\"\nis_palindrome = string == string[::-1]\nprint(string, is_palindrome)\n\nracecar True\napple False\n\n\nThis is probably how most people would first think about it: “Just reverse it and compare!”\n\nIt’s perfectly valid\nIt’s easy to understand\nIt uses built-in Python functions\nBut…It requires creating a whole new reversed string in memory\n\nOften, the “but” doesn’t really matter in whatever work you are doing. Many times “clear and maintainable” are much more important that optimal efficiency. However, in the world of algorithmic puzzles, the “but” is definitely something you want to address!\n\n\nSolution 2: Manual Comparison with a Loop\nFor this solution, we think, “Wait, do we really need to reverse the string? What a waste of time and space!”\nInstead, we will look at “pairs” of letters. First, let’s try a word that is a palindrome:\n\nstring = \"racecar\"\nprint(\"Checking {string}\")\nis_palindrome = True\n\nfor i in range(len(string)):\n    j = len(string) - i - 1\n\n    print(i, j, string[i], string[j])\n\n    if string[i] != string[j]:\n        is_palindrome = False\n        break\n\nprint(string, is_palindrome)\n\nChecking {string}\n0 6 r r\n1 5 a a\n2 4 c c\n3 3 e e\n4 2 c c\n5 1 a a\n6 0 r r\nracecar True\n\n\nNext, try a word that is not a palindrome, just to see the difference:\n\nstring = \"racethecar\"\nprint(\"\\n\\nChecking {string}\")\nis_palindrome = True\n\nfor i in range(len(string)):\n    j = len(string) - i - 1\n\n    print(i, j, string[i], string[j])\n\n    if string[i] != string[j]:\n        is_palindrome = False\n        break\n\nprint(string, is_palindrome)\n\n\n\nChecking {string}\n0 9 r r\n1 8 a a\n2 7 c c\n3 6 e e\n4 5 t h\nracethecar False\n\n\nDo you see how the non-palindrome stops before checking all the values?\nHere is a breakdown of the above solution:\n\nWe compare the first letter with last letter\nThen, the second letter with second-to-last letter\nAnd so on…\nBut…We’re doing each comparison twice!\n\nJust so it is clear, let’s explain that j index line. This diagram shows why we use that formula to calculate j:\nlen(\"racecar\") == 7\n\n┌───┬───┬───┬───┬───┬───┬───┐\n│ r │ a │ c │ e │ c │ a │ r │\n└───┴───┴───┴───┴───┴───┴───┘\n  0   1   2   3   4   5   6\n  ↑   ↑   ↑   ↑   ↑   ↑   ↑\n  i=0 │   │   │   │   │   j = 7 - 0 - 1 = 6\n      │   │   │   │   │\n      i=1 │   │   │   j = 7 - 1 - 1 = 5\n          │   │   │\n          i=2 │   j = 7 - 2 - 1 = 4\n              │\n              i=3, j = 7 - 3 - 1 = 3\n\nand so on...\n\n\nSolution 3: The Optimized Version\nThere was another “but” in Solution 2: Wouldn’t it be better if we didn’t have to do those duplicated checks? Let’s give it a shot.\n\nstring = \"racecar\"\nis_palindrome = True\n\ni = 0\nj = len(string) - 1\n\nwhile i &lt; j:\n    print(i, j, string[i], string[j])\n\n    if string[i] != string[j]:\n        is_palindrome = False\n        break\n\n    i += 1\n    j -= 1\n\nprint(string, is_palindrome)\n\n0 6 r r\n1 5 a a\n2 4 c c\nracecar True\n\n\nAnd here again with a string that is not a palindrome:\n\nstring = \"racethecar\"\nis_palindrome = True\n\ni = 0\nj = len(string) - 1\n\nwhile i &lt; j:\n    print(i, j, string[i], string[j])\n\n    if string[i] != string[j]:\n        is_palindrome = False\n        break\n\n    i += 1\n    j -= 1\n\nprint(string, is_palindrome)\n\n0 9 r r\n1 8 a a\n2 7 c c\n3 6 e e\n4 5 t h\nracethecar False\n\n\nAnd the breakdown:\n\nWe use two “pointers” moving toward each other\nWe only check each pair once\nWe stop at the middle\n\nThis is both time and space efficient, and doesn’t do any more checks than we need to do. Cool, right?\n\n\nGeneral Approach\nThe above process of refinement suggests a general approach to these kinds of algorithmic puzzles.\n\nStart simple\n\nImplement the first solution that comes to mind\nDon’t worry if it’s not perfect\nMake sure it works!\n\nQuestion your approach\n\nDo I need all these steps?\nAm I repeating work?\nIs there a more direct way?\n\nLook for patterns\n\nNotice we’re comparing pairs of letters\nNotice we’re moving inward from both ends\nThese observations lead to better solutions\n\nConsider resource usage\n\nTime: How many steps are we taking?\nSpace: How much extra memory do we need?\nCan we reduce either?\n\n\nFocus on making a basic version work before aiming for perfection. Begin with a simple solution, ensure it functions correctly, and then gradually improve it. This mirrors the process of algorithmic thinking described above of breaking down complex problems into manageable steps and refining your solution as needs evolve. This is similar to how programming works in real-world scenarios!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#summary-connection-to-bioinformatics",
    "href": "algorithms.html#summary-connection-to-bioinformatics",
    "title": "3  Algorithmic Thinking",
    "section": "Summary & Connection to Bioinformatics",
    "text": "Summary & Connection to Bioinformatics\nIn this tutorial, we introduced the concept of algorithmic thinking and simple algorithms. We went over some common patterns for simple problems that can form the building blocks of more complex solutions. Then we covered the basics of algorithmic complexity analysis, and finally, went through the process of solving a common algorithmic puzzle.\nYou might be wondering how these basic programming concepts connect to the bioinformatics tools you’ll use in your research. While we rely on sophisticated software for tasks like homology search and genome assembly, these powerful tools are built on the same fundamental programming principles we’re learning now.\nAlgorithmic thinking, or understanding how to break down problems and translate them into logical steps, is a foundational skill for all coding work. Though we won’t be building complex bioinformatics tools from scratch in this course, mastering the basics will give you a solid foundation for writing your own analysis scripts and understanding how the bioinformatics tools you use actually work.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "algorithms.html#bibliography",
    "href": "algorithms.html#bibliography",
    "title": "3  Algorithmic Thinking",
    "section": "Bibliography",
    "text": "Bibliography\nKnuth, Donald E. 1997. The Art of Computer Programming: Volume 1: Fundamental Algorithms. 3rd ed. Boston, MA: Addison Wesley.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithmic Thinking</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "4  Functions",
    "section": "",
    "text": "Introduction\nUntil now, we’ve mostly been using built-in Python’s functions like print(), len(), and range() to get things done. While Python’s standard library provides a ton of useful functionality, eventually you will need to create something more tailored to your specific needs.\nThis is where custom functions come in. Functions are like reusable blocks of code that you design to perform specific tasks. Creating your own functions allows you to extend Python’s capabilities beyond what’s available “out of the box.” In this module, we’ll explore functions from the ground up: their purpose, how to create them, and how to write them effectively.\nLearning to work with functions is a core programming skill that offers several key benefits:\nWhether you’re analyzing data sets, automating repetitive tasks, or building scientific applications, functions will become an essential part of your programming toolkit. Let’s dive in and learn how to create them!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#introduction",
    "href": "functions.html#introduction",
    "title": "4  Functions",
    "section": "",
    "text": "It helps you tackle large problems by breaking them into smaller, manageable parts\nIt makes your code more organized and easier to follow\nIt makes your code easier for others to understand and use\nIt eliminates the need to copy and paste code repeatedly\nIt reduces mistakes by creating tested, reliable code blocks you can reuse\n\n\n\nWhat is a Function?\nA function is a reusable block of code that performs a specific task, a sort of standard procedure for performing a particular computation or task in your code.\nFunctions help us organize our code by breaking down complex problems into smaller, more manageable pieces. For example, instead of writing one massive program that does everything, we can create separate functions for each logical step. This makes our code easier to write, understand, and fix when something goes wrong.\nOne important feature of functions is that we can give them descriptive names that explain what they do. When you see a function named calculate_average or find_peak_values, you immediately have a good idea of its purpose. This makes your code more readable and easier to maintain.\nYou might hear functions called by different names like methods, procedures, or subroutines, depending on the programming language or context. Don’t worry too much about these different terms; for now, we will simply call them functions. Regardless of their name, their main purpose remains the same: to package up a set of instructions that we can easily reuse.\nLet’s start with something familiar - mathematical functions. You might remember seeing expressions like \\(f(x) = 2x\\) in math class. This simple function takes any number \\(x\\) and doubles it. Graphically, it creates a straight line where each input \\(x\\) corresponds to an output \\(y\\) that’s twice as large:\n  y\n\n6 │        •\n  │       /\n  │      /\n4 │     •\n  │    /\n  │   /\n1 │  •\n  └─────────── x\n     1  2  3\nWe can create a function with similar behavior in Python:\n\ndef times_2(x):\n    return 2 * x\n\nHere we see a function called times_2 that takes a value x and returns a value that is twice as large (2 * x). Let’s use the seaborn package to visualize some inputs and outputs of the times_2 function:\n\n# Import the seaborn package\nimport seaborn as sns\n\n\n# Create some x values\nxs = [1, 2, 3]\n\n# The y values are results of running\n# the function on all the x values\nys = [times_2(x) for x in xs]\n\n# Print out the y values\nprint(ys)\n\n# Finally, draw a plot of the results.\nsns.relplot(ys, kind=\"line\", aspect=0.5)\n\n[2, 4, 6]\n\n\n\n\n\n\n\n\n\nAt its core, a function is like a machine that follows a specific set of instructions:\n          ┌─────┐\ninput ──► │  ✦  │ ──► output\n          └─────┘\na function ──┘\nYou put something in (input), the function processes it according to its instructions, and you get something out (output).\nAs we will see, Python functions are more flexible than mathematical ones. They can:\n\nTake multiple inputs (or none at all)\nReturn collections of values (or nothing at all)\nPerform actions beyond just calculations (like printing text or saving files)\n\n\n\nWhy Do We Need Functions?\nYou can think of Python functions as pre-packaged units of code that perform specific tasks. When you write a function, you’re creating a reusable tool that you can call whenever you need it. Functions create a logical boundary around related operations, giving them a clear purpose and identity, which can make code more readable, reusable, and maintainable. Here’s a simple example that shows how functions make code more readable:\ngene_expression_data = read_csv(\"gene_expression.csv\")\n\nupregulated_genes = find_upregulated_genes(gene_expression_data)\n\nplot_expression_data(upregulated_genes)\nEven if you don’t know exactly how these functions work internally, you can probably guess what this code does just by reading the function names. When you combine descriptive function names with clear variable names, your code becomes self-documenting, that is, it will be easier to understand without additional comments or explanations.\nHere are some of the advantages and benefits of using functions in programming:\n\nAbstraction\n\nFunctions hide complex operations behind simple, easy-to-use interfaces\nYou can use a function without needing to understand all the details of how it works internally\n\nOrganization\n\nFunctions help break down complex problems into smaller, manageable pieces\nA well-organized series of function calls is much easier to understand than a long block of detailed code\n\nReusability\n\nOnce you write a function, you can use it again in similar contexts\nFunctions can be shared across different projects\nThis saves time and reduces the chance of errors from rewriting the same code\n\nTestability\n\nFunctions make it easier to test your code in small, isolated pieces\nYou can verify each function works correctly on its own\nIf individual functions work properly, you can more confidently combine them into larger programs\n\n\nFunctions are the building blocks of your programs. Each block performs a specific task, and when you combine them thoughtfully, you can create complex programs that solve real-world problems. The key is to make each function clear and focused – if you can understand what each piece does on its own, it becomes much easier to work with them as a whole.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#function-syntax",
    "href": "functions.html#function-syntax",
    "title": "4  Functions",
    "section": "Function Syntax",
    "text": "Function Syntax\nNow that we understand the basic concept of functions, let’s look at how to write them in Python. In a sense, you will need to learn the grammar of writing functions in Python.\nHere’s a simple example that checks if a DNA sequence ends with certain stop codons:\n\ndef ends_with_stop_codon(dna_sequence, stop_codons):\n    \"\"\"Check if the given DNA sequence ends with any of the given stop codons.\"\"\"\n    result = dna_sequence.endswith(stop_codons)\n    return result\n\n\nstop_codons = (\"TAA\", \"TAG\", \"TGA\")\nresult = ends_with_stop_codon(\"ATGAAACCACTGGTGGTTAA\", stop_codons)\nprint(result)\n\nTrue\n\n\nLet’s break down that down:\n\nFunction Definition\n\ndef ends_with_stop_codon(dna_sequence, stop_codons): ...\n\n\nThe def keyword tells Python you’re creating a new function\nends_with_stop_codon is the name you’re giving your function\nInside the parentheses are your parameters (dna_sequence and stop_codons)\nThe colon : marks where the function details begin\n\nRemember to use lowercase letters with underscores for function names (like read_csv, not readCSV).\n\n\nKey Function Components\n\nDocstring\n\nThe text between triple quotes \"\"\"...\"\"\"\nExplains what the function does\n\nFunction body\n\nAll the indented code below the function definition\n\nReturn statement\n\nreturn result\nSpecifies what the function sends back\n\nFunction call\n\nends_with_stop_codon(\"ATGAAACCACTGGTGGTTAA\", stop_codons)\nHow you actually use the function\n\n\n\n\nIndentation Matters\nPython uses indentation to know what code belongs to your function. Everything indented after the function definition is part of that function:\n\ndef hello(name, excitement):\n    \"\"\"Create a greeting message.\"\"\"\n    msg = f\"Hello, {name}\"\n\n    if excitement == \"intense\":\n        msg += \".\"\n    elif excitement == \"happy\":\n        msg += \"!!!\"\n    else:\n        msg += \"!\"\n\n    return msg",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#sec-function-parameters",
    "href": "functions.html#sec-function-parameters",
    "title": "4  Functions",
    "section": "Function Parameters",
    "text": "Function Parameters\nThe most common way of getting data into functions is through function parameters, which hold the data your function needs to work with.\nA quick note about terminology you’ll encounter: You’ll hear people use both “parameters” and “arguments” when talking about functions, and while they’re often used interchangeably, in this tutorial we will make the following distinction:\n\nParameters are the names you list when you’re creating your function. They are the “placeholders” that tell the function what data to expect.\nArguments are the actual values you send to the function when you use it.\n\nFor example, in this function, a and b are the parameters of the add function, and 1 and 2 are arguments to the function when it is called:\n\ndef add(a, b):\n    return a + b\n\n\nadd(1, 2)\n\n3\n\n\nDon’t worry too much about mixing up these terms. The important thing is understanding that functions need a way to receive data, and parameters/arguments are how we do that.\nFunction parameters and arguments are very flexible in Python. Let’s see how we can define and use them.\n\nPositional parameters\nLet’s look at our add function again:\n\ndef add(a, b):\n    return a + b\n\nIn this function a and b are positional parameters. When we call this function, like add(1, 2), Python matches each argument to its parameter based on position – that’s why they’re called “positional parameters”: the first argument (1) goes with the first parameter (a), and the second argument (2) goes with the second parameter (b).\nThis is the most basic and common way to define parameters in Python functions.\n\n\nKeyword Arguments\nCheck out this function that “clamps” a number between an upper and lower bound:\n\ndef clamp(x, min, max):\n    \"Clamp a number between a minimum and a maximum\"\n    if x &lt; min:\n        return min\n    elif x &gt; max:\n        return max\n    else:\n        return x\n\n(For simplicity, we’ll assume users provide valid inputs where min is always less than max.)\nWhile this function works, it has some potential usability issues. When calling this function, it’s not immediately clear which argument goes where. For instance:\n\nShould we put min before max, or vice versa?\nShould the value we’re clamping (x) go first, last, or in the middle?\nLooking at a call like clamp(37, 45, 63), it’s not obvious which number represents what without checking the function definition.\n\nThis is where Python’s keyword arguments come to the rescue! Instead of relying on the order of arguments, we can explicitly label them:\n\nprint(clamp(10, min=25, max=75))\n\n25\n\n\nThe format is straightforward: keyword_arg=value. The real power of keyword arguments is their flexibility. You can use keywords for some arguments but not others, and arrange the arguments in the order that makes the most sense for your use case.\nHere are several valid ways to call the same function:\n\nprint(clamp(10, max=75, min=25))\nprint(clamp(max=75, min=25, x=10))\nprint(clamp(min=25, max=75, x=10))\n\n25\n25\n25\n\n\nAll these calls do exactly the same thing, but the labeled arguments make it clear what each value represents.\n\nRules for Using Keyword Arguments\nWhen using keyword arguments in Python, there are several important rules to follow. Let’s look at some common mistakes and how to avoid them.\nDon’t forget the required arguments:\n\ntry:\n    # Where is the number to clamp?\n    clamp(min=25, max=75)\nexcept TypeError as error:\n    print(error)\n\nclamp() missing 1 required positional argument: 'x'\n\n\nPut the positional arguments before the keyword arguments:\n# (If you try to run this, you will get a SyntaxError.)\n\n# The 10 needs to go first!\nclamp(min=25, max=75, 10)\nDon’t provide the same argument twice:\n\ntry:\n    # Here, we're trying to set 'min' twice\n    clamp(10, 75, min=25)\nexcept TypeError as error:\n    print(error)\n\nclamp() got multiple values for argument 'min'\n\n\nOnly use keyword arguments that the function expects:\n\ntry:\n    # 'number' isn't a parameter name in clamp()\n    clamp(number=10, min=25, max=75)\nexcept TypeError as error:\n    print(error)\n\nclamp() got an unexpected keyword argument 'number'\n\n\nEssentially, these rules exist to prevent ambiguity so that Python knows which arguments go with which parameters.\n\n\n\nDefault Argument Values\nWhen creating functions, you can set default values for their arguments. This means if you call your function without specifying all the arguments, Python will use these preset defaults instead. This essentially gives you a way to create optional functional parameters. Check it out:\n\ndef hello(who=\"World\"):\n    print(f\"Hello, {who}!\")\n\n\nhello(\"Delaware\")\nhello()\n\nHello, Delaware!\nHello, World!\n\n\nIn this case, if no argument is provided when calling hello, Python automatically uses “World” as the default value.\nYou can also use variables to set these default values:\n\nplace = \"World\"\n\n\ndef hello(who=place):\n    print(f\"Hello, {who}!\")\n\n\nhello(\"Delaware\")\nhello()\n\nHello, Delaware!\nHello, World!\n\n\nPython sets these default values when you first define the function, not when you call it. It will look at any variables available at that moment (what we call “in scope”) to determine the default value. If you’re using a variable as a default value, make sure it’s defined before you create your function. We’ll explore the concept of scope in more detail later in the tutorial.\n\nA Common Pitfall: Mutable Default Arguments\nLet’s look at a subtle but important quirk in Python functions that can catch you off guard. Consider this seemingly straightforward function:\n\ndef sneaky_append(number, numbers=[]):\n    numbers.append(number)\n    return numbers\n\nThis function looks like it should do something simple: take a number, add it to a list, and if no list is provided, start with an empty one. Sounds reasonable, right?\nLet’s see what happens when we run it multiple times:\n\nprint(sneaky_append(3))\nprint(sneaky_append(0))\nprint(sneaky_append(2))\n\n[3]\n[3, 0]\n[3, 0, 2]\n\n\nSurprise! Instead of getting three separate lists, each containing one number, we get a single list that keeps growing. What’s going on here?\nThe key is understanding when Python creates that empty list we used as a default argument. Python creates it once when the function is defined, not each time the function is called. This means all calls to the function are sharing the same list!\nHere’s a better way to write this function:\n\ndef cozy_append(number, numbers=None):\n    if numbers is None:\n        numbers = []\n\n    numbers.append(number)\n    return numbers\n\n\nprint(cozy_append(3))\nprint(cozy_append(0))\nprint(cozy_append(2))\n\n[3]\n[0]\n[2]\n\n\nIn this improved version, we use None as our default argument and create a new empty list inside the function each time it’s called. This gives us the behavior we actually wanted: a fresh, empty list for each function call when no list is provided.\nThe takeaway? Be very careful about using mutable objects (like lists, dictionaries, or sets) as default arguments. Instead, use None as your default and create your mutable object inside the function.\n\n\n\nCombining Keyword Arguments & Default Arguments\nKeyword arguments and default arguments look pretty similar. Let’s take a look at them in the same example.\nKeyword arguments can be combined with default arguments as well. Let’s turn the min and max of our clamp function to default arguments.\n\ndef clamp(x, min=25, max=75):\n    \"Clamp a number between a minimum and a maximum\"\n    if x &lt; min:\n        return min\n    elif x &gt; max:\n        return max\n    else:\n        return x\n\n\n# You can override just one default argument.\nprint(clamp(10, min=5))  # Uses the default max=75\n\n# All these calls do the same thing\n\n# Using keyword arguments for the default parameters\nprint(clamp(10, max=75, min=25))\n\n# Using all keyword arguments\nprint(clamp(max=75, min=25, x=10))\n\n# Different order, same result\nprint(clamp(min=25, max=75, x=10))\n\n10\n25\n25\n25\n\n\nThere are a few important points to understand here:\n\nDefault arguments (min=25, max=75 in the function definition) provide fallback values when those arguments aren’t specified in the function call.\nKeyword arguments (like min=5 in the function call) let you specify which parameter you’re passing a value to.\nEven required parameters (like x) can be passed using keyword syntax (x=10), though this isn’t required.\nWhen using keyword arguments, the order doesn’t matter as long as keyword arguments follow any positional arguments.\n\nThis flexibility can be useful when working with functions that have multiple parameters, as it can help make your code more readable and prevent mistakes from mixing up the order of arguments.\n\n\nFunctions That Can Take Any Number of Arguments\nSometimes you want to write a function that can handle different numbers of inputs. Python makes this possible with variable-length arguments. Let’s explore how this works.\n\nTaking Multiple Positional Arguments\nTo specify that a function takes a variable number of positional arguments, you use the *args syntax. The asterisk (*) tells Python to accept any number of arguments and pack them into a tuple. Sometimes you will see these called variadic arguments. Check it out:\n\ndef process_samples(*samples):\n    for sample in samples:\n        print(f\"Processing sample: {sample}\")\n\n\n# Works with any number of arguments\nprocess_samples(\"A1\")  # One sample\nprocess_samples(\"A1\", \"A2\", \"B1\", \"B2\")  # Three samples\n\nProcessing sample: A1\nProcessing sample: A1\nProcessing sample: A2\nProcessing sample: B1\nProcessing sample: B2\n\n\nYou can combine the variadic arguments with positional arguments, as long as the formally specified positional arguments come first:\n\ndef print_stuff(separator, *words):\n    msg = separator.join(words)\n    print(msg)\n\n\nprint_stuff(\", \", \"apple\", \"banana\", \"cherry\", \"domino's pizza\")\n\napple, banana, cherry, domino's pizza\n\n\nThe *words parameter tells Python to collect all remaining arguments into a tuple called words. Having to specify the separator first before all the words you want to print feels a bit awkward to me. We can improve this making separator a keyword argument that comes last:\n\ndef print_stuff(*words, separator):\n    msg = separator.join(words)\n    print(msg)\n\n\nprint_stuff(\"apple\", \"banana\", \"cherry\", \"domino's pizza\", separator=\", \")\n\napple, banana, cherry, domino's pizza\n\n\nWhen you have an argument that follows the variadic arguments, it must be used like a keyword argument. For example, this doesn’t work:\n\ntry:\n    print_stuff(\"apple\", \"banana\", \"cherry\", \"domino's pizza\", \", \")\nexcept TypeError as error:\n    print(error)\n\nprint_stuff() missing 1 required keyword-only argument: 'separator'\n\n\nWithout using a keyword argument, Python has no way of knowing that the final argument you passed into the function shouldn’t be part of the words tuple.\nFinally, it is very common in these situations that a default argument be provided to the argument that comes after a variadic argument list:\n\ndef print_stuff(*words, separator=\", \"):\n    msg = separator.join(words)\n    print(msg)\n\n\nprint_stuff(\"apple\", \"banana\", \"cherry\", \"domino's pizza\")\n\napple, banana, cherry, domino's pizza\n\n\n\nCollections & Variadic Arguments\nHere’s something that might surprise you:\n\nfoods = [\"apple\", \"banana\", \"cherry\", \"domino's pizza\"]\n\ntry:\n    print_stuff(foods)\nexcept TypeError as error:\n    print(error)\n\nsequence item 0: expected str instance, list found\n\n\nWhen we pass a list directly, Python treats it as a single argument. To tell Python to treat each item in the list as a separate argument, we need to “unpack” the list using the * operator:\n\nfoods = [\"apple\", \"banana\", \"cherry\", \"domino's pizza\"]\nprint_stuff(*foods)  # This works!\n\napple, banana, cherry, domino's pizza\n\n\nLet’s look at another example that shows how this behavior can be tricky:\n\ndef greeter(*names):\n    for name in names:\n        print(f\"Hello, {name}!\")\n\n\n# This passes three separate arguments\ngreeter(\"Ryan\", \"Pikachu\", \"Shaq\")\n\n# This passes one argument (a list)\nprint()\ngreeter([\"Ryan\", \"Pikachu\", \"Shaq\"])\n\n# This \"unpacks\" the list again into multiple arguments\nprint()\ngreeter(*[\"Ryan\", \"Pikachu\", \"Shaq\"])\n\nHello, Ryan!\nHello, Pikachu!\nHello, Shaq!\n\nHello, ['Ryan', 'Pikachu', 'Shaq']!\n\nHello, Ryan!\nHello, Pikachu!\nHello, Shaq!\n\n\nCompare that to the version of greeter that takes a single collection argument rather than a variable number of arguments.\n\ndef greeter(names):\n    for name in names:\n        print(f\"Hello, {name}!\")\n\n\n# This passes three separate arguments\ntry:\n    greeter(\"Ryan\", \"Pikachu\", \"Shaq\")\nexcept TypeError as error:\n    print(error)\n\n# This passes one argument (a list)\nprint()\ngreeter([\"Ryan\", \"Pikachu\", \"Shaq\"])\n\n# This \"unpacks\" the list again into multiple arguments\nprint()\n\ntry:\n    greeter(*[\"Ryan\", \"Pikachu\", \"Shaq\"])\nexcept TypeError as error:\n    print(error)\n\ngreeter() takes 1 positional argument but 3 were given\n\nHello, Ryan!\nHello, Pikachu!\nHello, Shaq!\n\ngreeter() takes 1 positional argument but 3 were given\n\n\nAs you see, we had to wrap some of these in try/except blocks since they are using the function incorrectly.\n\n\n\nTaking Multiple Keyword Arguments\nTo specify that a function takes a variable number of keyword arguments, you use the **kwargs syntax. The double asterisk (**) tells Python to accept any number of arguments and pack them into a dictionary.\nLet’s look at an example that creates a simple product description:\n\ndef keyword_example(price, **keywords):\n    print(f\"the price is ${price}\")\n    for keyword, value in keywords.items():\n        print(f\"{keyword:&gt;8s} =&gt; {value}\")\n\n\nkeyword_example(\n    7.99,\n    fruit=\"apple\",\n    dessert=\"tart\",\n    taste=\"yum!\",\n    coolness=\"very\",\n)\n\nthe price is $7.99\n   fruit =&gt; apple\n dessert =&gt; tart\n   taste =&gt; yum!\ncoolness =&gt; very\n\n\nIn this function, price is a regular parameter that must be provided, while **keywords captures any additional keyword arguments as a dictionary. The function then prints each keyword and its value.\nWhen using **kwargs, it must be the last parameter in your function definition. If you try to put other parameters after it, Python will raise an error. (Try it yourself by swapping the order of price and **keywords to see what happens!) Contrast this with the variable length positional arguments, which could come before some keyword-only arguments.\nYou can also do the reverse: take a dictionary and “unpack” it into keyword arguments. Here’s an example:\n\ndef greeter(**greetings):\n    for greeting, people in greetings.items():\n        for person in people:\n            print(f\"{greeting} {person}\")\n\n\ngreeter(hello=[\"Ash\", \"Pikachu\"], goodbye=[\"Gary\", \"Eevee\"])\n\nprint()\n\ngreetings = {\"hello\": [\"Ash\", \"Pikachu\"], \"goodbye\": [\"Gary\", \"Eevee\"]}\ngreeter(**greetings)\n\nhello Ash\nhello Pikachu\ngoodbye Gary\ngoodbye Eevee\n\nhello Ash\nhello Pikachu\ngoodbye Gary\ngoodbye Eevee\n\n\nBoth calls to greeter() produce the same output, showing two different ways to pass keyword arguments to a function.\n\n\n\nCombining Variable-Length Arguments\nPython gives you the flexibility to combine regular parameters with variable-length arguments in the same function. Here’s how it works:\n\ndef example(a, b, *arguments, **keyword_arguments):\n    print(\"a:\", a)\n    print(\"b:\", b)\n    print(\"arguments:\", arguments)\n    print(\"keyword_arguments:\", keyword_arguments)\n\n\nexample(1, 2, 3, 4, 5, hello=\"world\", color=\"orange\")\n\na: 1\nb: 2\narguments: (3, 4, 5)\nkeyword_arguments: {'hello': 'world', 'color': 'orange'}\n\n\nIn this example:\n\na and b are regular parameters that must be provided\n*arguments collects any extra positional arguments (here: 3, 4, 5)\n**keyword_arguments collects any extra name=value pairs (here: hello=\"world\", color=\"orange\")\n\nYou’ll often see these written in a shorter form as *args and **kwargs in Python code:\n\ndef example(a, b, *args, **kwargs): ...\n\nThis is just a common convention - args stands for “arguments” and kwargs for “keyword arguments”. The important part is the * and ** symbols, not the names themselves.\nCheck out this guide, Python args and kwargs: Demystified, if you want to dive deeper into this topic.\n\n\nControlling How Arguments Are Passed\nWhen we call functions in Python, we can typically pass arguments in two ways: by their position in the parameter list or by explicitly naming them with keywords. However, Python also lets you set stricter rules about how arguments should be passed, requiring some to be positional-only or keyword-only. Here is how that looks:\n\ndef example(a, b, /, c, d, *, e, f): ...\n\nThis function’s parameters are divided into three groups:\n\nPositional-only parameters (before the /):\n\na and b must be passed by position\n\nStandard parameters (between / and *):\n\nc and d can be passed either by position or keyword\n\nKeyword-only parameters (after the *):\n\ne and f must be passed by keyword\n\n\nHere are some examples of valid and invalid ways to call the example function:\n\n# Valid calls\n\nexample(1, 2, 3, 4, e=5, f=6)\nexample(1, 2, c=3, d=4, e=5, f=6)\n\n# Invalid calls\n\ntry:\n    example(a=1, b=2, c=3, d=4, e=5, f=6)\nexcept TypeError as error:\n    print(error)\n\n\ntry:\n    example(1, 2, 3, 4, 5, 6)\nexcept TypeError as error:\n    print(error)\n\nexample() got some positional-only arguments passed as keyword arguments: 'a, b'\nexample() takes 4 positional arguments but 6 were given\n\n\nDon’t worry if this syntax seems tricky. While Python offers advanced parameter options, you won’t need them too much when learning. Focus on mastering basic positional and keyword arguments first, as these cover most programming needs. For more details on advanced features, check the Python documentation on special parameters.\n\n\nArgument Evaluation in Function Calls\nWhen you call a function in Python, any expressions you use as arguments are evaluated before the function receives them, so that the function only receives the final result of any expressions. In other words, the function never “sees” the original expression, only its evaluated result. This applies to simple arithmetic, variables, other function calls, or any other valid Python expression. Check it out:\n\ndef print_name(name):\n    print(f\"the name is: '{name}'\")\n\n\nfirst_name = \"Zinedine\"\nlast_name = \"Zidane\"\n\n# A simple string as an argument\nprint_name(\"Lionel\")\n\n# Variables as arguments\nprint_name(first_name)\nprint_name(last_name)\n\n# A complex expression as an argument. Python calculates the complete string\n# first, then passes only the result to the function.\nprint_name(first_name + \" \" + last_name + \"!!\")\n\nthe name is: 'Lionel'\nthe name is: 'Zinedine'\nthe name is: 'Zidane'\nthe name is: 'Zinedine Zidane!!'\n\n\nFunction calls within arguments follow the same rule - they’re evaluated before the outer function receives the result:\n\ndef double(x):\n    return x * 2\n\n\nprint(double(2))\nprint(double(double(2)))\n\nfour = 4\nprint(double(8) * four)\n\n4\n8\n64\n\n\nTo sum it up, Python first evaluates all expressions in your function call, and only then passes these final values to the function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#return-values",
    "href": "functions.html#return-values",
    "title": "4  Functions",
    "section": "Return Values",
    "text": "Return Values\nEvery function in Python can give back (or “return”) a value when it finishes running. We use the return keyword to specify what value we want to get back:\nimport random\n\ndef random_number():\n    return random.random()\n\nprint(random_number())\nprint(random_number())\nLet’s explore some interesting ways we can use return statements.\n\nReturning Multiple Values\nSometimes we want a function to give back more than one piece of information. While Python doesn’t technically return multiple values at once, we can package multiple values into a tuple and return that instead:\n\ndef gc_count(dna_sequence):\n    g_count = dna_sequence.count(\"G\")\n    c_count = dna_sequence.count(\"C\")\n\n    return g_count, c_count\n\n\ndna_sequence = \"ACTGACTG\"\n\n# We can unpack the returned values into separate variables\ng_count, c_count = gc_count(dna_sequence)\nprint(g_count, c_count, sep=\", \")\n\n# Behind the scenes, Python is creating a tuple\nprint(type(gc_count(dna_sequence)))\n\n2, 2\n&lt;class 'tuple'&gt;\n\n\nThis is a convenient way to get multiple values out of a function. Python makes it easy to unpack these values into separate variables that we can use later in our code.\n\n\nFunctions With No Return Value\nWhat happens when a function doesn’t explicitly return anything? Check out this little function that just prints a greeting:\n\ndef say_hello(name):\n    msg = f\"Hello, {name}!\"\n    print(msg)\n\n\nsay_hello(\"Luka\")\n\nHello, Luka!\n\n\nWhen we don’t specify a return value, Python automatically returns a special value called None:\n\nresult = say_hello(\"LeBron\")\nprint(\"the result of `say_hello` is:\", result)\n\nHello, LeBron!\nthe result of `say_hello` is: None\n\n\nNone is Python’s way of representing “nothing” or “no value.” We can also explicitly return None when we want to indicate that a function couldn’t produce a meaningful result. Here’s an example:\n\ndef find_motif(dna_sequence, motif):\n    \"\"\"\n    Find the position of a motif in a DNA sequence.\n\n    Args:\n        dna_sequence (str): DNA sequence to search\n        motif (str): Motif to find\n\n    Returns:\n        position (int or None): Starting position of motif if found, None otherwise\n    \"\"\"\n    position = dna_sequence.find(motif)\n\n    if position == -1:\n        return None\n    else:\n        return position\n\n\nsequence = \"ATCGTATAGCAT\"\nprint(find_motif(sequence, \"TATA\"))\nprint(find_motif(sequence, \"GGGC\"))\n\n4\nNone\n\n\nIn this example, if we can’t find the motif, we return None to indicate that no result was found. This is a pretty common pattern. You’ll see it in built-in methods too, like the dictionary get() method:\n\nd = {\"a\": 1, \"b\": 2}\n\n# Tries to get a value that doesn't exist\nresult = d.get(\"c\")\n\nprint(result)\n\nNone\n\n\nNone is particularly useful when you need to indicate the absence of a value or when a function couldn’t complete its intended task successfully.\n\nAside: Early Returns & Conditional Expressions\nLet’s look at a few different ways to write the same function. We’ll use our motif-finding code as an example:\n\ndef find_motif(dna_sequence, motif):\n    position = dna_sequence.find(motif)\n\n    # Standard if/else\n    if position == -1:\n        return None\n    else:\n        return position\n\nWhile this standard if/else structure works perfectly well, you might encounter two other common patterns when reading Python code:\n\nEarly Return Pattern\nHere is the Early Return Pattern:\n\ndef find_motif_version_2(dna_sequence, motif):\n    position = dna_sequence.find(motif)\n\n    if position == -1:\n        # Exit the function early if no match is found\n        return None\n\n    # Otherwise return the position\n    return position\n\nIn this function, using the early return pattern doesn’t really make that much of a difference, because it is small. You could imagine a larger function that might have a few more checks leading to more nesting of conditional statements. In these cases, the early return pattern can really shine. Check out intentionally complicated example that takes some gene expression data and does some checks on it:\n\n# Without early returns =&gt; complex nesting and logic\ndef process_gene_data(gene_id, sequence, expression_level):\n    if gene_id is not None:\n        if sequence is not None:\n            if len(sequence) &gt;= 50:\n                if expression_level is not None:\n                    if expression_level &gt; 0:\n                        # Actually process the data\n                        processed_data = {\n                            \"id\": gene_id,\n                            \"sequence\": sequence,\n                            \"expression\": expression_level,\n                            \"normalized\": expression_level / 100,\n                        }\n                        return processed_data\n                    else:\n                        return {\"error\": \"Expression level must be positive\"}\n                else:\n                    return {\"error\": \"Missing expression level\"}\n            else:\n                return {\"error\": \"Sequence too short\"}\n        else:\n            return {\"error\": \"Missing sequence\"}\n    else:\n        return {\"error\": \"Missing gene ID\"}\n\nLet’s untangle that mess using the early return pattern:\n\ndef process_gene_data(gene_id, sequence, expression_level):\n    if gene_id is None:\n        return {\"error\": \"Missing gene ID\"}\n\n    if sequence is None:\n        return {\"error\": \"Missing sequence\"}\n\n    if len(sequence) &gt;= 50:\n        return {\"error\": \"Sequence too short\"}\n\n    if expression_level is None:\n        return {\"error\": \"Missing expression level\"}\n\n    if expression_level &lt;= 0:\n        return {\"error\": \"Expression level must be positive\"}\n\n    # At this point, we know the data is valid, so we can process it.\n    processed_data = {\n        \"id\": gene_id,\n        \"sequence\": sequence,\n        \"expression\": expression_level,\n        \"normalized\": expression_level / 100,\n    }\n\n    return processed_data\n\nAgain, this is an intentionally complex example to prove a point about early returns, but the point still stands: the second version is much less complex. It shows how early returns can:\n\nMake input validation clearer and more maintainable\nAvoid deeply nested conditional statements\nSeparate validation logic from the main processing\nMake it easier to add new validation steps later\n\n\n\nConditional Expressions\nThe other pattern is using a conditional expressions (also called a ternary operator) for the return value:\n\ndef find_motif_version_2(dna_sequence, motif):\n    position = dna_sequence.find(motif)\n\n    # Conditional expression\n    return None if position == -1 else position\n\nYou can also flip the condition around if it makes more sense for your specific case:\n\ndef find_motif_version_3(dna_sequence, motif):\n    position = dna_sequence.find(motif)\n\n    # Conditional expression with the check reversed\n    return position if position != -1 else None\n\n\n\nRecap\nAll these versions do exactly the same thing, but they express it in slightly different ways:\n\nThe standard if/else is the most verbose but also the most explicit\nThe early return pattern can make code more readable when you have multiple conditions to check\nThe conditional expression is more compact but might take some getting used to\n\nThere’s no “best” way – they’re all valid Python. However, it’s good practice to be consistent within a single script or project. In situations like these, it can often make sense to pick the style that makes the most sense to you and your colleagues and stick with it.\nCheck out PEP 308 for the full story behind Python’s conditional expressions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#sec-scope",
    "href": "functions.html#sec-scope",
    "title": "4  Functions",
    "section": "Scope & Name Resolution: Where Can You Use Your Variables?",
    "text": "Scope & Name Resolution: Where Can You Use Your Variables?\nScope is a set of rules that determine where you can use different names in your code (like variables, function names, etc.). Scope is essentially the “visibility” of variables in different parts of your code. Let’s look at a simple example to understand this better:\n\ndef some_genes():\n    # Create a list of genes inside the function\n    genes = [\"nrdA\", \"nrdJ\"]\n\n    # We can use `genes` here because we're inside the function\n    # where it was created\n    print(genes)\n\n\n# Let's try to access `genes` outside the function\ntry:\n    print(genes)\nexcept NameError as error:\n    print(error)\n\nname 'genes' is not defined\n\n\nIn this code, genes only exists inside the some_genes function. When we’re working inside that function, genes is “in scope”, meaning we can use it. However, as soon as we step outside the function, genes goes “out of scope”, and we can’t access it anymore.\nWhen does a name come into scope? In Python, this happens when you first create it, which can happen in several ways:\n\nAssigning a value to a variable (x = 1)\nImporting a module (import random)\nDefining a function (def add(): ...)\nCreating function parameters (def add(x, y): ...)\n\nNext, let’s look at how Python decides whether a name is in scope or not.\n\nUnderstanding Scope: How Python Finds Variables\nWhen you write code, Python needs to know where to look for the variables and functions you’re trying to use. It follows a specific search pattern called the LEGB Rule, which defines four levels of scope (think of scope as the “visibility” of variables in different parts of your code).\nLet’s break down these levels from smallest to largest:\n\nLocal Scope (L)\n\nThis is the most immediate scope, created whenever you define a function\nVariables created inside a function only exist inside that function\nEach time you call the function, Python creates a fresh local scope\n\nEnclosing Scope (E)\n\nThis comes into play when you have a function inside another function\nThe outer function’s scope becomes the “enclosing” scope for the inner function\nThe inner function can “see” and use variables from the outer function\nWe’ll explore this more when we discuss nested functions\n\nGlobal Scope (G)\n\nThese are variables defined in the main body of your script\nThey’re accessible throughout your program\n\nBuilt-in Scope (B)\n\nThis is Python’s pre-loaded toolkit\nIncludes all of Python’s built-in functions like print(), len(), and max()\nThese tools are always available, no matter where you are in your code\n\n\nWhen you use a variable name, Python searches through these scopes in order (L → E → G → B) until it finds a match. If it can’t find the name anywhere, you’ll get a NameError.\nLet’s go through each of these in more detail.\n\n\nLocal Scope: What Happens in a Function, Stays in a Function\nWhen we write functions, we create what’s called a “local scope”, which is sort of like the function’s private workspace. Any variables we create inside a function only exist inside that function. Once the function finishes running, these variables disappear, and we can’t access them from outside the function.\nImportant Note about Quarto and Jupyter Notebooks: In our notebook environment, code cells can see variables that were created in previous cells. For our examples to make sense, we need to start fresh. We’ll use a special notebook command called “reset” to clear out any existing variables.\n\n# This is a special notebook command that clears all our previous variables.\n# You won't use this in regular Python programs, it's just for notebooks.\n%reset -f\n\n\ndef example(x):\n    # Here, we can use 'x' because it's a parameter passed to our function\n    # We can also create new variables like 'y'\n    y = 10\n    print(x * y)\n\n\n# This works fine\nexample(2)\n\n# But watch what happens when we try to use these variables outside the\n# function.\n\ntry:\n    # This will be an error because 'x' doesn't exist out here\n    print(x)\nexcept NameError as error:\n    print(error)\n\ntry:\n    # Same problem with 'y': it only existed inside the function\n    print(y)\nexcept NameError as error:\n    print(error)\n\n20\nname 'x' is not defined\nname 'y' is not defined\n\n\nThis behavior is intentional! It helps keep our functions self-contained and prevents them from accidentally interfering with code outside the function. By default, functions can read variables from the global scope (outside the function), but they can’t modify them or create new global variables from inside the function.\n\n\nEnclosing Scope: Functions Within Functions\nSometimes we write functions inside other functions (nested functions). When we do this, the inner function has access to variables defined in the outer function. This relationship creates what we call an “enclosing scope.”\nLet’s look at an example:\n\n%reset -f\n\n\ndef outer():\n    # These variables are accessible throughout outer() and inner()\n    x = 1\n    y = 2\n\n    def inner():\n        # This creates a new 'y' that's different from the outer y\n        y = 10\n        print(f\"from inner -- x: {x}, y: {y}\")\n        return x + y\n\n    # Calculate z using inner()\n    z = inner()\n\n    print(f\"from outer -- x: {x}, y: {y}, z: {z}\")\n\n\nouter()\n\n# Once outer() finishes running, we can't access any of its variables\n# or the inner() function anymore:\n\ntry:\n    x\nexcept NameError as error:\n    print(f\"trying to access 'x' in global scope -- {error}\")\n\ntry:\n    y\nexcept NameError as error:\n    print(f\"trying to access 'y' in global scope -- {error}\")\n\ntry:\n    z\nexcept NameError as error:\n    print(f\"trying to access 'z' in global scope -- {error}\")\n\ntry:\n    inner()\nexcept NameError as error:\n    print(f\"trying to access 'inner' in global scope -- {error}\")\n\nfrom inner -- x: 1, y: 10\nfrom outer -- x: 1, y: 2, z: 11\ntrying to access 'x' in global scope -- name 'x' is not defined\ntrying to access 'y' in global scope -- name 'y' is not defined\ntrying to access 'z' in global scope -- name 'z' is not defined\ntrying to access 'inner' in global scope -- name 'inner' is not defined\n\n\nIn this example, inner() can “see” and use variables defined in outer() because outer() is its enclosing scope. Think of it like inner() being contained within outer()’s environment.\nA few key points:\n\nVariables defined in outer() are accessible inside inner()\nIf inner() creates a variable with the same name as one in outer(), the inner version takes precedence inside inner() (like y in our example)\nOnce outer() finishes running, none of its variables or the inner() function are accessible anymore\n\nThis concept of enclosing scope is useful when you want to create a function that needs access to variables from its surrounding context while keeping those variables private from the rest of your program.\n\n\nGlobal Scope: The World Is Your Variable\nNext up is global scope, which refers to variables that are defined at the top level of your Python script. While we’ll dive deeper into this when we cover modules, here’s what you need to know for now:\n\n%reset -f\n\nname = \"Pikachu\"\n\n\ndef say_hi():\n    print(f\"Hi, {name}!\")\n\n\nsay_hi()\n\nHi, Pikachu!\n\n\nIn this example, name is a global variable because it’s defined outside of any function. Functions can “see” and use global variables, which is why our say_hi() function can access name.\nHowever, there’s an important limitation: functions can’t modify global variables directly. Here’s what happens when we try:\n\n%reset -f\n\nbest_player = \"Messi\"\n\n\ndef update_best_player():\n    best_player = \"Ronaldo\"\n\n\nprint(best_player)\nupdate_best_player()\nprint(best_player)\n\nMessi\nMessi\n\n\nNotice that best_player stays as “Messi” even after we run the function. When we create best_player = \"Ronaldo\" inside the function, we’re actually creating a new, local variable that only exists inside the function. It doesn’t affect the global best_player.\nThere is a way to modify global variables from within a function using the global keyword, though it’s generally not recommended:\n\n%reset -f\n\nbest_player = \"Messi\"\n\n\ndef update_best_player():\n    # This tells Python we want to modify the global variable\n    global best_player\n    best_player = \"Ronaldo\"\n\n\nprint(best_player)\nupdate_best_player()\nprint(best_player)\n\nMessi\nRonaldo\n\n\nWhile using global works, it’s usually better to avoid it. Good programming practice encourage passing values as parameters and returning results from functions instead. This makes your code easier to understand and maintain.\n(There’s also a similar feature called nonlocal for nested functions, but that’s a topic for another day!)\n\nChanging Mutable Values from Within a Function\nLet’s look at how functions can modify certain types of data (like lists) that exist outside the function. This is a common pattern in Python, so it’s important to understand what’s happening.\n\nnumbers = [1, 2]\n\n\ndef append():\n    numbers.append(3)\n\n\nappend()\nprint(numbers)\nappend()\nprint(numbers)\n\n[1, 2, 3]\n[1, 2, 3, 3]\n\n\nThis works because lists in Python are mutable (changeable). Even though the list numbers is defined outside our function, we can still modify it from inside the function because Python is just following a reference to where that list lives in memory.\nHere’s another example that makes this concept more explicit:\n\ndef append_to(numbers, number):\n    print(f\"inside append_to, list id is {id(numbers)}\")\n    numbers.append(number)\n\n\nsome_numbers = [10, 11]\nprint(f\"list {id(some_numbers)}: {some_numbers}\")\n\nappend_to(some_numbers, 12)\nprint(f\"list {id(some_numbers)}: {some_numbers}\")\n\nappend_to(some_numbers, 13)\nprint(f\"list {id(some_numbers)}: {some_numbers}\")\n\nlist 4608799296: [10, 11]\ninside append_to, list id is 4608799296\nlist 4608799296: [10, 11, 12]\ninside append_to, list id is 4608799296\nlist 4608799296: [10, 11, 12, 13]\n\n\nIn this version, we’re being more explicit by passing the list as an argument to our function. The id() function shows us the memory address where our list lives. Notice how this ID stays the same throughout the program – that’s because we’re always working with the same list, just referring to it by different names (some_numbers outside the function and numbers inside it).\nThis behavior highlights an important aspect of how Python handles variables and mutable objects: variables are like name tags that point to data, rather than containers that hold data. When we pass a list to a function, we’re giving that function another name tag that points to the same data. We’ll revisit this topic again when we cover classes later in the course.\n\n\n\nBuilt-In Scope: Python’s Ready-to-Use Tools\nThink of Python’s built-in scope as your basic toolkit that’s always available when you start Python. These are the fundamental tools that Python automatically loads for you, and they’re the last place Python looks when trying to find where a name is defined (remember our LEGB rule).\nYou’ve already been using built-in functions throughout this course. For example:\n\ntotal = sum(range(6))\nprint(total)\n\n15\n\n\nThese functions like sum() and print() aren’t magic, they actually live in a specific place in Python called the builtins module, which contains essential tools like built-in functions, constants, and error types that you can use anywhere in your code.\nYou can see what’s available using Python’s special __builtins__ name:\n\n# Let's look at the first 10 built-in tools\nfor thing in dir(__builtins__)[0:10]:\n    print(thing)\n\nArithmeticError\nAssertionError\nAttributeError\nBaseException\nBaseExceptionGroup\nBlockingIOError\nBrokenPipeError\nBufferError\nBytesWarning\nChildProcessError\n\n\nWhile you normally don’t need to, you can also access these tools directly through the builtins module:\n\nimport builtins\n\ntotal = builtins.sum(builtins.range(6))\nbuiltins.print(total)\n\n%reset -f\n\n15\n\n\nThis explicit way of accessing built-ins isn’t common practice. It’s just helpful to understand where these tools actually live in Python’s structure.\n\n\nLEGB Recap\n┌─ BUILT-IN SCOPE ───────────────────────────────┐\n│ (print, len, str, etc.)                        │\n│ ┌─ GLOBAL SCOPE ─────────────────────────────┐ │\n│ │ global_var = 100                           │ │\n│ │ ┌─ ENCLOSING SCOPE ──────────────────────┐ │ │\n│ │ │ def outer_function():                  │ │ │\n│ │ │     outer_var = 200                    │ │ │\n│ │ │ ┌─ LOCAL SCOPE ──────────────────────┐ │ │ │\n│ │ │ │ def inner_function():              │ │ │ │\n│ │ │ │     local_var = 300                │ │ │ │\n│ │ │ │     print(local_var)               │ │ │ │\n│ │ │ │     print(outer_var)               │ │ │ │\n│ │ │ │     print(global_var)              │ │ │ │\n│ │ │ └────────────────────────────────────┘ │ │ │\n│ │ └────────────────────────────────────────┘ │ │\n│ └────────────────────────────────────────────┘ │\n└────────────────────────────────────────────────┘\n\nBuilt-in Scope: Contains Python’s pre-built functions and tools that are always available\nGlobal Scope: Where your main program variables live\nEnclosing Scope: Contains variables from outer functions\nLocal Scope: Contains variables defined within the current function\n\nWhen your code needs to use a variable, Python looks for it starting from the innermost scope (local) and works its way outward until it finds it. This is why you can use global variables inside functions, but you can’t use local variables outside their functions.\n\n\nSpecial Cases in Python Scope\nWhile we’ve covered the main rules of scope, Python has a few special situations that work a bit differently. Let’s look at three interesting cases that might surprise you.\n\nList Comprehensions Keep Their Variables Private\nWhen you use a list comprehension (that shorthand way to create lists), Python handles its variables differently than regular loops. Here’s what I mean:\n\n%reset -f\n\n# This list comprehension doubles each number from 0 to 4\nnumbers = [x * 2 for x in range(5)]\nprint(numbers)\n\n# Try to access 'x' - it won't work!\ntry:\n    x\nexcept NameError as error:\n    print(error)\n\n[0, 2, 4, 6, 8]\nname 'x' is not defined\n\n\nCompare this to a regular for loop, where the loop variable sticks around:\n\n%reset -f\n\n# Regular for loop doing the same thing\nnumbers = []\nfor x in range(5):\n    numbers.append(x * 2)\nprint(numbers)\n\n# Here we can still use 'x' - it exists!\nprint(x)\n\n[0, 2, 4, 6, 8]\n4\n\n\n\n\nError Variables Stay in Their Block\nWhen handling errors (which we’ll cover more in a later tutorial), variables created in except blocks are only available within that block:\n\ntry:\n    1 / 0\nexcept ZeroDivisionError as error:\n    print(error)  # Works here\n\n# Try to use 'error' later...\ntry:\n    print(error)  # Won't work!\nexcept NameError:\n    print(\"can't get the original 'error' here\")\n\ndivision by zero\ncan't get the original 'error' here\n\n\nThese special cases are part of Python’s design to prevent variables from accidentally leaking into parts of your code where they weren’t intended to be used. While they might seem like quirks at first, they actually help keep your code cleaner and more predictable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#best-practices-for-writing-functions",
    "href": "functions.html#best-practices-for-writing-functions",
    "title": "4  Functions",
    "section": "Best Practices for Writing Functions",
    "text": "Best Practices for Writing Functions\nWhen you’re learning to write functions, you’ll find lots of advice online, some of it contradictory! However, most guidelines share a common goal: making your code easier to understand and maintain. Here are some practical tips to help you write better functions from the start.\n\nUse Clear, Meaningful Names\nFunction names are like labels on laboratory equipment – they should clearly indicate what they do:\n\n# Not very helpful\ndef calc(x, y):\n    return x * y\n\n\n# Much clearer\ndef area(length, width):\n    return length * width\n\nStay consistent with your naming style. Pick one approach and stick with it:\n\n# Inconsistent naming\ndef area(length, width): ...\ndef getPerimeter(l, w): ...\ndef calcvolume(x, y, z): ...\n\n\n# Better -- consistent and clear\ndef area(length, width): ...\ndef perimeter(length, width): ...\ndef volume(length, width, height): ...\n\n\n\nKeep Functions Focused and Concise\nEach function should do one specific task well. If you find your function doing multiple different things, it’s usually better to split it into smaller, more focused functions.\nHere’s an example analyzing a DNA sequence:\n\n# Too many tasks in one function\ndef process_dna_sequence(sequence):\n    has_start = sequence.startswith(\"ATG\")\n    has_stop = sequence.endswith((\"TAA\", \"TAG\", \"TGA\"))\n    gc_content = (sequence.count(\"G\") + sequence.count(\"C\")) / len(sequence)\n    return has_start, has_stop, nucleotide_counts, gc_content\n\n\n# Better: Split into focused functions\ndef has_start_codon(sequence):\n    return sequence.startswith(\"ATG\")\n\n\ndef has_stop_codon(sequence):\n    return sequence.endswith((\"TAA\", \"TAG\", \"TGA\"))\n\n\ndef calculate_gc_percentage(sequence):\n    gc_count = sequence.count(\"G\") + sequence.count(\"C\")\n    return gc_count / len(sequence) * 100\n\n\n\nMake Function Behavior Clear and Predictable\nWhen writing functions, make it obvious what data the function needs to do its job. Pass required data directly to the function rather than having it rely on information defined elsewhere in your code.\n\n# Less clear -- relies on external data\nmeasurements = []\n\n\ndef add_measurement(new_measurement):\n    measurements.append(new_measurement)\n    return sum(measurements) / len(measurements)\n\n\n# Better -- everything the function needs is passed directly\ndef add_measurement(measurements, new_measurement):\n    measurements.append(new_measurement)\n    return sum(measurements) / len(measurements)\n\n\n\nWhen Should You Write a Function?\nConsider writing a function when:\n\nYou have a specific task that you can clearly define\nYou find yourself copying and pasting similar code multiple times\nYou want to make complex operations easier to understand\nYou plan to reuse the same code in different parts of your project\n\n\n\nSummary\nThese guidelines will help you write functions that are easier to understand, test, and maintain. As you gain experience, you’ll develop an intuition for when and how to apply these practices. Remember, the goal is to write code that both you and others can easily work with, even months or years after it was written.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#sec-function-docs",
    "href": "functions.html#sec-function-docs",
    "title": "4  Functions",
    "section": "Function Documentation: Making Your Code Clear and Useful",
    "text": "Function Documentation: Making Your Code Clear and Useful\nYou could think of documentation as leaving helpful instructions for yourself and others about how your code works. While it might seem tedious at first, good documentation is like a roadmap that helps people understand and use your code correctly.\nLet’s look at an example of a function without documentation:\n\ndef math_monster_addition(a, b):\n    if a &gt;= b:\n        return a + b\n    else:\n        return a - b\n\nLooking at this code, you can figure out what it does, but not why it exists or when you should use it. Let’s improve it by adding proper documentation:\n\ndef add_or_subtract(a, b):\n    \"\"\"\n    Performs addition or subtraction based on the relative values of two numbers.\n\n    This function models the Math Monster's arithmetic behavior.  When the\n    first number is bigger (or equal), the monster is happy and adds the\n    numbers.  When the first number is smaller, the monster gets grumpy and\n    subtracts them instead.\n\n    Args:\n        a (numeric): First number\n        b (numeric): Second number\n\n    Returns:\n        numeric: The sum of a and b if a is greater than or equal to b,\n          otherwise returns the difference (a - b).\n\n    Examples:\n        &gt;&gt;&gt; add_or_subtract(5, 3)  #=&gt;  8\n        &gt;&gt;&gt; add_or_subtract(2, 7)  #=&gt; -5\n        &gt;&gt;&gt; add_or_subtract(1, 1)  #=&gt;  2\n    \"\"\"\n    if a &gt;= b:\n        return a + b\n    else:\n        return a - b\n\n\nKey Parts of Good Documentation\n\nSummary Line: A brief, clear statement of what the function does.\nDescription: More detailed explanation of the function’s purpose and behavior.\nArgs: List of parameters with their types and descriptions.\nReturns: What the function gives back and under what conditions.\nExamples: Real-world usage examples showing inputs and outputs.\n\n\n\nDocumentation Style\nWhile there are several ways to format documentation (like Google’s style shown above), what matters most is consistency. Pick one style and stick with it throughout your project. You can explore different styles in Real Python’s documentation guide.\n\n\nWhy Documentation Matters\nGood documentation:\n\nMakes your code more accessible to others\nHelps you remember how your own code works months later\nCan reveal problems in your code design\nMakes your code more maintainable\nEnables automatic documentation generation\n\n\n\nA Note on Writing Documentation\nIf you find it difficult to write clear documentation for a function, it might be a sign that the function is too complex or trying to do too many things. Use documentation as an opportunity to review and potentially simplify your code.\nRemember: The goal isn’t to document every single line of code, but to provide enough information so that someone (including future you) can understand and use your code effectively.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#advanced-topics",
    "href": "functions.html#advanced-topics",
    "title": "4  Functions",
    "section": "Advanced Topics",
    "text": "Advanced Topics\nLet’s explore a few more interesting features of Python functions that you might encounter in your programming journey.\n\nFunction Names and Aliases\nThink of function names as labels. You can create multiple labels (aliases) that point to the same function, similar to how you might have different names for the same thing:\n\ndef add1(x):\n    return x + 1\n\n\n# Create another name for the same function\nadd_one = add1\n\nprint(add1(2))  # Prints 3\nprint(add_one(2))  # Also prints 3\n\n3\n3\n\n\nThis can be useful when you want to use a more context-appropriate name:\n\ndef calculate_tax(amount):\n    return amount * 0.2\n\n\n# Using a more specific name for sales contexts\nsales_tax = calculate_tax\n\nprice = 100\ntax = sales_tax(price)  # Clearer what this represents\n\nEven when two functions do exactly the same thing, Python treats them as distinct objects if they’re defined separately:\n\ndef add1(x):\n    return x + 1\n\n\ndef add_one(x):\n    return x + 1\n\n\nprint(\"add1 points to\", add1)\nprint(\"add_one points to\", add_one)\nprint(\"add1 == add_one is\", add1 == add_one)\n\nprint(f\"add1(2) == {add1(2)}\")\nprint(f\"add_one(2) == {add_one(2)}\")\n\nadd1 points to &lt;function add1 at 0x112b434c0&gt;\nadd_one points to &lt;function add_one at 0x112b43920&gt;\nadd1 == add_one is False\nadd1(2) == 3\nadd_one(2) == 3\n\n\n\n\nFunctions as Objects\nIn Python, functions are objects that you can work with just like numbers or strings. Here’s a practical example using Python’s sorted function:\n\n# Default sorting (alphabetical)\nwords = [\"apple\", \"pie\", \"is\", \"good\"]\nsorted_words = sorted(words)\nprint(sorted_words)\n\n# Sorting by length instead.  Here, `len` is the built-in length function.\nsorted_words = sorted(words, key=len)\nprint(sorted_words)\n\n['apple', 'good', 'is', 'pie']\n['is', 'pie', 'good', 'apple']\n\n\nWe can also write our own functions to customize sorting. Here’s an example with student grades:\n\ndef get_grade(student):\n    return student[1]\n\n\nstudent_grades = [(\"Pikachu\", 97), (\"Charmander\", 91), (\"Bulbasaur\", 86)]\nsorted_student_grades = sorted(student_grades, key=get_grade)\nprint(sorted_student_grades)\n\n[('Bulbasaur', 86), ('Charmander', 91), ('Pikachu', 97)]\n\n\n\n\nLambda Expressions\nSometimes writing a full function is overkill for a simple operation. That’s where lambda expressions come in – they’re tiny, unnamed functions:\nstudent_grades = [(\"Pikachu\", 97), (\"Charmander\", 91), (\"Bulbasaur\", 86)]\nsorted_student_grades = sorted(student_grades, key=lambda student: student[1])\nprint(sorted_student_grades)\nWhile you can store lambda functions in variables, it’s usually better to write a regular function if you plan to reuse the code.\n\n\nType Hints\nYou might see functions written with extra information about their inputs and outputs:\n\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\nThese are called type hints. They help document:\n\nWhat kind of data a function expects (like name: str meaning “name should be a string”)\nWhat kind of data it returns (like -&gt; str meaning “returns a string”)\n\nKey points about type hints:\n\nThey’re optional\nPython doesn’t enforce them automatically\nThey’re fairly common in larger projects\nYou can ignore them while learning Python\n\nType hints are helpful for documentation and code maintenance, but your code will work perfectly fine without them!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#wrap-up",
    "href": "functions.html#wrap-up",
    "title": "4  Functions",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nFunctions are one of the most fundamental and powerful concepts in Python programming. In this tutorial, we’ve covered everything from basic function syntax to advanced topics like variable-length arguments and scope rules. You’ve learned how to write clear, reusable code by packaging logic into well-named functions, how to work with different types of parameters, and how to use return values effectively. We’ve also explored important concepts like variable scope, documentation best practices, and some advanced features like lambda expressions and type hints. With this foundation in functions, you’re now equipped to write more organized, maintainable, and self-documenting Python code that can be easily shared and reused across your bioinformatics projects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#suggested-readings",
    "href": "functions.html#suggested-readings",
    "title": "4  Functions",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nYou might enjoy checking out some of these resources:\n\nPython Docs: 4.8. Defining Functions\nGoogle Python Style Guide: Functions and Methods\nReal Python: Python Scope & the LEGB Rule: Resolving Names in Your Code\nType hints cheat sheet",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html",
    "href": "object_oriented_programming.html",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "",
    "text": "Four Pillars of OOP\nThink about the natural world around you for a moment. You can imagine birds, trees, cells, and countless other entities. Each of these things has specific characteristics and behaviors that help you recognize them. One way to turn these entities into code is by using Classes and Objects.\nIn programming terms, an object is sort of like a specific entity that exists in the real world:\nJust like real-world things, objects have two key aspects:\nFor example, that robin outside your window has a particular color (characteristic) and can sing a specific song (behavior). The E. coli cell in your Petri dish has a particular size (characteristic) and can metabolize lactose (behavior).\nIf objects are the specific things (like a particular robin or E. coli cell) in your domain, then classes are the blueprints or templates that define the characteristics and behaviors of those entities.\nFor instance, the Robin class might specify that all robins have characteristics like species, wingspan, feather color, beak shape, plus behaviors such as flying, singing, and nest-building. But each individual robin instance would have its own specific values for these characteristics and behaviors that depend on those specific characteristics.\nClasses enable us to create our own custom data types that model real-world entities. Instead of just having numbers and strings, we can have Birds, Bacteria, Patients, Proteins, and Molecules, each with their own specialized characteristics and behaviors.\nThis connection between our programs and the real-world domain we are modeling is what makes object-oriented programming powerful. It allows us to represent and manipulate complex biological entities in a way that feels natural and intuitive, mirroring how we already think about these systems in our research.\nFor the rest of this tutorial, we’ll explore how to implement Object-Oriented Programming (OOP) in Python, giving you the tools to represent the biological systems you work with in a natural way.\nWhen learning about object-oriented programming (OOP), you’ll often hear about four fundamental concepts (pillars) that form its foundation. These concepts help us organize code in a way that mirrors how we think about real-world objects and their relationships:\nWe’ll introduce some of these concepts at a practical level in this tutorial and continue to explore them throughout the course.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#sec-four-pillars-oop",
    "href": "object_oriented_programming.html#sec-four-pillars-oop",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "",
    "text": "Encapsulation: bundling data and methods that work on that data into a single unit (a class)\n\nRestricts direct access to some components\nProtects internal state of an object\nHides data\n\nAbstraction: showing only essential features while hiding complicated implementation details\nPolymorphism: objects of different classes responding to the same method in their own ways\nInheritance: creating new classes that receive attributes and methods from existing classes",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#sec-class-basics",
    "href": "object_oriented_programming.html#sec-class-basics",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Classes in Python: Syntax and Structure",
    "text": "Classes in Python: Syntax and Structure\nNow that we have some conceptual framework for object-oriented programming, let’s look at how to write classes in Python. To define a class, we use the class keyword, followed by the name of the class. In the indented section that follows, we can define variables and functions that will be associated with the class and with instances of the class:\n\n# Define a class named \"Robin\".\n#\n# (Class names look LikeThis rather than like_this.)\nclass Robin:\n    # Class Attributes\n    common_name = \"American Robin\"\n    species = \"Turdus migratorius\"\n    beak_shape = \"pointy\"\n\n    # The special method to initialize instances of this class\n    def __init__(self, wingspan, weight, color):\n        self.wingspan = wingspan\n        self.weight = weight\n        self.color = color\n\n    # Define some instance methods that describe the behavior that Robins\n    # will have.\n\n    # Each of these methods will use `id(self)` so that you can see the identity\n    # of the object that it is being called on.\n\n    def fly(self, to):\n        print(f\"robin {id(self)} is flying to {to}!\")\n\n    def sing(self):\n        print(f\"robin {id(self)} is singing!\")\n\n    def eat(self, what):\n        print(f\"robin {id(self)} is eating {what}!\")\n\n    def build_nest(self, where):\n        print(f\"robin {id(self)} is building a nest {where}!\")\n\nCreating an instance of a class looks a lot like calling a function:\n\nrobin = Robin(wingspan=35, weight=80, color=\"gray & reddish brown\")\n\nYou can access the instance attributes using the “dot” syntax:\n\nprint(\n    f\"You see a {robin.color} robin \"\n    f\"that weighs {robin.weight} grams \"\n    f\"with a wingspan of {robin.wingspan} centimeters!\"\n    \"\\nThat's a nice find!!\\n\"\n)\n\nYou see a gray & reddish brown robin that weighs 80 grams with a wingspan of 35 centimeters!\nThat's a nice find!!\n\n\n\nEven though we specified what each of the objects attributes should be when we created it, that doesn’t mean we can’t change them later if we need to. Let’s say our robin eats a worm, and then it gains a little weight afterwards:\n\n# Show the robin's current weight\nprint(f\"before eating the worm, the robin weighs {robin.weight} grams\")\n\n# The robin eats the worm\nrobin.eat(\"a delicious worm\")\n\n# Then it gains 2 grams of weight\nrobin.weight += 2\n\n# Show the robin's weight again\nprint(f\"after eating the worm, the robin weighs {robin.weight} grams\")\n\nbefore eating the worm, the robin weighs 80 grams\nrobin 4388078320 is eating a delicious worm!\nafter eating the worm, the robin weighs 82 grams\n\n\nWe can access the object’s behavior by “calling” its methods:\n\nrobin.sing()\nrobin.fly(to=\"Mexico\")\nrobin.eat(what=\"a worm\")\nrobin.build_nest(where=\"in a tree\")\n\nrobin 4388078320 is singing!\nrobin 4388078320 is flying to Mexico!\nrobin 4388078320 is eating a worm!\nrobin 4388078320 is building a nest in a tree!\n\n\nMultiple distinct instances of the Robin class can be created. Check out how each of them has a different ID:\n\nrobin_1 = Robin(wingspan=35, weight=80, color=\"gray & reddish brown\")\nrobin_2 = Robin(wingspan=32, weight=78, color=\"gray & brownish orange\")\nrobin_3 = Robin(wingspan=36, weight=79, color=\"gray & reddish brown\")\n\nprint(robin_1)\nprint(robin_2)\nprint(robin_3)\n\nrobin_1.sing()\nrobin_2.sing()\nrobin_3.sing()\n\n&lt;__main__.Robin object at 0x10e3634d0&gt;\n&lt;__main__.Robin object at 0x10e363390&gt;\n&lt;__main__.Robin object at 0x105826520&gt;\nrobin 4533400784 is singing!\nrobin 4533400464 is singing!\nrobin 4387398944 is singing!\n\n\nEven though all three of the Robin objects were created from the same class, they are distinct entities in our program. If we change something about one of them, it won’t change the others:\n\nprint(\n    \"before changing weight and color of robin 1,\",\n    \"the weight and color of robin 2 are:\",\n)\nprint(robin_2.weight)\nprint(robin_2.color)\n\n\nrobin_1.weight += 1\nrobin_1.color = \"mostly gray, with some reddish brown\"\n\nprint(\n    \"\\nafter changing weight and color of robin 1,\",\n    \"the weight and color of robin 2 are:\",\n)\nprint(robin_2.weight)\nprint(robin_2.color)\n\nbefore changing weight and color of robin 1, the weight and color of robin 2 are:\n78\ngray & brownish orange\n\nafter changing weight and color of robin 1, the weight and color of robin 2 are:\n78\ngray & brownish orange\n\n\nFinally, we can even give objects completely new attributes if we want to:\n\nrobin_1.favorite_food = \"french fries\"\n\nprint(robin_1.favorite_food)\n\nfrench fries\n\n\nBe careful with this though. That attribute will not be available on all your objects. It will only exist on the specific object where you explicitly added it:\n\ntry:\n    print(robin_2.favorite_food)\nexcept AttributeError as error:\n    print(error)\n\n'Robin' object has no attribute 'favorite_food'\n\n\nNext, let’s dig into some more of the details of creating and using classes in Python.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#initializing-objects",
    "href": "object_oriented_programming.html#initializing-objects",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Initializing Objects",
    "text": "Initializing Objects\nWhen you instantiate an object from a class in Python, it doesn’t simply create an empty shell. Instead, it invokes a special initialization method called __init__ (if defined). This method serves as the object’s constructor, handling the necessary setup to ensure the newly created object is fully functional.\nLet’s make a new class called BlueJay to illustrate some of the details:\n\nclass BlueJay:\n    def __init__(self):\n        self.wingspan = 38  # centimeters\n        self.color = \"blue\"\n\nIn this class, the init method takes only one parameter, self, which refers to the newly created object. It assigns the instance attribute color a default value of “blue”. Take a look:\n\nblue_jay = BlueJay()\nprint(blue_jay)\nprint(blue_jay.wingspan)\nprint(blue_jay.color)\n\n# We can change the value of an instance attribute after the object is created.\nblue_jay.wingspan = 35\nblue_jay.color = \"shockingly blue\"\n\nprint(blue_jay.wingspan)\nprint(blue_jay.color)\n\n&lt;__main__.BlueJay object at 0x1058cc1a0&gt;\n38\nblue\n35\nshockingly blue\n\n\nCreating an object then immediately updating its attributes is such a common operation that Python lets you do it all in one step. This is done by adding additional parameters to the __init__ function:\n\nclass BlueJay:\n    def __init__(self, wingspan, color):\n        self.wingspan = wingspan\n        self.color = color\n\n\nblue_jay = BlueJay(wingspan=36, color=\"bright blue\")\nprint(blue_jay)\nprint(blue_jay.wingspan)\nprint(blue_jay.color)\n\n&lt;__main__.BlueJay object at 0x10e30b230&gt;\n36\nbright blue\n\n\nNow, we need to call BlueJay and provide the color and wingspan arguments. Failing to do so will result in an error:\n\ntry:\n    BlueJay()\nexcept TypeError as error:\n    print(error)\n\nBlueJay.__init__() missing 2 required positional arguments: 'wingspan' and 'color'\n\n\nThe error hints at Python’s inner workings. It mentions that two required arguments are missing: color and wingspan. But hang on, doesn’t __init__ have three parameters?\nIt does! This reveals how Python handles class instantiation: it first creates the object (via __new__ behind the scenes), then initializes it with __init__.\nWhen initializing, Python automatically passes the new object as the first argument (typically called self, but you could name it anything). You only need to provide the remaining arguments – hence the error about missing two arguments, not three.\nRemember that __init__ works like any other Python function. You can use all the parameter options we covered earlier (see Section 4.3), such as default values for parameters like color:\n\nclass BlueJay:\n    def __init__(self, wingspan, color=\"blue\"):\n        self.wingspan = wingspan\n        self.color = color\n\nblue_jay = BlueJay(wingspan=35)\n\nprint(blue_jay.color)\n\nblue\n\n\n\nValidating Inputs\nThe __init__ method plays a crucial role in validating data when creating class instances. For example, if we need to ensure birds can’t have negative wingspans or empty color strings, we can build these checks directly into initialization. When someone creates a bird with invalid data, instead of failing, the code can substitute sensible defaults. This approach guarantees that all instances meet our basic requirements, protecting against bad input:\n\nclass BlueJay:\n    def __init__(self, wingspan, color=\"blue\"):\n        if wingspan &lt; 0:\n            self.wingspan = 0\n        else:\n            self.wingspan = wingspan\n\n        if color == \"\":\n            self.color = \"blue\"\n        else:\n            self.color = color\n\n\nblue_jay = BlueJay(-234, \"\")\nprint(blue_jay.wingspan)\nprint(blue_jay.color)\n\n0\nblue\n\n\nIf you can’t identify a reasonable default value, the most straightforward approach is to simply raise an error. This strategy helps to prevent failures later on.\n\nclass Bird:\n    def __init__(self, species):\n        if species == \"\":\n            raise ValueError(\"species name cannot be blank\")\n\n\ntry:\n    Bird(\"\")\nexcept ValueError as error:\n    print(error)\n\nspecies name cannot be blank",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#attributes",
    "href": "object_oriented_programming.html#attributes",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Attributes",
    "text": "Attributes\nAttributes are the data that is associated with the class and with instances of that class.\n\nInstance Attributes\nWhen initializing a BlueJay object, the __init__ function sets up two attributes: wingspan and color. The self parameter refers to the actual instance being created, so self.wingspan = 1234 creates a wingspan attribute on your new BlueJay object.\nInstance attributes are not shared between different instances of the same class:\n\ntiny_blue_jay = BlueJay(wingspan=28)\nbig_blue_jay = BlueJay(wingspan=40)\n\nprint(tiny_blue_jay.wingspan)\nprint(big_blue_jay.wingspan)\n\n28\n40\n\n\nIf we change one after creation, it will not affect the other:\n\ntiny_blue_jay.wingspan += 1\n\nprint(tiny_blue_jay.wingspan)\nprint(big_blue_jay.wingspan)\n\n29\n40\n\n\nNice! Each object independently manages its state, completely separate from other objects. Just be careful with mutable values in instance attributes though. For example, if you store a bird’s colors as a list (since birds can have multiple colors), you might run into some unexpected behavior:\n\ncolors = [\"blue\", \"white\", \"black\"]\n\n# Create two BlueJay instances with the same color list.\ntiny_blue_jay = BlueJay(wingspan=28, color=colors)\nbig_blue_jay = BlueJay(wingspan=40, color=colors)\nprint(tiny_blue_jay.color)\nprint(big_blue_jay.color)\n\n# Can you guess what will happen if we change one of the colors?\ntiny_blue_jay.color[0] = \"electric blue\"\nprint(\"\\nafter changing color[0] of tiny_blue_jay\")\nprint(tiny_blue_jay.color)\nprint(big_blue_jay.color)\n\n# Or add a color to one of them?\nprint(\"\\nafter appending a new color to big_blue_jay\")\nbig_blue_jay.color.append(\"light blue\")\nprint(tiny_blue_jay.color)\nprint(big_blue_jay.color)\n\n['blue', 'white', 'black']\n['blue', 'white', 'black']\n\nafter changing color[0] of tiny_blue_jay\n['electric blue', 'white', 'black']\n['electric blue', 'white', 'black']\n\nafter appending a new color to big_blue_jay\n['electric blue', 'white', 'black', 'light blue']\n['electric blue', 'white', 'black', 'light blue']\n\n\nI know I just explained that instance attributes are independent between objects, which might seem contradictory here. But remember our discussion about mutable parameters back in Section 4.5.4.1? Python variables are actually references to objects, not the objects themselves. In the example, both BlueJay instances ended up referencing the identical list object. Keep this behavior in mind: it’s a common source of subtle bugs.\nAs mentioned earlier, you can add more instance attributes to an object after creation:\n\nblue_jay = BlueJay(wingspan=35)\n\nblue_jay.sneakiness = \"very sneaky\"\n\nprint(f\"this blue jay is {blue_jay.sneakiness}!\")\n\nthis blue jay is very sneaky!\n\n\n\n\nClass Attributes\nIn our earlier look at the Robin class in Section 5.2, we used class attributes for data shared across all instances. This approach is ideal for information common to all robins, things like common name, species, and beak shape. Class attributes make sense when the data belongs to the entire group rather than to specific individuals.\nClass attributes are defined directly within the class, but outside any methods:\n\nclass BlueJay:\n    common_name = \"Blue Jay\"\n    species = \"Cyanocitta cristata\"\n    beak_shape = \"medium-length, conical\"\n\n\nblue_jay = BlueJay()\n\nprint(blue_jay.species)\nprint(blue_jay.beak_shape)\n\nCyanocitta cristata\nmedium-length, conical\n\n\nYou can also access class attributes directly on the class object itself:\n\nprint(BlueJay.species)\nprint(BlueJay.beak_shape)\n\nCyanocitta cristata\nmedium-length, conical\n\n\nAside: that might seem a bit weird, but in Python, classes themselves are also objects that have properties and methods:\n\nprint(BlueJay.__class__)\n\n&lt;class 'type'&gt;\n\n\nWe will talk more about this later in the course.\nLet’s update the BlueJay class to have both class and instance attributes:\n\nclass BlueJay:\n    # Set class attributes\n    common_name = \"Blue Jay\"\n    species = \"Cyanocitta cristata\"\n    beak_shape = \"medium-length, conical\"\n\n    def __init__(self, wingspan=38, color=\"blue\"):\n        # Set instance attributes\n        self.wingspan = wingspan\n        self.color = color\n\n\ntiny_blue_jay = BlueJay(wingspan=28)\nbig_blue_jay = BlueJay(wingspan=40)\n\n# All blue jays will have the same values for the class attributes, but likely\n# have different values for the instance attributes.\nprint(tiny_blue_jay.wingspan, tiny_blue_jay.species)\nprint(big_blue_jay.wingspan, big_blue_jay.species)\n\n28 Cyanocitta cristata\n40 Cyanocitta cristata\n\n\n\nModifying Class Attributes\nClass attributes don’t have to be constant, unchanging things. Let’s look at an example where we change the value of a class attribute from within an instance method to create sequential IDs for instances of that class. Check it out:\n\nclass Amoeba:\n    # This is a class attribute\n    counter = 0\n\n    def __init__(self, name=None):\n        # We increment the value stored in the counter class attribute by 1.\n        Amoeba.counter += 1\n\n        # Then, we set that value to the value of this amoeba instance's `id`\n        # attribute.\n        self.id = Amoeba.counter\n\n        # If the user doesn't specify a name, then we create a default name\n        # that includes the ID.\n        if name is None:\n            self.name = f\"Amoeba_{self.id}\"\n        else:\n            self.name = name\n\n\namoeba_1 = Amoeba()\namoeba_2 = Amoeba(name=\"Bob the Amoeba\")\namoeba_3 = Amoeba()\n\nprint(amoeba_1.name)\nprint(amoeba_2.name)\nprint(amoeba_3.name)\n\nAmoeba_1\nBob the Amoeba\nAmoeba_3\n\n\nPretty neat! We will go into more fancy details like this in a future tutorial.\n\n\nA Tricky Example\nIn our last example, we contained mutations within class methods—a safer approach than external state modification, which often causes bugs. Let’s flip this and see what happens when we modify class variables from outside. Warning: it gets a bit confusing!\n\nprint(\"tiny_blue_jay species:\", tiny_blue_jay.species, id(tiny_blue_jay.species))\nprint(\"big_blue_jay species:\", big_blue_jay.species, id(big_blue_jay.species))\nprint()\n\ntiny_blue_jay.species = \"i don't know!\"\n\nprint(\"tiny_blue_jay species:\", tiny_blue_jay.species, id(tiny_blue_jay.species))\nprint(\"big_blue_jay species:\", big_blue_jay.species, id(big_blue_jay.species))\nprint()\n\nBlueJay.species = \"something else\"\n\nprint(\"tiny_blue_jay species:\", tiny_blue_jay.species, id(tiny_blue_jay.species))\nprint(\"big_blue_jay species\", big_blue_jay.species, id(big_blue_jay.species))\nprint()\n\nanother_blue_jay = BlueJay()\nprint(\n    \"another_blue_jay species:\",\n    another_blue_jay.species,\n    id(another_blue_jay.species),\n)\n\ntiny_blue_jay species: Cyanocitta cristata 4388541616\nbig_blue_jay species: Cyanocitta cristata 4388541616\n\ntiny_blue_jay species: i don't know! 4388305072\nbig_blue_jay species: Cyanocitta cristata 4388541616\n\ntiny_blue_jay species: i don't know! 4388305072\nbig_blue_jay species something else 4388382768\n\nanother_blue_jay species: something else 4388382768\n\n\nLet’s break this down:\n\nInitially, all BlueJay instances share the class attribute species with value \"Cyanocitta cristata\" (note the matching IDs in the first two lines).\nWhen we set tiny_blue_jay.species = \"i don't know!\", we’re not changing the class attribute, we’re creating a new instance attribute that shadows it. The ID changes for tiny_blue_jay but stays the same for big_blue_jay.\nWith BlueJay.species = \"something else\", we modify the actual class attribute. This affects all instances that don’t have their own shadowing attribute—big_blue_jay sees the new value, but tiny_blue_jay still shows its instance-specific value.\nAny new instance (like another_blue_jay) gets the updated class attribute value.\n\nThe apparent complexity stems from Python’s attribute lookup sequence:\n\nCheck the instance namespace first\nThen check the class namespace\nFinally, check parent classes\n\nThis enables both shared values and individual customization with the same name. This is very flexible, but potentially confusing if you don’t understand the lookup mechanism.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#methods",
    "href": "object_oriented_programming.html#methods",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Methods",
    "text": "Methods\nMethods are functions inside classes that usually work with instances of that class. Class methods exist too (like class attributes), but we’ll skip those for now.\nOur previous examples were pretty basic. Let’s look at something a bit more interesting to showcase why classes and objects are actually useful.\n\nclass Amoeba:\n    \"\"\"\n    Represent an amoeba with position and energy tracking.\n\n    This class models an amoeba that can move around in a 2D space and manage\n    its energy levels. Each amoeba has a unique ID, position coordinates, and\n    energy value.\n\n    Attributes:\n        counter (int): Class variable that keeps track of how many amoeba instances\n            have been created.\n        id (str): Unique identifier for each amoeba instance.\n        position (list): A list of two integers representing [x, y] coordinates.\n        energy (int): Current energy level of the amoeba.\n    \"\"\"\n\n    counter = 0\n\n    def __init__(self, position=None, energy=5):\n        Amoeba.counter += 1\n        self.id = f\"Amoeba #{Amoeba.counter}\"\n\n        if position is None:\n            self.position = [0, 0]\n        else:\n            self.position = position\n\n        self.energy = energy\n\n    # This method controls how Amoebas will be printed\n    def __str__(self):\n        return f\"{self.id} -- Position: {self.position}, Energy: {self.energy}\"\n\n    def move(self, direction):\n        \"\"\"Move the amoeba, consuming energy.\"\"\"\n\n        if self.energy &lt;= 0:\n            print(f\"{self.id} is too weak to move!\")\n            return\n\n        if direction == \"right\":\n            print(f\"{self.id} moves right!\")\n            # Moving to the right means adding one to the x position\n            self.position[0] += 1\n        elif direction == \"left\":\n            print(f\"{self.id} moves left!\")\n            # Moving to the left means subtracting one from the x position\n            self.position[0] -= 1\n        elif direction == \"up\":\n            print(f\"{self.id} moves up!\")\n            # Moving up means adding one to the y position\n            self.position[1] += 1\n        elif direction == \"down\":\n            print(f\"{self.id} moves down!\")\n            # Moving down means subtracting one from the y position\n            self.position[1] -= 1\n        else:\n            raise ValueError(\"direction must one of up, down, left, or right\")\n\n        self.energy -= 1\n\n    def eat(self):\n        \"\"\"The amoeba eats, increasing its energy.\"\"\"\n        print(f\"{self.id} eats\")\n        self.energy += 2\n\nThere are a lot of things to break down about the Amoeba class. Let’s look at a few key points:\n\nClass attributes vs. Instance attributes:\n\ncounter is a class attribute shared across all instances\nid, position, and energy are instance attributes unique to each object\n\nConstructor Implementation:\n\nUses __init__ to set up each new amoeba with its initial state\nAutomatically increments the counter to assign unique IDs\nHandles default parameters (position=None, energy=5)\nSets a default position if none is provided\n\nString Representation:\n\nImplements __str__ to provide a human-readable representation\nReturns formatted string containing the amoeba’s ID, position and energy\n\n`move() Method Implementation:\n\nmove() validates input parameters\nChecks current state before performing actions (if self.energy &lt;= 0)\nDemonstrates internal state modification (changing position and reducing energy)\nShows error handling with a descriptive error message (ValueError) for bad inputs\n\n2D Movement Representation:\n\nUses a list [x, y] to represent position in 2D space\nAdjusts coordinates based on movement direction\n\nState Management:\n\nClass methods track and update the amoeba’s internal state (position, energy)\nBehavior depends on internal state (the amoeba can’t move with 0 energy)\n\nCode Organization:\n\nUses docstrings for class and method documentation\nFollows consistent indentation and naming conventions (methods names are short verbs)\n\n\nNow, let’s try it out!\n\nimport random\n\n# These are the four directions that an amoeba can move\nfour_directions = [\"up\", \"down\", \"left\", \"right\"]\n\n# Seed the random generator so that we get the same result each time we run the code\nrandom.seed(37424)\n\n# Generate a random \"walk\"\ndirections = random.choices(four_directions, k=10)\n\n# Create a new amoeba instance\namoeba = Amoeba()\n\n# Go through each of the moves one by one\nfor direction in directions:\n    print(amoeba)\n\n    # Each turn, the amoeba has a 1/5 chance in eating some lunch\n    if random.random() &lt; 0.2:\n        amoeba.eat()\n\n    # Then the amoeba tries to move\n    amoeba.move(direction)\n    print()\n\nAmoeba #1 -- Position: [0, 0], Energy: 5\nAmoeba #1 eats\nAmoeba #1 moves down!\n\nAmoeba #1 -- Position: [0, -1], Energy: 6\nAmoeba #1 moves down!\n\nAmoeba #1 -- Position: [0, -2], Energy: 5\nAmoeba #1 moves left!\n\nAmoeba #1 -- Position: [-1, -2], Energy: 4\nAmoeba #1 eats\nAmoeba #1 moves down!\n\nAmoeba #1 -- Position: [-1, -3], Energy: 5\nAmoeba #1 moves up!\n\nAmoeba #1 -- Position: [-1, -2], Energy: 4\nAmoeba #1 moves down!\n\nAmoeba #1 -- Position: [-1, -3], Energy: 3\nAmoeba #1 moves up!\n\nAmoeba #1 -- Position: [-1, -2], Energy: 2\nAmoeba #1 moves left!\n\nAmoeba #1 -- Position: [-2, -2], Energy: 1\nAmoeba #1 moves up!\n\nAmoeba #1 -- Position: [-2, -1], Energy: 0\nAmoeba #1 is too weak to move!\n\n\n\n\nAside: Refactoring the Amoeba Class\nAfter building our Amoeba class, we can see that some functionality, specifically position tracking and movement, isn’t necessarily Amoeba-specific. We’re currently using a two-element list for position and updating it within the Amoeba’s move method. While this works for our small class, let’s extract this common behavior into a dedicated Position class.\n\nclass Position:\n    \"\"\"\n    Represents a position in 2D space.\n\n    This class handles tracking and updating a position in a 2D grid system,\n    with methods for moving in cardinal directions.\n\n    Attributes:\n        x (int): The x-coordinate\n        y (int): The y-coordinate\n    \"\"\"\n\n    def __init__(self, x=0, y=0):\n        self.x = x\n        self.y = y\n\n    def __str__(self):\n        return f\"({self.x}, {self.y})\"\n\n    def move_right(self):\n        \"\"\"Move one unit to the right (increase x).\"\"\"\n        self.x += 1\n\n    def move_left(self):\n        \"\"\"Move one unit to the left (decrease x).\"\"\"\n        self.x -= 1\n\n    def move_up(self):\n        \"\"\"Move one unit up (increase y).\"\"\"\n        self.y += 1\n\n    def move_down(self):\n        \"\"\"Move one unit down (decrease y).\"\"\"\n        self.y -= 1\n\nNow that we have the Position class, we can use it in the Amoeba class instead of the original two-element list. In this way, the Amoeba class delegates the behavior of position and movement to the Position class rather than manage that itself.\n\nclass Amoeba:\n    \"\"\"\n    Represent an amoeba with position and energy tracking.\n\n    This class models an amoeba that can move around in a 2D space and manage\n    its energy levels. Each amoeba has a unique ID, position coordinates, and\n    energy value.\n\n    Attributes:\n        counter (int): Class variable that keeps track of how many amoeba instances\n            have been created.\n        id (str): Unique identifier for each amoeba instance.\n        position (Position): A Position object representing the amoeba's location.\n        energy (int): Current energy level of the amoeba.\n    \"\"\"\n\n    counter = 0\n\n    def __init__(self, position=None, energy=5):\n        Amoeba.counter += 1\n        self.id = f\"Amoeba #{Amoeba.counter}\"\n\n        if position is None:\n            self.position = Position()\n        else:\n            self.position = position\n\n        self.energy = energy\n\n    def __str__(self):\n        return f\"{self.id} -- Position: {self.position}, Energy: {self.energy}\"\n\n    def move(self, direction):\n        \"\"\"Move the amoeba, consuming energy.\"\"\"\n\n        if self.energy &lt;= 0:\n            print(f\"{self.id} is too weak to move!\")\n            return\n\n        if direction == \"right\":\n            print(f\"{self.id} moves right!\")\n            self.position.move_right()\n        elif direction == \"left\":\n            print(f\"{self.id} moves left!\")\n            self.position.move_left()\n        elif direction == \"up\":\n            print(f\"{self.id} moves up!\")\n            self.position.move_up()\n        elif direction == \"down\":\n            print(f\"{self.id} moves down!\")\n            self.position.move_down()\n        else:\n            raise ValueError(\"direction must one of up, down, left, or right\")\n\n        self.energy -= 1\n\n    def eat(self):\n        \"\"\"The amoeba eats, increasing its energy.\"\"\"\n        self.energy += 1\n\nThere’s a new syntax element to note: self.position.move_right(). This expression chains multiple “dots” to connect attributes and method calls, something you’ll see frequently in Python. You can read it left to right like this:\n\n\n\nA schematic of chained method calls\n\n\nExtracting the Position class from the Amoeba class brings a couple of nice benefits:\n\nSeparation of concerns: Each class now has a single responsibility\nReusability\n\nThe Position class could now be used for other entities that need position tracking.\nIf we need to change how positions work, we only need to change one place rather than in every class that need position tracking.\n\nEncapsulation: Position management details are hidden inside the Position class\n\nThis process is called a refactoring, and in this case, we have shown how to spot shared functionality and extract it into a separate class. While this is a good skill to have in your toolkit, don’t feel that you have to separate everything. Sometimes it’s overkill, especially for one-off scripts or when code reuse is unlikely. Use your judgment!\n\n\nSpecial Methods\nSpecial methods (like __init__ and __str__) let your custom classes work with Python’s built-in operations. These “magic methods” act as interfaces between your code and core Python functionality. Though they have a cool name, these methods aren’t mysterious, rather, they’re like standardized hooks that let your classes interact more easily with Python’s built-in functions.\nFor example, when Python performs certain operations, it looks for certain methods:\n\nWhen using the + operator, Python looks for the __add__ method\nWhen using the len() function, Python looks for the __len__ method\nWhen initializing an object, Python looks for the __init__ method\nWhen printing an object, Python looks for the __str__ method\n\nBy implementing these methods, your custom objects can behave like Python’s native types. For example, a DNA sequence class with __add__ could allow sequence concatenation using the simple + operator, just like with strings.\nIn the Amoeba and Position classes, we implemented __str__. The __str__ method is called whenever Python needs a human-readable string representation of your object, such as when you use the print() function.\nThe __str__ method should return a concise, informative string that gives the user meaningful information about the object. If you don’t define a __str__ method for your class, Python will use a default representation. Compare the output when printing an Amoeba instance versus when printing a BlueJay instance:\n\nprint(Amoeba())\nprint(BlueJay())\n\nAmoeba #1 -- Position: (0, 0), Energy: 5\n&lt;__main__.BlueJay object at 0x10e666650&gt;\n\n\nSince we didn’t explicitly define a __str__ method for the BlueJay class, Python prints a default representation of the object, rather than a nice, informative message.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#duck-typing",
    "href": "object_oriented_programming.html#duck-typing",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Duck Typing",
    "text": "Duck Typing\nThe duck test goes something like this:\n\nIf it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck!\n\nMany programming languages, including Python, use this idea for something we call duck typing. In this context, Python is essentially saying, “Well, I don’t know what class this object is, but if it can swim or fly or quack, then I will treat it like a Duck.” In other words, it only cares whether the object can perform the needed actions. If it can, then Python will happily work with it regardless of what it actually is. This practical approach represents one way Python implements the polymorphism pillar of OOP mentioned earlier.\nLet’s see an example of this in action:\n\nclass Duck:\n    def quack(self):\n        print(\"quack, QUACK!!\")\n\n    def fly(self):\n        print(\"the duck is flying!\")\n\n    def swim(self):\n        print(\"the duck is swimming\")\n\n\nclass Gull:\n    def squawk(self):\n        print(\"squawk!!!\")\n\n    def fly(self):\n        print(\"the crow flies!\")\n\n    def swim(self):\n        print(\"the gull swims near the beach\")\n\n\nclass Whale:\n    def swim(self):\n        print(\"the whale is swims underwater\")\n\nAll three of these classes have a method called swim. If we have a collection of those objects, we can call the swim method on each of them without having to know what type the object is:\n\nanimals = [Duck(), Duck(), Gull(), Whale()]\n\nfor animal in animals:\n    animal.swim()\n\nthe duck is swimming\nthe duck is swimming\nthe gull swims near the beach\nthe whale is swims underwater\n\n\nWhile convenient, you might not always know if your objects have the method you need. There are two common ways to deal with this:\n\nSimply call the method and catch any exceptions that occur. (the Python docs call this EAFP style programming – “Easier to ask for forgiveness than permission”)\nCheck if the object has the method before attempting to call it. (the Python docs call this LBYL style programming – “look before you leap”)\n\nBoth approaches have their place, depending on your specific situation and coding style preferences. Let’s look at examples of both approaches.\n\nAsking for Forgiveness\nIn this example, we call the method fly on each animal and catch any exceptions that occur using try/except.\n\nanimals = [Duck(), Duck(), Gull(), Whale()]\n\nfor animal in animals:\n    try:\n        animal.fly()\n    except AttributeError as error:\n        print(error)\n\nthe duck is flying!\nthe duck is flying!\nthe crow flies!\n'Whale' object has no attribute 'fly'\n\n\n\n\nLooking Before You Leap\nIn this example, we use hasattr (short for “has attribute”) to check if each animal has the fly attribute before attempting to call the method.\n\nanimals = [Duck(), Duck(), Gull(), Whale()]\n\nfor animal in animals:\n    if hasattr(animal, \"fly\"):\n        animal.fly()\n    else:\n        print(f\"{animal} can't fly!\")\n\nthe duck is flying!\nthe duck is flying!\nthe crow flies!\n&lt;__main__.Whale object at 0x105910050&gt; can't fly!\n\n\n\n\nSpecial Methods & Duck Typing\nLet’s return once more to Python’s special “magic methods”. These are a great example of how duck typing can be useful. If we implement special methods like __str__ or __len__ for our custom classes, then Python can treat them like built-ins. Here’s an example of a Gene class:\n\nclass Gene:\n    def __init__(self, name, sequence):\n        self.name = name\n        self.sequence = sequence\n\n    # Special method for string representation\n    def __str__(self):\n        return f\"&gt;{self.name}\\n{self.sequence}\"\n\n    # Special method for length\n    def __len__(self):\n        return len(self.sequence)\n\n\ngene = Gene(\"awesome_gene\", \"ATGATATCCATCGCTACTAGACTACTACGCGCGGCTCT\")\n\nprint(len(gene))\nprint(gene)\n\n38\n&gt;awesome_gene\nATGATATCCATCGCTACTAGACTACTACGCGCGGCTCT\n\n\nIn this example, our __str__ method formats the gene to match FASTA file conventions, while the length method returns the sequence length. These additions help our Gene class behave more like native Python objects, seamlessly integrating with the language’s expectations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#standard-python-classes",
    "href": "object_oriented_programming.html#standard-python-classes",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Standard Python Classes",
    "text": "Standard Python Classes\nIn Chapter 2, we introduced built-in Python data structures like lists, dictionaries, and sets. Now we can recognize these as classes that follow the principles we’ve just learned: they combine data and behavior into objects with methods that help us work with them safely and effectively.\nLet’s look at a few examples of how built-in Python classes provide methods that operate on their internal data:\n\n# List methods\nmy_list = [1, 2, 3, 4]\n\n# Adds an element to the end\nmy_list.append(5)\n\n# Reverses the list in-place\nmy_list.reverse()\n\nprint(my_list)\n\n# String methods\ntext = \"  hello world  \"\n\n# Strip whitespace\nprint(text.strip())\n\n# Strip whitespace, then uppercase\nprint(text.strip().upper())\n\n# Strip whitespace, then uppercase, then replace capital L with lowercase l\nprint(text.strip().upper().replace(\"L\", \"l\"))\n\n\ngene_info = {\"name\": \"some_gene\", \"chromosome\": 10}\n\n# Print the keys\nprint(gene_info.keys())\n\n# \"Merge\" the dictionaries\ngene_info.update({\"function\": \"tumor suppressor\"})\n\nprint(gene_info)\n\n[5, 4, 3, 2, 1]\nhello world\nHELLO WORLD\nHEllO WORlD\ndict_keys(['name', 'chromosome'])\n{'name': 'some_gene', 'chromosome': 10, 'function': 'tumor suppressor'}\n\n\nOther commonly used built-in classes include:\n\nset – for handling collections of unique items\nfile objects – returned when opening files with the open() function\ndatetime – for working with dates and times\n\nEach of these classes encapsulates both data (like the items in a list) and behavior (methods like append or sort), just like the custom classes we wrote in this tutorial.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#object-oriented-thinking",
    "href": "object_oriented_programming.html#object-oriented-thinking",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Object-Oriented Thinking",
    "text": "Object-Oriented Thinking\nNow that you have seen the mechanics of building and using classes in Python, let’s get a little meta. When thinking in an object-oriented way, you see your system in terms interacting entities – an approach that fits naturally with how we like to think about biological systems. Here’s a quick guide to developing an object-oriented mindset:\n\nIdentify key entities\n\nWhat are your main working pieces?\nPhysical things like cells, proteins, genes, patients?\nAbstract concepts like Files, Queues, Nodes, Graphs?\n\nDetermine their properties\n\nWhat characteristics define them?\nExample: cells have size, type, metabolic rate\n\nIdentify their actions\n\nWhat can they do?\nCells divide, proteins fold, birds fly, files can be read\n\nGroup related functionality: cell properties and behaviors belong in a Cell class\nSplit classes when they become unwieldy or overly complex\n\nStart with these steps as your foundation. As you gain experience, you’ll naturally develop a feel for modeling systems that works best for your specific projects.\n\nBalancing Approaches\nPython’s flexibility lets you mix programming approaches within a single project or script.\nGood Python code often combines different paradigms:\n\nObject-oriented for modeling biological entities where data and behavior are linked\nFunctional for data transformations and analysis\nProcedural for simple sequential operations\n\nLet the problem you’re tyring to solve guide your programming approach. Sometimes a simple module with functions works better than a class, while other times a well-designed class hierarchy models your domain in the clearest way.\nIn general, classes work well when you’re working with entities that have both data (attributes) and behaviors (methods) that naturally belong together. Consider using classes when:\n\nYou need multiple similar entities with shared structure and behavior (like 1000 cells in a simulation)\nYou repeatedly apply the same operations to specific data types\nYour data needs validation (ensuring DNA sequences only contain valid nucleotides)\nData and actions are tightly coupled\nYou need to track state across operations\nYour project needs additional structure\nYour data has natural hierarchies (DNA and Proteins are both Molecules)\n\nNote: We didn’t get into class hierarchies in this tutorial. That’s an advanced topic for a later time.\nConsider simpler alternatives when:\n\nWriting straightforward data processing scripts\nDealing with mostly static data\nUsing functions that transform data without maintaining state\nWorking with data where built-in Python structures are sufficient\nYour project is small and classes would add unnecessary complexity\n\nMany popular data science libraries like pandas and seaborn emphasize data transformations and function chaining rather than object-oriented approaches. When using these libraries, follow their patterns instead of forcing objects where they don’t make sense.\nThe goal is clear, maintainable code that you and colleagues understand. Start simple, adding complexity only when needed. With experience, you’ll develop intuition for when classes are the right tool.\n\n\nGeneral Tips and Pitfalls\nLet’s wrap up with some broad best practices and common pitfalls for working with classes. We’ll refine these as you tackle more complex programs throughout the course.\nGeneral tips:\n\nFollow naming conventions\n\nPascalCase for classes (BacterialCell), snake_case for methods/attributes (growth_rate)\n\nKeep classes focused\n\nOne clear purpose per class\nAvoid mixing responsibilities\n\nDesign intuitive interfaces\n\nConsider how others will interact with your classes\nMake correct usage easy, incorrect usage difficult\n\nDocument with docstrings\nValidate inputs in __init__\nImplement magic methods when appropriate\n\nGeneral pitfalls:\n\nModeling excessive behavior\n\nInclude only what’s relevant to your specific application\nA bird has many attributes, but you only need what serves your research question\n\nOverloaded classes\n\nA Cell class shouldn’t also process sequencing data and generate plots\n\nUnderutilized classes\n\nData-only classes might work better as dictionaries\nMethod-only classes might work better as function modules\n\nPoor naming choices\n\nclass Gene communicates purpose better than class X",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#wrap-up",
    "href": "object_oriented_programming.html#wrap-up",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nPython classes create custom data types that bundle related data and functions together, helping organize code by grouping attributes and methods. In scientific programming, they’re great for representing complex entities with intertwined data and behavior. Use classes when they make your code more organized and readable – with experience, you’ll develop instincts for when to use them versus simpler approaches. Keep exploring, stay curious, and experiment with different solutions to your scientific problems!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "object_oriented_programming.html#suggested-readings",
    "href": "object_oriented_programming.html#suggested-readings",
    "title": "5  Introduction to Object-Oriented Programming",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nYou might enjoy checking out some of these resources:\n\nPython Docs: Classes\nReal Python\n\nObject-Oriented Programming (OOP) in Python\nPython Class Constructors: Control Your Object Instantiation\nClass and Instance Attributes\nDuck Typing in Python: Writing Flexible and Decoupled Code\n\nWikipedia’s Object-oriented programming",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Object-Oriented Programming</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html",
    "href": "errors_exceptions.html",
    "title": "6  Errors & Exceptions",
    "section": "",
    "text": "Syntax Errors\nErrors and unexpected situations happen in nearly all programs, even simple ones. It’s important to understand that errors are a normal part of programming, and there are good techniques to help us write code that can handle common errors effectively.\nThis chapter explores errors and exceptions and how to manage them in your Python code. We’ll begin with basic errors like syntax mistakes (similar to grammar errors in writing) or mathematical issues like dividing by zero. Then we’ll progress to more advanced techniques that help your programs handle unexpected situations gracefully.\nThroughout this guide, you’ll see practical examples that show how to:\nBy the end, you’ll feel more confident writing code that can recover from problems rather than crashing when something unexpected happens. This skill is essential for creating robust and reliable programs.\nSyntax errors are problems that happen when you write Python code that breaks the language’s grammar rules. These errors are very common when you’re first learning Python because you’re still getting used to how Python code should be structured. Here is an example of a syntax error:\nIf you run this Python code, you would get an error message like this:\nAs you can see, I forgot to put the colon (:) at the end of the first line. This is a classic syntax mistake in Python, similar to forgetting a period at the end of a sentence.\nRemember in previous tutorials and assignments, like when we discussed removing items from dictionaries (Section 2.6.2.3), scope (Section 4.5), and the “ask for forgiveness” approach (Section 5.6.1), we used try/except blocks to “catch” errors. However, syntax errors cannot be caught using try/except. This is because the Python interpreter checks your code’s syntax before running any of it. The syntax error is detected during this checking phase, before your program even starts executing, so the try/except block never gets a chance to run.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#syntax-errors",
    "href": "errors_exceptions.html#syntax-errors",
    "title": "6  Errors & Exceptions",
    "section": "",
    "text": "for i in range(10)\n    print(i)\n\nfor i in range(10)\n                  ^\nSyntaxError: expected ':'",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#understanding-python-exceptions",
    "href": "errors_exceptions.html#understanding-python-exceptions",
    "title": "6  Errors & Exceptions",
    "section": "Understanding Python Exceptions",
    "text": "Understanding Python Exceptions\nErrors can still appear in your code even when the syntax is correct. In Python, these runtime errors are called exceptions. If an exception happens and you don’t address it in your code, your program will crash and stop running.\n\nCommon Exceptions\nThere are many different exceptions in Python. Let’s start with two simple ones: NameError and TypeError.\n\nName Errors\nA NameError occurs when Python can’t find a name you’re trying to use. This typically happens when you try to use a variable or function that doesn’t exist or hasn’t been defined yet.\nx = 1 + y\nIf you ran this code in the Python interpreter or from a program, you would see an error message like this:\nNameError             Traceback (most recent call last)\nFile 06_errors.qmd:1\n----&gt; 1 x = 1 + y\n\nNameError: name 'y' is not defined\nThis error message shows the type of error, the file where it happened, and a helpful explanation of the problem. We will go into more detail about reading error messages later in this tutorial.\n\n\n\n\n\n\nTip 6.1: Stop & Think\n\n\n\n\n\nLook at the following code:\ntry:\n    print(gene)\nexcept NameError as error:\n    print(f\"{error=}\")\nWhat do you think will happen and why?\n\n\n\n\n\nType Errors\nA TypeError happens when you try to perform an operation on a data type that doesn’t support that operation. (It’s sort of like trying to use a lab technique on the wrong type of sample.)\n47 + \"102\"\nRunning this code produces the following error:\nTypeError              Traceback (most recent call last)\nFile 06_errors.qmd:1\n----&gt; 1 47 + \"102\"\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\nThis message tells you that Python doesn’t allow you to add an integer and a string together.\n\n\nOther Common Exceptions\nSome other exceptions you might encounter include:\n\nIndexError: When you try to access a position beyond the end of a list or string (like trying to access the 5th nucleotide in a 3-nucleotide codon)\nFileNotFoundError: When Python can’t find the file or directory you’re trying to access\nKeyError: When you try to access a key that doesn’t exist in a dictionary (similar to looking up a gene ID that isn’t in your database)\nAttributeError: When you try to access features or properties that don’t exist for a particular object\nValueError: When you provide the right type of value but with incorrect content, such as:\n\nTrying to use a square root transformation on a sample with a negative value (math.sqrt(-1))\nTrying to convert na expression values (float(\"na\"))\n\n\nFor a complete list of Python exceptions, see the Built-in Exceptions section of the Python manual.\n\n\n\n\n\n\nTip 6.2: Stop & Think\n\n\n\n\n\nIf you were writing a program to read and parse FASTA files, which type of error would occur if the file you were trying to read did not exist?\n\n\n\n\n\n\nReading Error Messages\nOne challenge many beginners face is understanding error messages, both how to interpret them and how to fix the problems they indicate.\nAs a new programmer, you’ll likely encounter different types of errors than experienced programmers do. You’ll often see syntax errors or errors from misusing Python’s language features or misunderstanding how functions and classes work. This can be particularly frustrating because these aren’t errors you might anticipate. While you might have expected a user-provided file name might not exist, you probably would not have anticipated using a built-in function incorrectly!\nLearning to read error messages is an essential skill for your programming journey. In Python, error messages typically contain:\n\nError type: The kind of error that occurred (e.g., RuntimeError)\nTraceback: The sequence of function calls that led to the error\nFile location: Where the error occurred (file name and line number)\nArrow pointer: Points to the specific part of code causing the error\nError message: A description of what went wrong\n\nLet’s examine a few error messages to better understand their structure and meaning. We will start with a simple example:\ndef wibble():\n    raise RuntimeError(\"oh no!!\")\n\n\ndef wobble():\n    wibble()\n\n\ndef woo():\n    wobble()\n\n\nwoo()\nIf you ran this code in a Quarto notebook, you would get an error that looks like this:\nRuntimeError              Traceback (most recent call last)\nCell In[2], line 13\n      9 def woo():\n     10     wobble()\n---&gt; 13 woo()\n\nCell In[2], line 10, in woo()\n      9 def woo():\n---&gt; 10     wobble()\n\nCell In[2], line 6, in wobble()\n      5 def wobble():\n----&gt; 6     wibble()\n\nCell In[2], line 2, in wibble()\n      1 def wibble():\n----&gt; 2     raise RuntimeError(\"oh no!!\")\n\nRuntimeError: oh no!!\nIn this example, the process went like this:\n\nFirst, the program called woo()\nThen, inside woo(), it called wobble()\nNext, inside wobble(), it called wibble()\nFinally, inside wibble(), a RuntimeError occurred\n\nYou can read the traceback in two ways:\nTop-to-bottom (the order the code executed):\n\nOn line 13, woo() is called\nOn line 10, in woo, wobble() is called\nOn line 6, in wobble, wibble() is called\nOn line 2, in wibble, raise RuntimeError is called, which crashes the program\n\nBottom-to-top (starting with the actual error):\n\nOn line 2, in wibble, raise RuntimeError is called, which crashes the program\nOn line 6, in wobble, wibble() is called\nOn line 10, in woo, wobble() is called\nOn line 13, woo() is called\n\nExamining error tracebacks in both directions will often help you better understand what went wrong in your code.\nLet’s look another example:\ndef parse_line(line):\n    gene, sample, expression = line.strip().split(\",\")\n\n    return (gene, sample, float(expression))\n\n\ndef read_expression_data(filename):\n    with open(filename) as f:\n        for line in f:\n            gene, sample, expression = parse_line(line)\n            print(f\"{gene}-{sample} =&gt; {expression}\")\n\n\nread_expression_data(\"expression_data.csv\")\nPretend there is a file called expression_data.csv that has the following contents:\ngene1,sample1,25\ngene1,sample2,50\ngene2,sample1,na\ngene2,sample2,15\nLet’s assume that you have saved that code in a script called example.py. When you run it you would see output that looks something like this:\ngene1-sample1 =&gt; 25.0\ngene1-sample2 =&gt; 50.0\nTraceback (most recent call last):\n  File \"example.py\", line 14, in &lt;module&gt;\n    read_expression_data(\"expression_data.csv\")\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n  File \"example.py\", line 10, in read_expression_data\n    gene, sample, expression = parse_line(line)\n                               ~~~~~~~~~~^^^^^^\n  File \"example.py\", line 4, in parse_line\n    return (gene, sample, float(expression))\n                          ~~~~~^^^^^^^^^^^^\nValueError: could not convert string to float: 'na'\nLet’s break it down:\nThe script processes gene expression data from a CSV file. It runs smoothly for the first two lines of the file, but crashes when it encounters \"na\" in the third line. The error happens because Python’s float() function cannot convert the text \"na\" (which stands for “not available” in biological data) into a floating-point number.\nThe error message shows us the exact path of execution that led to the problem. It starts at line 14 where we call read_expression_data(), then moves to line 10 where we call parse_line(), and finally reaches line 4 where the actual error occurs when trying to convert \"na\" to a float.\nThis is a common issue when working with biological datasets, which often contain missing values represented as “na”, “N/A”, or similar placeholders. To fix this problem, we would need to add error handling when running the float function, or use some other technique to check for these special values before attempting the conversion.\n\n\n\n\n\n\nTip 6.3: Stop & Think\n\n\n\n\n\nIn the above example, can you think of a better way to handle the na value rather than letting the program crash?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#basic-exception-handling",
    "href": "errors_exceptions.html#basic-exception-handling",
    "title": "6  Errors & Exceptions",
    "section": "Basic Exception Handling",
    "text": "Basic Exception Handling\nNow that we’ve discussed what exceptions are, let’s explore how we can recover from them and prevent unexpected errors from crashing our programs.\nThe basic format uses try: followed by an indented block of code, then except SomeError:, followed by another indented block of code:\n\ntry:\n    47 + \"102\"\nexcept TypeError as error:\n    print(f\"an type error occurred: {error=}\")\n\nan type error occurred: error=TypeError(\"unsupported operand type(s) for +: 'int' and 'str'\")\n\n\nLet’s break this down:\n\nThe “try clause” contains the code that might cause an error.\n\nIt is placed between the try and except keywords.\nHere, our try clause has just one expression: 47 + \"102\"\n\nThe “except clause” contains the code that runs if an error occurs.\n\nIt is placed after the except keyword and continues until the indentation ends.\n\nexcept TypeError as error:\n\nTypeError specifies which kind of error we want to catch\nIf a TypeError happens, Python saves the error information in the variable named error so we can use it\n\nprint(f\"a TypeError occurred: {error}\") is the code that runs when a TypeError occurs in the try clause\n\nThe try/except statement is a bit like a safety net for your code: you’re trying something that might fail (the try clause), but you’ve prepared a backup plan (the except clause) just in case.\nNext, let’s see how Python runs through this type of code.\n\nTry/Except Code Flow\nPython follows a specific process when it encounters a try/except block. First, it attempts to run all the code inside the “try clause.” If this code runs without any problems, Python simply skips the except clause and continues with the rest of your program.\nHowever, if an error occurs while running the try clause, Python immediately stops executing that section. It then checks if the error type matches what you specified after the except keyword. For instance, if you wrote except TypeError:, Python looks specifically for TypeErrors.\nIf the error matches what you specified, Python runs the code in the except clause and then continues with the rest of your program.\nIf the error doesn’t match what you specified, Python considers it an “unhandled exception”. In this case, your program will stop running and display an error message.\nThis might sound a bit abstract, so let’s go through some examples to see how it works step by step.\n\nNo Exception in the Try Clause\nIn this example, no exception occurs in the try clause.\n\ntry:\n    print(\"hi\")\nexcept TypeError as error:\n    print(f\"an error occurred: {error=}\")\n\nprint(\"yo!\")\n\nhi\nyo!\n\n\n\nPython runs the contents of the try clause: print(\"hi\")\nprint(\"hi\") runs without error and displays “hi” on the screen\nSince no error occurred, Python skips the except clause completely\nPython continues to the next line and runs print(\"yo!\")\n\n\n\nException in the Try Clause\nIn this example, an error occurs that matches the one in our except clause:\n\ntry:\n    print(greeting)\nexcept NameError as error:\n    print(f\"an error occurred: {error=}\")\n\nprint(\"yo!\")\n\nan error occurred: error=NameError(\"name 'greeting' is not defined\")\nyo!\n\n\n\nPython runs the contents of the try clause: print(greeting)\nprint(greeting) causes a NameError because greeting hasn’t been defined (you must define variables before using them)\nSince a NameError occurred and we specifically included NameError in our except statement, Python executes the except block: print(f\"an error occurred: {error=}\")\nAfter completing the try/except block, Python continues to the next line and runs print(\"yo!\")\n\n\n\nNon-Matching Exception in the Try Clause\nThis example shows what happens when the error that occurs is different from the one we’re trying to catch:\ntry:\n    47 + \"102\"\nexcept NameError as error:\n    print(f\"an error occurred: {error=}\")\n\nprint(\"yo!\")\n\nPython runs the contents of the try clause: 47 + \"102\"\n47 + \"102\" causes a TypeError because Python can’t add a number to text\nPython checks if TypeError matches what we’re catching in our except statement, but we’re only catching NameError\nSince the error types don’t match and there are no other try statements, the error remains uncaught\nThe program crashes with an error message and stops running\nprint(\"yo!\") never runs because the program already crashed\n\n\n\n\nCatching Multiple Exceptions\nYou can catch different types of exceptions within a single try/except block by adding multiple except clauses. When an error occurs in the try block, Python looks for a matching except block to handle that specific error type. Once it finds a match, it runs that code and then continues with the rest of your program. If no match is found, then the program crashes.\nFor example, if your code causes a NameError (like using a variable that doesn’t exist), only the except clause that handles NameError will run. The other except clauses, like one for TypeError, will be skipped:\n\ntry:\n    print(greeting)\nexcept NameError as name_error:\n    print(f\"a NameError occurred: {name_error=}\")\nexcept TypeError as type_error:\n    print(f\"a TypeError occurred: {type_error=}\")\n\na NameError occurred: name_error=NameError(\"name 'greeting' is not defined\")\n\n\nIn this case, it is the opposite – the code in the try clause causes a TypeError, causing only the expect clause handling type errors to run:\n\ntry:\n    47 + \"102\"\nexcept NameError as name_error:\n    print(f\"a NameError occurred: {name_error}\")\nexcept TypeError as type_error:\n    print(f\"a TypeError occurred: {type_error}\")\n\na TypeError occurred: unsupported operand type(s) for +: 'int' and 'str'\n\n\nSometimes you might want to handle different errors in the same way. In these cases, you can group multiple exceptions in a single except clause:\n\ntry:\n    print(greeting)\nexcept (NameError, TypeError) as error:\n    print(f\"an error occurred: {error=}\")\n\ntry:\n    47 + \"102\"\nexcept (NameError, TypeError) as error:\n    print(f\"an error occurred: {error=}\")\n\nan error occurred: error=NameError(\"name 'greeting' is not defined\")\nan error occurred: error=TypeError(\"unsupported operand type(s) for +: 'int' and 'str'\")\n\n\n\n\nClauses Can Contain Multiple Statements\nEach section (clause) in a try/except block can include multiple lines of code:\n\ntry:\n    # This line will run successfully\n    print(1 + 2)\n\n    # This line will cause a TypeError (mixing number and text)\n    print(10 + \"20\")\n\n    # This line will never execute because the error above stops the try block\n    print(100 + 200)\nexcept TypeError as error:\n    # This line runs because we caught a TypeError from above\n    print(f\"an error occurred: {error=}\")\n\n    # This line also runs since all code in the except clause executes\n    # (unless another error happens)\n    print(\"this will also run!\")\n\n3\nan error occurred: error=TypeError(\"unsupported operand type(s) for +: 'int' and 'str'\")\nthis will also run!\n\n\nNote: We’ll discuss this more later, but it’s generally best practice to keep the code in each clause short and simple, with as few statements as possible.\n\n\nExceptions Can Happen in the Except Block\nOne important thing to remember is that exceptions can also occur in your except block code:\ntry:\n    # This line will run\n    print(1 + 2)\n\n    # This line will raise the TypeError\n    print(10 + \"20\")\n\n    # This line will not run because the previous line caused a TypeError\n    print(100 + 200)\nexcept TypeError as error:\n    # This line will run because a TypeError occurred in the above clause\n    print(f\"an occurred: {error=}\")\n\n    # This line will raise a NameError since the name `twenty` has not been\n    # defined\n    print(10 + twenty)\n\n    # This line will not run because the above line raises another exception!\n    print(\"this will also run!\")\nIf you ran this code, you would see output like this:\nTypeError                                 Traceback (most recent call last)\nCell In[1], line 6\n      5 # This line will raise the TypeError\n----&gt; 6 print(10 + \"20\")\n      8 # This line will not run because the previous line caused a TypeError\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\nDuring handling of the above exception, another exception occurred:\n\nNameError                                 Traceback (most recent call last)\nCell In[1], line 16\n     12 print(f\"an occurred: {error=}\")\n     14 # This line will raise a NameError since the name `twenty` has not been\n     15 # defined\n---&gt; 16 print(10 + twenty)\n     18 # This line will not run because the above line raises another exception!\n     19 print(\"this will also run!\")\n\nNameError: name 'twenty' is not defined\nNotice the key message: “During handling of the above exception, another exception occurred”. This tells us what happened. While the program was trying to recover from one error, it encountered a second error. Since there was no error handler set up for this second error, the program crashed.\nFor more information about this concept, check out the Python documentation on exception chaining.\n\n\n\n\n\n\nTip 6.4: Stop & Think\n\n\n\n\n\nIf you’re developing a program to analyze biological sequencing data, can you think of a reason why allowing exceptions to occur within an except block might lead to tricky issues in your code?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#advanced-exception-handling",
    "href": "errors_exceptions.html#advanced-exception-handling",
    "title": "6  Errors & Exceptions",
    "section": "Advanced Exception Handling",
    "text": "Advanced Exception Handling\nLet’s explore some more advanced ways to handle exceptions in Python. While you might not need to use these techniques in your current assignments and miniprojects, it’s still valuable to understand them. You’ll encounter these patterns when reading other people’s code or working with existing Python libraries and tools.\n\nException Hierarchies\nPython exceptions form a hierarchy, and are organized a bit like a family tree. Here’s a simplified view of this hierarchy:\nBaseException\n └── Exception\n      ├── ArithmeticError\n      │    ├── FloatingPointError\n      │    ├── OverflowError\n      │    └── ZeroDivisionError\n      ├── RuntimeError\n      │    ├── NotImplementedError\n      │    ├── PythonFinalizationError\n      │    └── RecursionError\nTo understand this relationship:\n\nA ZeroDivisionError is a type of ArithmeticError\nAn ArithmeticError is a type of Exception\nAn Exception is a type of BaseException\n\nThis is similar to how taxonomic relationships work. Just as a cat is a feline, all felines are mammals, and all mammals are animals, a ZeroDivisionError is an ArithmeticError, all ArithmeticErrors are Exceptions, and all Exceptions are BaseExceptions.\nNote that a ZeroDivisionError is an ArithmeticError, but it is not a RuntimeError – similar to how cats and dogs are both mammals, but a cat is not a canine.\nThe complete exception hierarchy is is available in the Python documentation.\nThe benefit of this hierarchy is that we can catch a whole group of related errors without listing each one individually. Here’s a simple example with a function that performs basic math operations:\n\nimport math\n\ndef silly_math(x, y):\n    math.exp(x) / y\n\nThis function can raise several different errors:\n\nIf y is 0, we get a ZeroDivisionError\nIf x is too large, we get an OverflowError\nIf x or y aren’t numbers, we get a TypeError\n\nImagine we’re reading the values of x and y from a data file, so we don’t know what they’ll be until the program runs. We can handle potential errors like this:\n\n# ZeroDivisionError\ntry:\n    x = 1\n    y = 0\n    silly_math(x, y)\nexcept (ZeroDivisionError, OverflowError) as error:\n    print(f\"there was an arithmetic error! {error=}\")\nexcept TypeError as error:\n    print(f\"one of the values wasn't numeric! {error=}\")\n\n\n# OverflowError\ntry:\n    x = 1000\n    y = 2\n    silly_math(x, y)\nexcept (ZeroDivisionError, OverflowError) as error:\n    print(f\"there was an arithmetic error! {error=}\")\nexcept TypeError as error:\n    print(f\"one of the values wasn't numeric! {error=}\")\n\n\n# TypeError\ntry:\n    x = 1\n    y = \"2\"\n    silly_math(x, y)\nexcept (ZeroDivisionError, OverflowError) as error:\n    print(f\"there was an arithmetic error! {error=}\")\nexcept TypeError as error:\n    print(f\"one of the values wasn't numeric! {error=}\")\n\nthere was an arithmetic error! error=ZeroDivisionError('float division by zero')\nthere was an arithmetic error! error=OverflowError('math range error')\none of the values wasn't numeric! error=TypeError(\"unsupported operand type(s) for /: 'float' and 'str'\")\n\n\nThis works, but we can simplify by using the parent class ArithmeticError instead:\n\n# ZeroDivisionError\ntry:\n    x = 1\n    y = 0\n    silly_math(x, y)\nexcept ArithmeticError as error:\n    print(f\"there was an arithmetic error! {error=}\")\nexcept TypeError as error:\n    print(f\"one of the values wasn't numeric! {error=}\")\n\n\n# OverflowError\ntry:\n    x = 1000\n    y = 2\n    silly_math(x, y)\nexcept ArithmeticError as error:\n    print(f\"there was an arithmetic error! {error=}\")\nexcept TypeError as error:\n    print(f\"one of the values wasn't numeric! {error=}\")\n\nthere was an arithmetic error! error=ZeroDivisionError('float division by zero')\nthere was an arithmetic error! error=OverflowError('math range error')\n\n\nBoth ZeroDivisionError and OverflowError are caught by the ArithmeticError handler because they are both types of ArithmeticError.\nHowever, it’s important to understand that this doesn’t work in reverse. Let’s see what happens if we try to catch an ArithmeticError with a ZeroDivisionError handler:\ntry:\n    # This code cause a specific error, in this case an ArithmeticError,\n    # to happen\n    raise ArithmeticError(\"oops!\")\nexcept ZeroDivisionError:\n    print(\"this won't catch the ArithmeticError\")\nHere is the error message:\nArithmeticError                  Traceback (most recent call last)\nCell In[1], line 2\n      1 try:\n----&gt; 2     raise ArithmeticError(\"oops!\")\n      3 except ZeroDivisionError:\n      4     print(\"this won't catch the ArithmeticError\")\n\nArithmeticError: oops!\nThis is because while every ZeroDivisionError is an ArithmeticError, every ArithmeticError is not a ZeroDivisionError.\nNote: check out the raise statement for more about manually raising exceptions.\nYou may have noticed that this connects back to the Object-Oriented Programming concepts we discussed in Section 5.1. This error class hierarchy shows inheritance at work: exception classes inherit from their parent exception classes. When we catch an ArithmeticError, we’re using this inheritance relationship to handle any type of exception that belongs to that family. This is a practical example of why inheritance is useful in programming.\n\n\n\n\n\n\nTip 6.5: Stop & Think\n\n\n\n\n\nConsider a function that reads a FASTA file. What exception types might you want to catch, and which parent exception class could catch all of them?\n\n\n\n\n\nNesting Try/Except Blocks\nIt can sometimes be useful to nest try/except blocks. This is like having backup plans for your backup plans!\nThe first example shows how Python searches for an appropriate error handler. If an inner error handler doesn’t match the exception type, Python will check outer handlers:\n\ntry:\n    try:\n        raise TypeError(\"oh no!\")\n    except NameError:\n        print(\"this won't print because it's trying to catch a NameError\")\nexcept TypeError:\n    print(\"caught a TypeError\")\n\ncaught a TypeError\n\n\nIn this case, the inner handler is looking for a NameError, but we raised a TypeError. Since the inner handler can’t catch it, Python checks the outer handler, which successfully catches the TypeError.\nOur second example is different. Here, the inner handler does catch the TypeError, but then it raises a new NameError:\n\ntry:\n    try:\n        raise TypeError(\"oh no!\")\n    except TypeError:\n        print(\"we caught a TypeError\")\n\n        raise NameError(\"here is a name error\")\n\n        print(\"this will not print!\")\nexcept NameError:\n    print(\"caught a NameError\")\n\nwe caught a TypeError\ncaught a NameError\n\n\nThe inner handler catches the TypeError but then creates a new problem by raising a NameError. Fortunately, the outer handler catches this new error, preventing our program from crashing.\n\n\nUsing else and finally\nWhen working with try/except blocks, we can add two special clauses that give us more control over our code: else and finally.\nThe else clause lets us run code only if no errors occurred in the try block. As in, “try this code, and if it works without errors, do this extra step.”\nThe finally clause runs its code regardless of whether an error happened or not. It’s like saying “no matter what happens, always do this cleanup step.” This is a good choice for tasks that need to happen even if errors occur like closing files or database connections.\nThe else clause is useful for several reasons:\n\nIt keeps your error handling code separate from your normal processing code\nIt ensures certain operations only happen when everything works correctly\nIt prevents catching unrelated errors that might occur in your processing code\n\nWhile the else clause isn’t very common in Python code, it serves specific purposes. One important use is running additional code before any finalization steps without including it in the try block itself. Without using else, you’d have to put this code in the try block, which means unintended errors might get caught and handled incorrectly.\nUnlike finally (which always runs), the else clause only runs when the try block succeeds completely. This makes it useful for operations that should only happen when everything works as expected.\nHere’s a simple example showing how else can make your code cleaner:\n\ndef example(): ...\ndef handle_failure(error): ...\ndef handle_success(result): ...\n\ntry:\n    result = example()\n    # Make a boolean flag that says we were successful\n    success = True\nexcept MagicError as error:\n    handle_failure(error)\n\n# We only want to run this on success, so it must be behind a flag\nif success:\n    handle_success(result)\n\n\n# With else -- cleaner approach\ntry:\n    result = example()\nexcept MagicError as error:\n    handle_failure(error)\nelse:\n    handle_success(result)\n\nLet’s try an example that uses all of the clauses: try, except, else, and finally. This simple example shows how to handle different situations when searching a protein database. The different parts handle specific situations:\n\ndef find_protein(protein_database, entry):\n    try:\n        # Try to find the protein in the database.\n        info = protein_database[entry]\n    except KeyError:\n        # If it is not found, log an error.\n        print(f\"Entry '{entry}' not found in the database\")\n    else:\n        # If it is found, return the info\n        return info\n    finally:\n        # Regardless of success or failure, log a message saying you checked for the entry.\n        print(f\"Search completed for entry {entry}\")\n\n\n# Example usage\nprotein_database = {\n    \"P00452\": {\n        \"gene\": \"nrdA\",\n        \"protein\": \"Ribonucleoside-diphosphate reductase 1 subunit alpha\",\n    },\n    \"P00582\": {\"gene\": \"polA\", \"protein\": \"DNA polymerase I\"},\n}\n\nentries = [\"P00582\", \"P19822\", \"P00452\"]\nfor entry in entries:\n    # If the entry is found, return the info, if not, it will return None\n    info = find_protein(protein_database, entry)\n\n    # Since this might be none, we have to check that it exists before working\n    # with it.\n    if info:\n        print(info[\"gene\"], info[\"protein\"], sep=\" -&gt; \")\n\n    print()\n\nSearch completed for entry P00582\npolA -&gt; DNA polymerase I\n\nEntry 'P19822' not found in the database\nSearch completed for entry P19822\n\nSearch completed for entry P00452\nnrdA -&gt; Ribonucleoside-diphosphate reductase 1 subunit alpha\n\n\n\nHere is a flowchart to help you visualize how the logic flows through the try/except/else/finally code structure.\n\n\n\ntry/except/else/finally flowchart\n\n\n\n\n\n\n\n\nTip 6.6: Stop & Think\n\n\n\n\n\nCan you think of any scenarios in which the finally clause would be useful when working with biological data or analysis?\n\n\n\n\n\nCombining Techniques\nLet’s take a look at an example that combines a few of the techniques we have talked about so far. Here we’re trying to read a file, convert each line to an integer, and store those integers in a list. Our code is structured with a try clause followed by several except clauses to handle different types of errors:\n\ndef read_integers(file_name):\n    \"\"\"Read a file and convert each line to an integer.\"\"\"\n    with open(file_name) as file:\n        return [int(line.strip()) for line in file]\n\n\ntry:\n    read_integers(\"the_best_numbers.txt\")\n# Dealing with files can cause errors in the OSError family\nexcept OSError as error:\n    print(f\"an error reading the file occurred: {error=}\")\n# Trying to a convert a string to an integer can fail too\nexcept ValueError as error:\n    print(f\"could not convert line to an integer: {error=}\")\n# This is a \"catch-all\" clause, since we want to log any unexpected errors\nexcept Exception as error:\n    print(f\"Unexpected error: {error=}\")\n    # Since it's not good practice to handle errors we are not expecting,\n    # we reraise the error and then the caller of this code can handle it\n    # how they want.\n    raise\n\nan error reading the file occurred: error=FileNotFoundError(2, 'No such file or directory')\n\n\nThe first except clause catches any OSError. These errors happen when something goes wrong with the file system, like if \"the_best_numbers.txt\" doesn’t exist. The OSError family includes specific errors like FileNotFoundError (when the file doesn’t exist), PermissionError (when you don’t have permission to read the file), and other file-related problems.\nThe second except clause catches ValueError. This happens if the int() function can’t convert a line to an integer. For instance, if a line contains “ABC” instead of a number like “123”, a ValueError will occur.\nThe last except clause catches any other type of Exception. This acts as a safety net for unexpected errors. When an unexpected error occurs, we:\n\nPrint information about the error\nRe-raise the error using the raise statement\n\nRe-raising an error means that after we handle it partially (in this case, by printing information), we pass the error up to whatever code called our function. This is useful when you want to log an error but still want the calling code to to have to deal with it because there is no way to recover at the location which the error happened (i.e., you might not have enough context to do anything about it right then and there).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#creating-custom-exceptions",
    "href": "errors_exceptions.html#creating-custom-exceptions",
    "title": "6  Errors & Exceptions",
    "section": "Creating Custom Exceptions",
    "text": "Creating Custom Exceptions\nPython packages and libraries often create their own custom error types. For example:\n\nBiopython defines a TreeError class to alert users when something goes wrong with phylogenetic trees.\nPandas defines a MergeError for problems that occur when trying to merge data frames.\n\nThese custom errors make it easier for users to handle problems specific to that library. Users can then distinguish between errors from the library and errors from their own code.\nLet’s look at an example. First, we’ll write code without a custom error, then improve it by adding one.\n\ndef parse_dna_string(dna_string):\n    \"\"\"Parse a DNA string, validating the nucleotides.\"\"\"\n    valid_bases = {\"A\", \"C\", \"G\", \"T\", \"N\"}\n\n    for i, base in enumerate(dna_string):\n        if base not in valid_bases:\n            raise ValueError(f\"Invalid DNA base at index {i}: '{base}'\")\n\n    return dna_string\n\n\nprint(parse_dna_string(\"ACTG\"))\n\ntry:\n    print(parse_dna_string(\"ACXG\"))\nexcept ValueError as error:\n    print(f\"{error=}\")\n\nACTG\nerror=ValueError(\"Invalid DNA base at index 2: 'X'\")\n\n\nThis works fine, but imagine you’re writing a library with many classes for different aspects of sequence analysis and file parsing. Error handling could become confusing. Creating custom errors helps users understand and handle problems more clearly.\nA common approach is to define a base error class for your package, and then create specific error types that inherit from it. For example, if our package is called EasyBio, we might do this:\n\nclass EasyBioError(Exception):\n    \"\"\"Base class for all EasyBio package errors.\"\"\"\n    pass\n\nclass InvalidBaseError(EasyBioError):\n    \"\"\"Error raised when a DNA sequence contains invalid characters\"\"\"\n    pass\n\n# We could define more specific errors for other situations too\n\ndef parse_dna_string(dna_string):\n    valid_bases = {\"A\", \"C\", \"G\", \"T\", \"N\"}\n\n    for i, base in enumerate(dna_string):\n        if base not in valid_bases:\n            raise InvalidBaseError(f\"Invalid DNA base at index {i}: '{base}'\")\n\n    return dna_string\n\n\nprint(parse_dna_string(\"ACTG\"))\n\ntry:\n    print(parse_dna_string(\"ACXG\"))\nexcept InvalidBaseError as error:\n    print(f\"{error=}\")\n\nACTG\nerror=InvalidBaseError(\"Invalid DNA base at index 2: 'X'\")\n\n\nThis code is more descriptive about what went wrong - specifically that we found an invalid nucleotide. The benefits of this approach become clearer in larger packages with many different types of potential errors.\nBenefits of using custom exceptions include:\n\nClear hierarchy: Users can catch just the base exception (EasyBioError) to handle any error from your library, or catch specific exceptions for targeted handling.\nImproved error messaging: Custom exceptions can include field-specific information that helps users understand what went wrong in their context.\nDocumentation: Custom exceptions serve as self-documenting code, showing users what can go wrong.\n\nUsers of your code will often expect to see custom error types that are specific to your package or library. This approach lets you control which errors users need to handle and gives them clear information about what went wrong.\n\n\n\n\n\n\nTip 6.7: Stop & Think\n\n\n\n\n\nIf you were creating a package for RNA-seq analysis, what custom exception types might be useful to define?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#general-tips",
    "href": "errors_exceptions.html#general-tips",
    "title": "6  Errors & Exceptions",
    "section": "General Tips",
    "text": "General Tips\nNow that we’ve explored errors and exceptions, let’s go through some general tips about how to deal with them.\n\nUse Descriptive Error Messages\nIt’s important to include helpful details when raising errors. This makes your code more robust and provides valuable context to anyone using your functions.\nBe specific and descriptive:\n# Too vague\nraise ValueError(\"Invalid input\")\n\n# More descriptive\nraise ValueError(\"Expected DNA sequence but found invalid characters\")\n\n# Shows the specific problems with the sequence\nraise ValueError(f\"Expected DNA sequence but found invalid characters at positions {invalid_positions}\")\nInclude relevant values:\nif start &gt;= end:\n    raise ValueError(f\"Start position ({start}) must be less than end position ({end})\")\nSuggest solutions when possible:\nif not os.path.exists(filepath):\n    raise FileNotFoundError(f\"File '{filepath}' not found. Check spelling or use absolute path.\")\n\n# Not as good:\nraise ConnectionError(\"connection failed\")\n\n# Better\nraise ConnectionError(\"Database connection failed: check that server is running on port 5432\")\nDocument the exceptions your functions can raise:\ndef parse_fasta(filename):\n    \"\"\"\n    Parse a FASTA file and return sequences.\n\n    Args:\n        filename: Path to the FASTA file\n\n    Returns:\n        List of (header, sequence) tuples\n\n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        ValueError: If the file is not in valid FASTA format\n    \"\"\"\n    pass\nThese suggestions will make debugging easier for both you and anyone using your code.\n\n\nWhen to Catch Exceptions\nLet’s talk about you should catch exceptions and when it’s better to let them bubble up through your program.\nYou should catch exceptions when:\n\nYou can recover meaningfully\n\nYou can provide a default value if an error occurs (for example, when calculating fold change and dividing by zero, just return one)\nYou can safely skip problematic data (for example, when processing multiple sequences from a FASTA file and one is corrupted, just skip that sequence)\n\nYou need to clean up resources (for example, close a database connection if a query fails)\nYou want to translate an error into a different, more appropriate error type\n\nYou should not catch exceptions when:\n\nThere is no reasonable way to recover from the error\n\nIf you have no recovery strategy, let the exception move up to a level that can handle it\nFor example:\n\nWhen a user provides a file name that doesn’t exist\nWhen a database connection fails\n\n\nThere is a cleaner alternative (for example, using dict.get to provide a default value instead of catching a KeyError)\n\n\n\n\nDecision Tree for Catching Exceptions\n\n\n\n\nOther tips\n\nWhen you decide to catch an exception, catch it as specifically as possible\n\nPrefer catching specific exceptions like ValueError or FileNotFoundError rather than broad ones like Exception\nThis prevents accidentally hiding bugs by catching exceptions you weren’t expecting\n\nDon’t silently ignore exceptions\n\nAt minimum, you should at least log a message to let users know something went wrong\n\n\n\n\nExample\nHere is a tiny example that breaks pretty much all the suggestions that we have given:\n\ndef example(x, y, z):\n    try:\n        return potentially_risky_function(x, y, z)\n    except Exception:\n        pass\n\nThis is better though!\ndef example(x, y, z):\n    try:\n        return potentially_risky_function(x, y, z)\n    except ValueError as e:\n        print(f\"WARNING -- Invalid value encountered: {e}\")\n        return default_value\n    except IOError as e:\n        print(f\"ERROR -- IO error: {e}\")\n        # Re-raise errors we can't handle\n        raise",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#summary",
    "href": "errors_exceptions.html#summary",
    "title": "6  Errors & Exceptions",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we covered Python exceptions and how to handle them.\n\nSyntax errors happen when your code breaks Python’s grammar rules. These must be fixed before your code can run.\nExceptions occur during program execution when something unexpected happens, like trying to divide by zero.\ntry/except blocks let you catch exceptions and handle them smoothly, similar to how you might have contingency plans in an experiment.\nThe exception hierarchy allows you to catch specific error types (like FileNotFoundError) or broader categories of errors (like OSError).\nelse and finally clauses give you extra control in error handling, letting you run code when no errors occur or ensure cleanup happens regardless.\nCustom exceptions help you write more readable and maintainable code by creating error types specific to your program.\n\nError handling is a crucial skill for building robust programs that can recover gracefully when things go wrong. Practice identifying where your code might fail (like when reading files or processing data) and implement appropriate exception handling to build more reliable applications for your research.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#suggested-readings",
    "href": "errors_exceptions.html#suggested-readings",
    "title": "6  Errors & Exceptions",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nYou might enjoy checking out some of these resources:\n\nPython docs tutorial about errors and exceptions\nReal Python’s introduction to exceptions\nReal Python’s discussion of built-in exceptions\nStackOverflow post about the else block\nStackOverflow post about raising exceptions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions.html#practice-problems",
    "href": "errors_exceptions.html#practice-problems",
    "title": "6  Errors & Exceptions",
    "section": "Practice Problems",
    "text": "Practice Problems\n\n6.1\nWrite a try/except block that attempts to convert a string to a float, but catches the ValueError if the string isn’t a valid number. When an error occurs, it should print “Not a valid number”.\n\n\n6.2\nModify this code to catch the potential error:\ncounts = {\"A\": 1, \"C\": 2, \"G\": 0, \"T\": 4}\ntotal = sum(counts.values())\nn_ratio = counts[\"N\"] / total \n\n\n6.3\nHere is a tiny, misbehaving Python function:\nimport random\n\ndef silly_divide(x, y):\n    if random.random() &lt; 0.25:\n        raise Exception(\"oops!\")\n\n    return x / y\nAbout 75% of the time, it divides two numbers. However, the other 25% of the time, it raises an Exception.\nWrite code that runs this function inside a try/except block. It should have two except clauses, one to catch the ZeroDivisionError and one to catch the potential Exception. You should give the user info about the error that was caught.\n\n\n6.4\nWrite a function called fold_change that takes two expression values and calculates their fold change. Make sure to handle any potential errors that could occur. If there is an error, the function should return None.\n\n\n6.5\nCreate a custom exception called SequenceLengthError that inherits from Exception. Then use it in a function called validate_sequence_length that raises a SequenceLengthError if the given sequence length is not between 50 and 150 bases.\n\n\n6.6\nConsider the following code:\ndef run_simulation(max_turns):\n    if max_turns &lt; 1:\n        raise ValueError(\"bad input\")\n    \n    if max_turns &gt; 1000:\n        raise ValueError(\"bad input\")\n    \n    # Simulation code would follow\n    pass\nRewrite this function so that it provides the uses with better error messages.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Errors & Exceptions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html",
    "href": "exploratory_data_analysis.html",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "",
    "text": "Introduction\nIn this tutorial, we will learn the basics of exploratory data analysis in Python using Pandas (and just a tiny bit of seaborn).\nExploratory Data Analysis, or EDA for short, is the process of building an understanding of your data. Before jumping into complex statistical analyses or building predictive models, EDA helps you understand what your data actually contains. It’s about visually and statistically summarizing your dataset, identifying patterns, spotting anomalies, and generating hypotheses.\nEDA is a critical step in discovery-based research (sometimes known as foundational or exploratory research). As biologists, you will be familiar with hypothesis-driven research, whereby you start with the answer (the hypothesis), and try to work back to either prove or disprove it using the scientific method. Discovery-based research fits in even before hypothesis-driven research can begin, and is especially useful in cases where we know so little about the topic or system in question that we can’t craft useful hypotheses. One of its main goals is to build understanding of complex systems and generate hypotheses that can be tested in the more classical style of hypothesis-driven research, and EDA is a critical step in this process.\nWhile EDA is often closely connected with discovery-based research, it is important to note that it is also a critical aspect of hypothesis-driven processes as well. For example, EDA can be a powerful tool for data quality control and assumption checking. It’s important to identify missing values, outliers, or other bad data that could compromise your analysis. Further, many statistical methods have assumptions about your data (like normality or constant variance of errors). EDA helps you verify if these assumptions are reasonable. Without proper exploration of your data, you might miss critical insights or, worse, draw incorrect conclusions from your analyses.\nEDA has a role in helping you to build an intuition for your problem domain and your data. Regularly engaging with EDA will help you get a “feel” for your data and better understand its strengths and limitations. This intuition is critical for effective communication of your findings and for productive discussion of your data and problem domain with collaborators and stakeholders, or in publications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#introduction",
    "href": "exploratory_data_analysis.html#introduction",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "",
    "text": "Important Python libraries for doing EDA\nPython has a strong set of libraries for exploratory data analysis (EDA). Here are some of the more common ones:\n\nPandas: Essential for working with tabular data, offering powerful DataFrame operations.\nNumPy: Provides fast array operations, forming the backbone of numerical computing in Python.\nSciPy: Useful for advanced statistical analysis and scientific computing.\nStatsmodels: Extends on the statistical models provided by SciPy and provides and alternative interface.\nMatplotlib: A versatile library for creating static, animated, and interactive plots.\nSeaborn: Simplifies statistical visualization with built-in themes and functions.\nJupyter and Quarto: Computational notebooks\n\nThere are many more, but you will see these popping up again and again.\n\n\nWhy We’re Using Pandas\nWe’re using Pandas in this tutorial because:\n\nWorks well with tabular data: Most biological data is structured like a spreadsheet or database table, and Pandas is built for handling this format.\nWidely used: It’s a common tool in both academia and industry.\nRelatively easy to use: Pandas provides a straightforward way to explore and manipulate data.\nHas useful built-ins: Filtering, grouping, summarizing, and plotting data often take just a few lines of code.\nPlays well with others: Pandas integrates smoothly with visualization and statistical tools.\nUses similar concepts to R’s tidyverse, which many of you have experience with from your previous coursework\n\nWe’ll also use a bit of seaborn for visualization, as it can help with certain types of plots or when data is in a certain format.\n\n\nPractical Examples\nIn this tutorial, we’ll learn exploratory data analysis (EDA) by working through real research questions with real datasets. Instead of covering every Pandas function upfront, we’ll introduce tools as we need them.\nWe’ll use datasets from the CORGIS collection, which offers accessible real-world data. Our examples include:\n\nState Demographics: Analyzing population patterns and economic indicators across the U.S.\nCancer Statistics: Examining cancer rates and their potential links to demographics.\nVaccination Impact: Exploring historical disease data to see how vaccines have shaped public health.\n\nThese examples will help you learn Pandas in context, and use techniques that are similar to those you could use when getting started with a real research project.\n\n\nKey Pandas Functionality\nAfter working through the examples, we’ll summarize the essential Pandas operations, including:\n\nLoading and examining data\nSelecting, filtering, and sorting\nGrouping and aggregating\nBasic visualization\nMerging and joining datasets\n\nBy focusing on core functions, you’ll gain practical skills without getting lost in the details. Let’s dive in!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#import-needed-libraries",
    "href": "exploratory_data_analysis.html#import-needed-libraries",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Import Needed Libraries",
    "text": "Import Needed Libraries\nThe first thing we need to do is to import some libraries. We are only using numpy for a couple things in this tutorial: specifying data types the NaN value.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\npd.options.mode.copy_on_write = \"warn\"\n\npd.set_option(\"display.max_rows\", 10)\n\nNote: While we don’t need to import it, you will also need to have SciPy installed to run the clustered heatmaps.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#state-demographics-data",
    "href": "exploratory_data_analysis.html#state-demographics-data",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "State Demographics Data",
    "text": "State Demographics Data\nTo start, we are going to look at the state demographics data from The Collection of Really Great, Interesting, Situated Datasets (CORGIS). CORGIS is a collection of datasets that have been cleaned and otherwise made ready for teaching/learning purposes. It was created in part by Dr. Cory Bart, who is a professor at UD.\nThis state demographics data includes a lot of info about states that we will be able to use to try and explain some of the cancer trends that we see in the next section. To give you an idea of the kinds of data we’ll be working with, here are some of the data categories:\n\nPopulation\nAge\nEthnicities\nHousing\nIncome\nEmployment\n\n\nImporting Data\nThe first thing we need to do is import the data. To do that, we can use the read_csv() function:\n\nstate_demographics = pd.read_csv(\"./_data/state_demographics.csv\")\n\n\n\nData Overview\nAfter importing data, it’s always a good idea to check out its basic info, things like shape, column names, basic summary statistics, etc.\nTo get the number of rows and columns in a data frame, we use [shape()][https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html]:\n\nstate_demographics.shape\n\n(51, 48)\n\n\nThat’s not too much data, so let’s look at the table directly. We will use the head() function to take only the first few rows:\n\nstate_demographics.head()\n\n\n\n\n\n\n\n\nState\nPopulation.Population Percent Change\nPopulation.2014 Population\nPopulation.2010 Population\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nMiscellaneous.Percent Female\nEthnicities.White Alone\nEthnicities.Black Alone\n...\nEmployment.Nonemployer Establishments\nEmployment.Firms.Total\nEmployment.Firms.Men-Owned\nEmployment.Firms.Women-Owned\nEmployment.Firms.Minority-Owned\nEmployment.Firms.Nonminority-Owned\nEmployment.Firms.Veteran-Owned\nEmployment.Firms.Nonveteran-Owned\nPopulation.Population per Square Mile\nMiscellaneous.Land Area\n\n\n\n\n0\nConnecticut\n-10.2\n3605944\n3574097\n5.1\n20.4\n17.7\n51.2\n79.7\n12.2\n...\n286874\n326693\n187845\n106678\n56113\n259614\n31056\n281182\n738.1\n4842.36\n\n\n1\nDelaware\n8.4\n989948\n897934\n5.6\n20.9\n19.4\n51.7\n69.2\n23.2\n...\n68623\n73418\n38328\n23964\n14440\n54782\n7206\n60318\n460.8\n1948.54\n\n\n2\nDistrict of Columbia\n17.3\n689545\n601723\n6.4\n18.2\n12.4\n52.6\n46.0\n46.0\n...\n62583\n63408\n30237\n27064\n29983\n29521\n5070\n54217\n9856.5\n61.05\n\n\n3\nFlorida\n14.2\n21538187\n18801310\n5.3\n19.7\n20.9\n51.1\n77.3\n16.9\n...\n2388050\n2100187\n1084885\n807817\n926112\n1121749\n185756\n1846686\n350.6\n53624.76\n\n\n4\nGeorgia\n9.6\n10711908\n9687653\n6.2\n23.6\n14.3\n51.4\n60.2\n32.6\n...\n955621\n929864\n480578\n376506\n371588\n538893\n96787\n800585\n168.4\n57513.49\n\n\n\n\n5 rows × 48 columns\n\n\n\nTo get summary statistics of the numeric rows of a data frame, we use describe(). This can help you to get an overall sense of your data.\n\nstate_demographics.describe()\n\n\n\n\n\n\n\n\nPopulation.Population Percent Change\nPopulation.2014 Population\nPopulation.2010 Population\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nMiscellaneous.Percent Female\nEthnicities.White Alone\nEthnicities.Black Alone\nEthnicities.American Indian and Alaska Native Alone\n...\nEmployment.Nonemployer Establishments\nEmployment.Firms.Total\nEmployment.Firms.Men-Owned\nEmployment.Firms.Women-Owned\nEmployment.Firms.Minority-Owned\nEmployment.Firms.Nonminority-Owned\nEmployment.Firms.Veteran-Owned\nEmployment.Firms.Nonveteran-Owned\nPopulation.Population per Square Mile\nMiscellaneous.Land Area\n\n\n\n\ncount\n51.000000\n5.100000e+01\n5.100000e+01\n51.000000\n51.000000\n51.000000\n51.000000\n51.000000\n51.000000\n51.000000\n...\n5.100000e+01\n5.100000e+01\n5.100000e+01\n5.100000e+01\n5.100000e+01\n5.100000e+01\n51.000000\n5.100000e+01\n51.000000\n51.000000\n\n\nmean\n5.147059\n6.499006e+06\n6.053834e+06\n5.958824\n22.139216\n16.878431\n50.598039\n78.068627\n11.872549\n2.005882\n...\n5.193242e+05\n5.452549e+05\n2.923801e+05\n1.938421e+05\n1.560321e+05\n3.737521e+05\n49611.882353\n4.733260e+05\n384.403922\n69253.047843\n\n\nstd\n6.870760\n7.408023e+06\n6.823984e+06\n0.607018\n1.996805\n2.008812\n0.836777\n13.024907\n10.704057\n3.105441\n...\n6.688605e+05\n6.614342e+05\n3.524479e+05\n2.468993e+05\n2.992485e+05\n3.615840e+05\n51941.581563\n5.892086e+05\n1377.354603\n85526.076023\n\n\nmin\n-13.300000\n5.768510e+05\n5.636260e+05\n4.700000\n18.200000\n11.400000\n47.900000\n25.500000\n0.600000\n0.300000\n...\n5.304200e+04\n6.242700e+04\n3.003900e+04\n1.934400e+04\n2.354000e+03\n2.952100e+04\n5070.000000\n5.135300e+04\n1.200000\n61.050000\n\n\n25%\n1.950000\n1.816411e+06\n1.696962e+06\n5.700000\n21.050000\n16.100000\n50.200000\n71.250000\n3.650000\n0.500000\n...\n1.216330e+05\n1.431060e+05\n7.392400e+04\n4.478700e+04\n1.472200e+04\n1.261350e+05\n14892.500000\n1.200765e+05\n45.800000\n33334.515000\n\n\n50%\n4.100000\n4.505836e+06\n4.339367e+06\n6.000000\n22.100000\n16.900000\n50.700000\n80.600000\n8.500000\n0.700000\n...\n3.026530e+05\n3.393050e+05\n1.878450e+05\n1.230150e+05\n6.125200e+04\n2.762690e+05\n36273.000000\n2.887900e+05\n101.200000\n53624.760000\n\n\n75%\n9.800000\n7.428392e+06\n6.636084e+06\n6.200000\n23.250000\n17.850000\n51.200000\n86.900000\n16.300000\n1.600000\n...\n5.568705e+05\n5.790585e+05\n3.276305e+05\n2.041645e+05\n1.288060e+05\n4.462370e+05\n58167.500000\n4.975955e+05\n221.450000\n80692.730000\n\n\nmax\n17.300000\n3.953822e+07\n3.725396e+07\n7.700000\n29.000000\n21.200000\n52.600000\n94.400000\n46.000000\n15.600000\n...\n3.453769e+06\n3.548449e+06\n1.852580e+06\n1.320085e+06\n1.619857e+06\n1.819107e+06\n252377.000000\n3.176341e+06\n9856.500000\n570640.950000\n\n\n\n\n8 rows × 47 columns\n\n\n\nDo you notice how the values in the table have a lot of precision, and are all using scientific notation? Sometimes this is what we want, but we really don’t need all that here, and it’s only serving to clutter up the view. We can control the precision of the numbers in the table using the .style.format() pattern. Since we have some large numbers in there, let’s add a thousands place separator as well.\n\nstate_demographics.describe().style.format(\n    # Set precision of numbers to 2 decimal places\n    precision=2,\n    # Use a comma to separate out the thousands in the big numbers\n    thousands=\",\",\n)\n\n\n\n\n\n\n \nPopulation.Population Percent Change\nPopulation.2014 Population\nPopulation.2010 Population\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nMiscellaneous.Percent Female\nEthnicities.White Alone\nEthnicities.Black Alone\nEthnicities.American Indian and Alaska Native Alone\nEthnicities.Asian Alone\nEthnicities.Native Hawaiian and Other Pacific Islander Alone\nEthnicities.Two or More Races\nEthnicities.Hispanic or Latino\nEthnicities.White Alone, not Hispanic or Latino\nMiscellaneous.Veterans\nMiscellaneous.Foreign Born\nHousing.Housing Units\nHousing.Homeownership Rate\nHousing.Median Value of Owner-Occupied Units\nHousing.Households\nHousing.Persons per Household\nMiscellaneous.Living in Same House +1 Years\nMiscellaneous.Language Other than English at Home\nHousing.Households with a computer\nHousing.Households with a Internet\nEducation.High School or Higher\nEducation.Bachelor's Degree or Higher\nMiscellaneous.Percent Under 66 Years With a Disability\nMiscellaneous.Percent Under 65 Years Without Health insurance\nSales.Accommodation and Food Services Sales\nMiscellaneous.Manufacturers Shipments\nSales.Retail Sales\nMiscellaneous.Mean Travel Time to Work\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\nEmployment.Nonemployer Establishments\nEmployment.Firms.Total\nEmployment.Firms.Men-Owned\nEmployment.Firms.Women-Owned\nEmployment.Firms.Minority-Owned\nEmployment.Firms.Nonminority-Owned\nEmployment.Firms.Veteran-Owned\nEmployment.Firms.Nonveteran-Owned\nPopulation.Population per Square Mile\nMiscellaneous.Land Area\n\n\n\n\ncount\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n49.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n\n\nmean\n5.15\n6,499,005.51\n6,053,834.08\n5.96\n22.14\n16.88\n50.60\n78.07\n11.87\n2.01\n4.55\n0.40\n3.11\n12.25\n67.67\n357,457.29\n9.41\n2,738,906.75\n65.67\n233,176.47\n2,367,765.65\n2.56\n85.24\n14.98\n89.98\n82.05\n89.54\n31.77\n9.16\n9.96\n13,885,070.55\n116,411,562.24\n82,741,605.31\n24.80\n63,097.86\n33,743.06\n12.17\n519,324.16\n545,254.90\n292,380.10\n193,842.12\n156,032.14\n373,752.12\n49,611.88\n473,326.00\n384.40\n69,253.05\n\n\nstd\n6.87\n7,408,022.55\n6,823,984.27\n0.61\n2.00\n2.01\n0.84\n13.02\n10.70\n3.11\n5.52\n1.41\n3.21\n10.35\n16.18\n351,951.29\n6.11\n2,888,415.00\n5.44\n109,205.70\n2,531,758.54\n0.17\n2.06\n9.77\n2.55\n3.79\n2.70\n6.43\n1.77\n3.64\n16,274,397.82\n131,704,461.49\n91,289,768.45\n3.98\n10,715.13\n5,689.58\n2.68\n668,860.54\n661,434.25\n352,447.95\n246,899.31\n299,248.47\n361,584.05\n51,941.58\n589,208.57\n1,377.35\n85,526.08\n\n\nmin\n-13.30\n576,851.00\n563,626.00\n4.70\n18.20\n11.40\n47.90\n25.50\n0.60\n0.30\n0.80\n0.00\n1.30\n1.70\n21.70\n26,156.00\n1.70\n280,291.00\n41.60\n119,000.00\n230,101.00\n2.30\n80.50\n2.60\n83.80\n71.50\n83.30\n20.60\n6.40\n3.50\n1,564,272.00\n309,832.00\n4,439,933.00\n17.20\n45,081.00\n24,369.00\n7.30\n53,042.00\n62,427.00\n30,039.00\n19,344.00\n2,354.00\n29,521.00\n5,070.00\n51,353.00\n1.20\n61.05\n\n\n25%\n1.95\n1,816,411.00\n1,696,961.50\n5.70\n21.05\n16.10\n50.20\n71.25\n3.65\n0.50\n1.80\n0.10\n2.10\n5.30\n57.40\n116,811.50\n4.75\n801,166.00\n64.10\n159,750.00\n681,296.50\n2.46\n83.95\n7.30\n88.80\n80.70\n87.30\n27.85\n7.85\n7.30\n4,171,798.50\n24,553,072.00\n23,908,598.50\n22.25\n55,560.50\n30,276.50\n10.10\n121,633.00\n143,106.00\n73,924.00\n44,787.00\n14,722.00\n126,135.00\n14,892.50\n120,076.50\n45.80\n33,334.51\n\n\n50%\n4.10\n4,505,836.00\n4,339,367.00\n6.00\n22.10\n16.90\n50.70\n80.60\n8.50\n0.70\n3.00\n0.10\n2.40\n9.80\n71.10\n270,775.00\n7.20\n2,006,358.00\n66.30\n194,500.00\n1,734,618.00\n2.52\n85.30\n11.80\n89.90\n82.50\n90.20\n31.30\n8.90\n9.30\n9,542,068.00\n81,927,799.00\n54,869,978.00\n24.80\n61,439.00\n32,176.00\n11.80\n302,653.00\n339,305.00\n187,845.00\n123,015.00\n61,252.00\n276,269.00\n36,273.00\n288,790.00\n101.20\n53,624.76\n\n\n75%\n9.80\n7,428,391.50\n6,636,084.50\n6.20\n23.25\n17.85\n51.20\n86.90\n16.30\n1.60\n5.10\n0.20\n2.90\n13.90\n79.10\n459,667.50\n13.65\n3,135,492.50\n69.00\n271,750.00\n2,732,946.50\n2.62\n86.70\n21.00\n91.65\n84.80\n91.70\n34.45\n10.20\n12.15\n17,652,438.00\n139,960,482.00\n101,458,882.50\n27.35\n71,463.50\n36,454.00\n13.50\n556,870.50\n579,058.50\n327,630.50\n204,164.50\n128,806.00\n446,237.00\n58,167.50\n497,595.50\n221.45\n80,692.73\n\n\nmax\n17.30\n39,538,223.00\n37,253,956.00\n7.70\n29.00\n21.20\n52.60\n94.40\n46.00\n15.60\n37.60\n10.10\n24.20\n49.30\n93.00\n1,574,531.00\n26.80\n14,366,336.00\n73.20\n615,300.00\n13,044,266.00\n3.12\n89.80\n44.20\n95.30\n88.30\n93.60\n58.50\n14.00\n20.80\n90,830,372.00\n702,603,073.00\n481,800,461.00\n33.60\n86,420.00\n56,147.00\n19.60\n3,453,769.00\n3,548,449.00\n1,852,580.00\n1,320,085.00\n1,619,857.00\n1,819,107.00\n252,377.00\n3,176,341.00\n9,856.50\n570,640.95\n\n\n\n\n\nOne more thing we can do to clean up this view is to sort the variables by their name.\n\n(\n    state_demographics.describe()\n    # Sort the data frame by the row index (a.k.a., the row names)\n    .sort_index(axis=\"columns\")\n    .style.format(precision=2, thousands=\",\")\n)\n\n\n\n\n\n\n \nAge.Percent 65 and Older\nAge.Percent Under 18 Years\nAge.Percent Under 5 Years\nEducation.Bachelor's Degree or Higher\nEducation.High School or Higher\nEmployment.Firms.Men-Owned\nEmployment.Firms.Minority-Owned\nEmployment.Firms.Nonminority-Owned\nEmployment.Firms.Nonveteran-Owned\nEmployment.Firms.Total\nEmployment.Firms.Veteran-Owned\nEmployment.Firms.Women-Owned\nEmployment.Nonemployer Establishments\nEthnicities.American Indian and Alaska Native Alone\nEthnicities.Asian Alone\nEthnicities.Black Alone\nEthnicities.Hispanic or Latino\nEthnicities.Native Hawaiian and Other Pacific Islander Alone\nEthnicities.Two or More Races\nEthnicities.White Alone\nEthnicities.White Alone, not Hispanic or Latino\nHousing.Homeownership Rate\nHousing.Households\nHousing.Households with a Internet\nHousing.Households with a computer\nHousing.Housing Units\nHousing.Median Value of Owner-Occupied Units\nHousing.Persons per Household\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\nMiscellaneous.Foreign Born\nMiscellaneous.Land Area\nMiscellaneous.Language Other than English at Home\nMiscellaneous.Living in Same House +1 Years\nMiscellaneous.Manufacturers Shipments\nMiscellaneous.Mean Travel Time to Work\nMiscellaneous.Percent Female\nMiscellaneous.Percent Under 65 Years Without Health insurance\nMiscellaneous.Percent Under 66 Years With a Disability\nMiscellaneous.Veterans\nPopulation.2010 Population\nPopulation.2014 Population\nPopulation.Population Percent Change\nPopulation.Population per Square Mile\nSales.Accommodation and Food Services Sales\nSales.Retail Sales\n\n\n\n\ncount\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n49.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n51.00\n\n\nmean\n16.88\n22.14\n5.96\n31.77\n89.54\n292,380.10\n156,032.14\n373,752.12\n473,326.00\n545,254.90\n49,611.88\n193,842.12\n519,324.16\n2.01\n4.55\n11.87\n12.25\n0.40\n3.11\n78.07\n67.67\n65.67\n2,367,765.65\n82.05\n89.98\n2,738,906.75\n233,176.47\n2.56\n63,097.86\n33,743.06\n12.17\n9.41\n69,253.05\n14.98\n85.24\n116,411,562.24\n24.80\n50.60\n9.96\n9.16\n357,457.29\n6,053,834.08\n6,499,005.51\n5.15\n384.40\n13,885,070.55\n82,741,605.31\n\n\nstd\n2.01\n2.00\n0.61\n6.43\n2.70\n352,447.95\n299,248.47\n361,584.05\n589,208.57\n661,434.25\n51,941.58\n246,899.31\n668,860.54\n3.11\n5.52\n10.70\n10.35\n1.41\n3.21\n13.02\n16.18\n5.44\n2,531,758.54\n3.79\n2.55\n2,888,415.00\n109,205.70\n0.17\n10,715.13\n5,689.58\n2.68\n6.11\n85,526.08\n9.77\n2.06\n131,704,461.49\n3.98\n0.84\n3.64\n1.77\n351,951.29\n6,823,984.27\n7,408,022.55\n6.87\n1,377.35\n16,274,397.82\n91,289,768.45\n\n\nmin\n11.40\n18.20\n4.70\n20.60\n83.30\n30,039.00\n2,354.00\n29,521.00\n51,353.00\n62,427.00\n5,070.00\n19,344.00\n53,042.00\n0.30\n0.80\n0.60\n1.70\n0.00\n1.30\n25.50\n21.70\n41.60\n230,101.00\n71.50\n83.80\n280,291.00\n119,000.00\n2.30\n45,081.00\n24,369.00\n7.30\n1.70\n61.05\n2.60\n80.50\n309,832.00\n17.20\n47.90\n3.50\n6.40\n26,156.00\n563,626.00\n576,851.00\n-13.30\n1.20\n1,564,272.00\n4,439,933.00\n\n\n25%\n16.10\n21.05\n5.70\n27.85\n87.30\n73,924.00\n14,722.00\n126,135.00\n120,076.50\n143,106.00\n14,892.50\n44,787.00\n121,633.00\n0.50\n1.80\n3.65\n5.30\n0.10\n2.10\n71.25\n57.40\n64.10\n681,296.50\n80.70\n88.80\n801,166.00\n159,750.00\n2.46\n55,560.50\n30,276.50\n10.10\n4.75\n33,334.51\n7.30\n83.95\n24,553,072.00\n22.25\n50.20\n7.30\n7.85\n116,811.50\n1,696,961.50\n1,816,411.00\n1.95\n45.80\n4,171,798.50\n23,908,598.50\n\n\n50%\n16.90\n22.10\n6.00\n31.30\n90.20\n187,845.00\n61,252.00\n276,269.00\n288,790.00\n339,305.00\n36,273.00\n123,015.00\n302,653.00\n0.70\n3.00\n8.50\n9.80\n0.10\n2.40\n80.60\n71.10\n66.30\n1,734,618.00\n82.50\n89.90\n2,006,358.00\n194,500.00\n2.52\n61,439.00\n32,176.00\n11.80\n7.20\n53,624.76\n11.80\n85.30\n81,927,799.00\n24.80\n50.70\n9.30\n8.90\n270,775.00\n4,339,367.00\n4,505,836.00\n4.10\n101.20\n9,542,068.00\n54,869,978.00\n\n\n75%\n17.85\n23.25\n6.20\n34.45\n91.70\n327,630.50\n128,806.00\n446,237.00\n497,595.50\n579,058.50\n58,167.50\n204,164.50\n556,870.50\n1.60\n5.10\n16.30\n13.90\n0.20\n2.90\n86.90\n79.10\n69.00\n2,732,946.50\n84.80\n91.65\n3,135,492.50\n271,750.00\n2.62\n71,463.50\n36,454.00\n13.50\n13.65\n80,692.73\n21.00\n86.70\n139,960,482.00\n27.35\n51.20\n12.15\n10.20\n459,667.50\n6,636,084.50\n7,428,391.50\n9.80\n221.45\n17,652,438.00\n101,458,882.50\n\n\nmax\n21.20\n29.00\n7.70\n58.50\n93.60\n1,852,580.00\n1,619,857.00\n1,819,107.00\n3,176,341.00\n3,548,449.00\n252,377.00\n1,320,085.00\n3,453,769.00\n15.60\n37.60\n46.00\n49.30\n10.10\n24.20\n94.40\n93.00\n73.20\n13,044,266.00\n88.30\n95.30\n14,366,336.00\n615,300.00\n3.12\n86,420.00\n56,147.00\n19.60\n26.80\n570,640.95\n44.20\n89.80\n702,603,073.00\n33.60\n52.60\n20.80\n14.00\n1,574,531.00\n37,253,956.00\n39,538,223.00\n17.30\n9,856.50\n90,830,372.00\n481,800,461.00\n\n\n\n\n\nThat’s a pretty nice looking summary now!\nNote: Do you see how we put that little pipeline in parentheses? This is so that we can separate operations on their own line, which can improve readability, and also let us add comments as needed.\nI want to make something clear. We haven’t done anything to change the data frame that we imported.\n\nstate_demographics.head()\n\n\n\n\n\n\n\n\nState\nPopulation.Population Percent Change\nPopulation.2014 Population\nPopulation.2010 Population\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nMiscellaneous.Percent Female\nEthnicities.White Alone\nEthnicities.Black Alone\n...\nEmployment.Nonemployer Establishments\nEmployment.Firms.Total\nEmployment.Firms.Men-Owned\nEmployment.Firms.Women-Owned\nEmployment.Firms.Minority-Owned\nEmployment.Firms.Nonminority-Owned\nEmployment.Firms.Veteran-Owned\nEmployment.Firms.Nonveteran-Owned\nPopulation.Population per Square Mile\nMiscellaneous.Land Area\n\n\n\n\n0\nConnecticut\n-10.2\n3605944\n3574097\n5.1\n20.4\n17.7\n51.2\n79.7\n12.2\n...\n286874\n326693\n187845\n106678\n56113\n259614\n31056\n281182\n738.1\n4842.36\n\n\n1\nDelaware\n8.4\n989948\n897934\n5.6\n20.9\n19.4\n51.7\n69.2\n23.2\n...\n68623\n73418\n38328\n23964\n14440\n54782\n7206\n60318\n460.8\n1948.54\n\n\n2\nDistrict of Columbia\n17.3\n689545\n601723\n6.4\n18.2\n12.4\n52.6\n46.0\n46.0\n...\n62583\n63408\n30237\n27064\n29983\n29521\n5070\n54217\n9856.5\n61.05\n\n\n3\nFlorida\n14.2\n21538187\n18801310\n5.3\n19.7\n20.9\n51.1\n77.3\n16.9\n...\n2388050\n2100187\n1084885\n807817\n926112\n1121749\n185756\n1846686\n350.6\n53624.76\n\n\n4\nGeorgia\n9.6\n10711908\n9687653\n6.2\n23.6\n14.3\n51.4\n60.2\n32.6\n...\n955621\n929864\n480578\n376506\n371588\n538893\n96787\n800585\n168.4\n57513.49\n\n\n\n\n5 rows × 48 columns\n\n\n\nAs you see, it’s the same data frame we started with. All the functions we have used so far have returned new data frames. You will see this pattern a lot–many Pandas functions return new data rather modifying existing data.\n\n\n\n\n\n\nTip 7.1: Stop & Think\n\n\n\n\n\nWhy is it a good idea to look at a summary of data that you imported?\n\n\n\n\n\nFiltering Columns\nThis table has a lot of different kinds of data about state demographics. The nice thing is that each of the different categories is used as a prefix to the column name, e.g., data about income is prefixed with Income, data about population is prefixed with Population, and so on. We can leverage this labeling scheme to chop our data frame into more manageable chunks.\nWe can use the filter() function to filter columns in a bunch of different ways. For now we will use the regular expression (regex) argument to specify that we want to match at the start of the column name. For example, getting all the data columns about ethnicities:\n\n(\n    state_demographics\n    # Use set_index to convert the state column to the row names\n    .set_index(\"State\")\n    # Use filter() to keep columns matching the given pattern\n    .filter(regex=r\"^Ethnicities\")\n    # Only display the first 5 rows\n    .head()\n)\n\n\n\n\n\n\n\n\nEthnicities.White Alone\nEthnicities.Black Alone\nEthnicities.American Indian and Alaska Native Alone\nEthnicities.Asian Alone\nEthnicities.Native Hawaiian and Other Pacific Islander Alone\nEthnicities.Two or More Races\nEthnicities.Hispanic or Latino\nEthnicities.White Alone, not Hispanic or Latino\n\n\nState\n\n\n\n\n\n\n\n\n\n\n\n\nConnecticut\n79.7\n12.2\n0.6\n5.0\n0.1\n2.5\n16.9\n65.9\n\n\nDelaware\n69.2\n23.2\n0.7\n4.1\n0.1\n2.7\n9.6\n61.7\n\n\nDistrict of Columbia\n46.0\n46.0\n0.6\n4.5\n0.1\n2.9\n11.3\n37.5\n\n\nFlorida\n77.3\n16.9\n0.5\n3.0\n0.1\n2.2\n26.4\n53.2\n\n\nGeorgia\n60.2\n32.6\n0.5\n4.4\n0.1\n2.2\n9.9\n52.0\n\n\n\n\n\n\n\nNote: If you need a refresher on regular expressions check out Appendix I. \nWe needed to use the set_index() here so that the state name would still be present on the resulting data frames.\nThis works for other categories as well. Here it is for Income:\n\nstate_demographics.set_index(\"State\").filter(regex=r\"^Income\").head()\n\n\n\n\n\n\n\n\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\n\n\nState\n\n\n\n\n\n\n\nConnecticut\n78444\n44496\n10.0\n\n\nDelaware\n68287\n35450\n11.3\n\n\nDistrict of Columbia\n86420\n56147\n13.5\n\n\nFlorida\n55660\n31619\n12.7\n\n\nGeorgia\n58700\n31067\n13.3\n\n\n\n\n\n\n\nWe can pass these filtered tables to describe() and other functions as well:\n\nstate_demographics.set_index(\"State\").filter(regex=r\"^Income\").describe()\n\n\n\n\n\n\n\n\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\n\n\n\n\ncount\n51.000000\n51.000000\n51.000000\n\n\nmean\n63097.862745\n33743.058824\n12.170588\n\n\nstd\n10715.134497\n5689.577086\n2.678006\n\n\nmin\n45081.000000\n24369.000000\n7.300000\n\n\n25%\n55560.500000\n30276.500000\n10.100000\n\n\n50%\n61439.000000\n32176.000000\n11.800000\n\n\n75%\n71463.500000\n36454.000000\n13.500000\n\n\nmax\n86420.000000\n56147.000000\n19.600000\n\n\n\n\n\n\n\nThis is another way that we can start to get a feel for our data.\n\n\n\n\n\n\nTip 7.2: Stop & Think\n\n\n\n\n\nWhy might filtering columns by category prefixes (like “Population” or “Income”) be useful during exploratory data analysis?\n\n\n\n\n\nExploring Your Data\nNow that we have a basic idea of what our data looks like, we can start to explore it a bit more. The best place to start is to actually look at the data. Pandas gives you the ability to create basic charts without having to use a 3rd-party package. As long as you don’t want anything too complex, it will be fine to start with.\n\nBasic Population Plots\nLet’s start by plotting some basic state population info. We can use Panda’s plot() function for this:\n\n(\n    state_demographics\n    # Take only the columns that start with Population.201X, where X is some digit\n    # E.g., Population.2010, or Population.2014.\n    .filter(regex=r\"^Population\\.201\\d\")\n    # Draw a bar plot\n    .plot(kind=\"bar\")\n)\n\n\n\n\n\n\n\n\nThat’s not bad, but the axes are a bit weird. Let’s adjust them. The simplest way to do that is to be more specific about which data we will need in our chart. Then, we can explicitly set the x and y axes.\n\n# Make a list of the columns that we want to keep\ncolumns = [\"State\", \"Population.2010 Population\", \"Population.2014 Population\"]\n\n# Use the \"bracket notation\" to select only those columns specified in the list we just\n# created.\nplot_data = (\n    state_demographics[columns]\n    # Rename the population columns to something shorter.\n    # It will make the chart legends look nicer.\n    .rename(\n        # We want to rename columns.  The keys of this dictionary are the old column\n        # names, and the values are the new column names.\n        columns={\n            \"Population.2010 Population\": \"2010 Population\",\n            \"Population.2014 Population\": \"2014 Population\",\n        }\n    )\n)\n\n# Plot the data subset\nplot_data.plot(\n    # Make it a bar chart\n    kind=\"bar\",\n    # Put \"State\" on the x-axis\n    x=\"State\",\n    # And put both population columns on the y-axis\n    y=[\"2010 Population\", \"2014 Population\"],\n    figsize=(8, 3),\n)\n\n\n\n\n\n\n\n\nNot bad! The plotting function was smart enough to put both population data series on the chart, and to include a nice legend so that we can tell them apart.\nAt least we have the State names on the x-axis now, but they are pretty smooshed together. There are a bunch of ways we could fix it:\n\nShrink the label size\nAdjust the chart proportions\nUse a horizontal bar chart (and adjust the proportions)\n\nShrinking the label size:\n\nplot_data.plot(\n    kind=\"bar\",\n    x=\"State\",\n    y=[\"2010 Population\", \"2014 Population\"],\n    fontsize=6,\n)\n\n\n\n\n\n\n\n\nThat works, but now the labels are tiny, and it shrunk both the x and y axis labels. You can shrink just the x-axis tick labels, but to do so you need to “eject” out of the pandas API and drop down into matplotlib code.\n\nimport matplotlib.pyplot as plt\n\n# Create your plot\nax = plot_data.plot(\n    kind=\"bar\",\n    x=\"State\",\n    y=[\"2010 Population\", \"2014 Population\"],\n)\n\n# Adjust only x-axis tick labels\n# 8 is the fontsize, adjust it as needed\nax.tick_params(axis=\"x\", labelsize=8)\n\n# Show the chart\nplt.show()\n\n\n\n\n\n\n\n\nDon’t worry too much about the details of this. I just wanted to show you that the plots returned by Pandas are really matplotlib objects, and can be interacted with in the usual way when required.\nLet’s adjust the figure proportions next:\n\nplot_data.plot(\n    kind=\"bar\",\n    x=\"State\",\n    y=[\"2010 Population\", \"2014 Population\"],\n    # Set the width to 8 units, and the height to 3\n    figsize=(8, 3),\n    # Set the font size of all tick labels to 9\n    fontsize=9\n)\n\n\n\n\n\n\n\n\nThat’s fairly readable. Above, I mentioned how long data and plots tend to fit better on the screen both in your reports and when actively doing the analysis. So let’s switch to a horizontal bar chart. That way, we can give the states a little more room on the plot.\n\nplot_data.plot(\n    # Draw a horizontal bar chart. Note the `h` at the end of `barh`.\n    kind=\"barh\",\n    x=\"State\",\n    y=[\"2010 Population\", \"2014 Population\"],\n    figsize=(5, 8),\n)\n\n\n\n\n\n\n\n\nNote: You might find it a little weird that we still specify State as the x values and Population as the y values, even thought the chart shows the on the opposite axis. Just roll with it :)\nThat’s looking pretty good now! You might think it is a little bit weird for the state names to be going in reverse alphabetical order as you go down the page. I suppose this is chosen because in “normal” math plots, the origin is 0, and the values “increase” as you move away from the origin. However, it just feels weird for it to do this when the y-axis is categorical rather than continuous. So let’s reverse it.\n\n(\n    plot_data\n    # Reverse sort the rows based on the State column\n    .sort_values(\"State\", ascending=False).plot(\n        kind=\"barh\",\n        x=\"State\",\n        y=[\"2010 Population\", \"2014 Population\"],\n        figsize=(5, 8),\n    )\n)\n\n\n\n\n\n\n\n\nOne last thing. Let’s add an axis label to the x-axis. I know there is a legend there, but I think it is still good practice to label all axes in a plot.\n\n(\n    plot_data\n    # Reverse sort the rows based on the State column\n    .sort_values(\"State\", ascending=False).plot(\n        kind=\"barh\",\n        x=\"State\",\n        y=[\"2010 Population\", \"2014 Population\"],\n        figsize=(5, 8),\n        xlabel=\"Population\"\n    )\n)\n\n\n\n\n\n\n\n\nThat’s what I’m talking about! As you can see, with just a few lines of code, you can make totally reasonable looking plots.\nDepending on your use case, you might want to sort the data so that the bars are always decreasing. That way, it’s easier for the viewer to look at overall trends, rather than being able to quickly pick out specific states.\nTo do this we will sort by population. However, we are plotting two different years on the chart, so we need to decide which way to sort it. Reasonable options might be:\n\nPick one of the years and sort by that one\nSort by the mean\nSort by the max\n\nAny of them could work depending on your situation, but let’s keep it simple and sort by the 2014 population:\n\n(\n    plot_data\n    # Reverse sort the rows based on the State column\n    .sort_values(\"2014 Population\").plot(\n        kind=\"barh\",\n        x=\"State\",\n        y=[\"2010 Population\", \"2014 Population\"],\n        figsize=(5, 8),\n        xlabel=\"Population\",\n    )\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 7.3: Stop & Think\n\n\n\n\n\nWhy might it be important to have clean, professional looking data visualizations, even during the exploratory data analysis phase of your project?\n\n\n\n\n\nPercent Population Change\nNotice any trends? One thing that we can kind of see is that there are some states that look like they had bigger population changes than other states. One of our data columns already tracks this: Population.Population Percent Change. Let’s take a look at the basic plot. We will use a lot of the same options as we did in the last one.\n\nstate_demographics.plot(\n    kind=\"barh\",\n    x=\"State\",\n    y=\"Population.Population Percent Change\",\n    figsize=(5, 8),\n    xlabel=\"Population\",\n)\n\n\n\n\n\n\n\n\nThis plot is pretty good, but we can do better with a bit more effort. Here’s a couple things that will improve it:\n\nRemove the legend since there is only one series to plot\nAdjust the bar color so that population increase blue and decrease is orange\nSort states from most increase to most decrease\n\n\nplot_data = state_demographics[\n    [\"State\", \"Population.Population Percent Change\"]\n].sort_values(\"Population.Population Percent Change\")\n\n# Create list with colors based on positive/negative values\ncolors = [\n    # Negative values will be orange, positive values will be blue\n    \"tab:orange\" if x &lt; 0 else \"tab:blue\"\n    # Do this calculation for each value in the series that we want to plot.\n    for x in plot_data[\"Population.Population Percent Change\"]\n]\n\n\nplot_data.plot(\n    kind=\"barh\",\n    x=\"State\",\n    y=\"Population.Population Percent Change\",\n    figsize=(5, 8),\n    xlabel=\"Population Change (%)\",\n    # Specify the list of colors to use\n    color=colors,\n    # Remove the legend\n    legend=False,\n)\n# Put a gray line at x=0 to help guide the viewer's attention.\nplt.axvline(x=0, color=\"#666666\")\n\n\n\n\n\n\n\n\nTo adjust the colors, we had to create a list of colors the same length as the data that we wanted to plot. In this way, each row will be given its correct color.\nNote: tab:blue and tab:orange are built-in colors in matplotlib.\nNote: You can specify these labeled arguments in any order.\nThat’s really not a bad plot! I wonder if the states with the biggest population changes were already some of the most populous states?\n\nstate_demographics.plot(\n    kind=\"scatter\",\n    x=\"Population.2010 Population\",\n    y=\"Population.Population Percent Change\",\n)\n\n\n\n\n\n\n\n\nDoesn’t look like a super strong trend there, but let’s log the x scale as maybe orders of magnitude matter here.\n\nstate_demographics.plot(\n    kind=\"scatter\",\n    x=\"Population.2010 Population\",\n    y=\"Population.Population Percent Change\",\n    # Log the x axis\n    logx=True,\n)\n\n\n\n\n\n\n\n\nNope, nothing there.\n\n\n\n\n\n\nTip 7.4: Stop & Think\n\n\n\n\n\nWhen might you use a logarithmic scale on either axis of a plot, and what insights can this reveal that a linear scale might not?\n\n\n\nDo you think any of the state demographic data is correlated with the percent population change? To try and answer this question, we can calculate all the correlation values for the columns in the state demographics data frame.\n\n# Get the correlation between columns in the data frame\ncorrelation_matrix = state_demographics.corr(\n    # Restrict the calculation to only numeric columns\n    numeric_only=True\n)\n\n\n\n\n\n\n\nTip 7.5: Stop & Think\n\n\n\n\n\nWhat can correlation values tell us during exploratory data analysis, and what are their limitations?\n\n\n\nNow that we have the correlation values, let’s take a look at them. First, we need to make a data frame with the data in a nice format that makes it easy to plot the data.\n\n# Display the correlations values between the percent population change and the other\n# variables.\nplot_data = (\n    # Select only the percent change column\n    correlation_matrix[[\"Population.Population Percent Change\"]]\n    # Then sort the rows based on their correlation to the percent population change\n    .sort_values(\"Population.Population Percent Change\")\n    # Remove the Population.Population Percent Change row from the results, since\n    # we don't care about the \"self-correlation\"\n    .drop(\"Population.Population Percent Change\")\n    # Convert the row names to a column called \"Variable\"\n    .reset_index(names=\"Variable\")\n    # Rename the 2nd column for a nicer looking chart\n    .rename(columns={\"Population.Population Percent Change\": \"Correlation\"})\n)\nplot_data\n\n\n\n\n\n\n\n\nVariable\nCorrelation\n\n\n\n\n0\nMiscellaneous.Living in Same House +1 Years\n-0.690011\n\n\n1\nAge.Percent 65 and Older\n-0.423079\n\n\n2\nHousing.Homeownership Rate\n-0.343682\n\n\n3\nMiscellaneous.Percent Under 66 Years With a Di...\n-0.306390\n\n\n4\nEthnicities.White Alone, not Hispanic or Latino\n-0.269663\n\n\n...\n...\n...\n\n\n41\nHousing.Persons per Household\n0.277861\n\n\n42\nAge.Percent Under 18 Years\n0.323823\n\n\n43\nMiscellaneous.Percent Under 65 Years Without H...\n0.330227\n\n\n44\nHousing.Households with a computer\n0.455228\n\n\n45\nAge.Percent Under 5 Years\n0.459584\n\n\n\n\n46 rows × 2 columns\n\n\n\nNote: earlier we used drop() to remove columns. Now you see that it can also be used to drop rows, depending on the arguments used. You will find Pandas has a lot of functions like this.\nNext, we can create a list of colors for the bars of the plot. We want to color variables with a positive correlation to the percent population change blue, and a negative correlation orange.\n\n# Create list with colors based on positive/negative values\ncolors = [\n    # Negative values will be orange, positive values will be blue\n    \"tab:orange\" if x &lt; 0 else \"tab:blue\"\n    # Do this calculation for each correlation value in the series.\n    for x in plot_data[\"Correlation\"]\n]\n\nFinally, let’s make the plot. The code will be very similar to the previous plots we have made.\n\nplot_data.plot(\n    kind=\"barh\",\n    x=\"Variable\",\n    y=\"Correlation\",\n    figsize=(5, 10),\n    xlabel=\"Correlation with Percent Population Change\",\n    legend=False,\n    color=colors,\n)\nplt.axvline(x=0, color=\"#666666\")\n\n\n\n\n\n\n\n\nThat’s nice! Do you see any interesting trends?\n\nAge data\n\nIncreasing proportion of young population is strongly correlated with positive population change\nIncreasing proportion of elderly population is highly correlated with decreasing population.\nMakes sense…\n\nEthnicity data\n\nThese are all positively correlated with population change: “Hispanic or Latino”, “Languages other than English at home”, “Foreign Born”.\nIn contrast, “White Alone” and “White Alone, not Hispanic or Latino” are negatively correlated with population change.\nIs this suggesting immigration is driving some of the population growth?\n\nEducation & Income data\n\n“Households with a computer”, “Households with internet”, “Bachelor’s Degree or Higher” are positively correlated with population change\n“Persons Below Poverty Level” is negatively correlated with population change\nMaybe it’s suggesting people are moving to more educated or prosperous areas?\n\n\nThese are all avenues that you might want to take a look at if this data was important to your research.\nAs you can see, there are a lot variables there and most of them have pretty weak correlation. This is a good time to show you how to filter rows based on some criteria of the data. Let’s filter out any rows that have correlation values between -0.15 and 0.15. To do this, we can use query():\n\nplot_data_with_some_correlation = plot_data.query(\n    \"Correlation &lt; -0.15 or Correlation &gt; 0.15\"\n)\n\nThe query() function is very cool and highly flexible and dynamic. We will see a few more examples of it later in the tutorial. For now, just know that you can access columns of your data frame and use them to filter rows in a natural looking way.\nOnce we have the filtered data, we can regenerate the color list, and plot the data.\n\n# We need to redo the colors again.\ncolors = [\n    \"tab:orange\" if x &lt; 0 else \"tab:blue\"\n    for x in plot_data_with_some_correlation[\"Correlation\"]\n]\n\nplot_data_with_some_correlation.plot(\n    kind=\"barh\",\n    x=\"Variable\",\n    y=\"Correlation\",\n    figsize=(5, 10),\n    xlabel=\"Correlation with Percent Population Change\",\n    legend=False,\n    color=colors,\n)\nplt.axvline(x=0, color=\"#666666\")\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to take correlation values at face value: it doesn’t measure all types of dependencies and it can be tempting to assign causation to things that have high correlation. So for correlation, it’s always a good idea to look at your data whenever possible. Let’s do that now with some of the most highly correlated or anti-correlated variables. We are going to use seaborn for this, as it makes it super simple to compare multiple variables in a single plot. We can even put regression lines with confidence intervals on the plots by setting the kind=\"reg\" argument!\nHere is a plot containing some of the most highly correlated variables:\n\ncolumns = [\n    \"Population.Population Percent Change\",\n    \"Age.Percent Under 5 Years\",\n    \"Housing.Households with a computer\",\n    \"Ethnicities.Hispanic or Latino\",\n]\nsns.pairplot(\n    state_demographics[columns],\n    kind=\"reg\",\n    x_vars=columns[0],\n    y_vars=columns[1:],\n    height=4,\n)\n\n\n\n\n\n\n\n\nAnd one with the most highly anti-correlated variables:\n\ncolumns = [\n    \"Population.Population Percent Change\",\n    \"Ethnicities.White Alone, not Hispanic or Latino\",\n    \"Age.Percent 65 and Older\",\n    \"Miscellaneous.Living in Same House +1 Years\",\n]\nsns.pairplot(\n    state_demographics[columns],\n    kind=\"reg\",\n    x_vars=columns[0],\n    y_vars=columns[1:],\n    height=4,\n)\n\n\n\n\n\n\n\n\nNeat, at least we have discovered a couple of variables that we may want to look into. One definite potential issue I see here is that the states that have dropped in population seem to be off on their own in all these plots. It would probably be a good idea to see if we are violating any major assumptions of the basic linear model with them, or at least see if they (or any other points) have too much leverage and are misleading us. But that is a topic for a different course!\nBefore we move on, let’s do one more thing with correlation as it is so common: a correlation heatmap! We can use seaborn’s clustermap() for this:\n\nclustermap_plot = sns.clustermap(\n    correlation_matrix,\n    # Specify the complete linkage for calculating clusters\n    method=\"complete\",\n    # The relative space the dendrograms will occupy\n    dendrogram_ratio=0.05,\n    # Use the \"icefire\" diverging palette\n    cmap=\"icefire\",\n    # Make sure the min color value occurs at -1\n    vmin=-1,\n    # Make sure the max color value occurs at 1\n    vmax=1,\n    figsize=(12, 12),\n    # Remove the x-axis tick labels\n    xticklabels=True,\n    # Remove the y-axis tick labels\n    yticklabels=True,\n    # Set the options for the color palette legend\n    cbar_kws={\n        \"label\": \"Correlation\",  # Set the label for the color palette legend\n        \"location\": \"bottom\",  # Set the location of the color palette legend\n    },\n    # Set the location for the color palette legend\n    # This is for top left\n    # cbar_pos=(\n    #     0.03,  # Distance from the left\n    #     0.92,  # Distance from the bottom\n    #     0.10,  # Width\n    #     0.05,  # Height\n    # ),\n    cbar_pos=(\n        0.65,  # Distance from the left\n        0.20,  # Distance from the bottom\n        0.25,  # Width\n        0.14,  # Height\n    ),\n)\n\n\n\n\n\n\n\n\nCool! There are a couple things to note about this:\n\nOne important consideration is setting the minimum and maximum values of your color palette. While you don’t always need to adjust these parameters (and sometimes only need to set one or two of them), being aware of this option is important. Making these adjustments ensures the most informative part of your color palette corresponds to the most relevant range in your data.\nFor fine-tuning your colorbar, check out the cbar_kws parameter, which passes arguments directly to matplotlib’s colorbar() method. This pattern of documentation referral is something you’ll encounter frequently in the Python ecosystem. Libraries will often direct you to another component’s documentation for parameter details, especially when they’re simply passing those arguments through to underlying functions.\nDon’t forget about customizing your clustering approach! The linkage type and distance metric can impact your hierarchical clustering results. The SciPy documentation provides comprehensive details on these options, allowing you to select methods that best represent the relationships in your data.\n\n\n\n\n\n\n\nTip 7.6: Stop & Think\n\n\n\n\n\nWhy is it important to ensure that your color palettes represent the correct data? For example,\n\nHow would it change your interpretation if the center of the palette (the black part) was on 0.2 rather than zero?\nHow would it change your interpretation if the brightest blue was -0.2 but the brightest orange was 1.0?\n\n\n\n\nThere is a lot to unpack with this figure, but the most obvious thing that I see is that blob of bright orange in the bottom right. If you look at the data in those columns, you will see that they are numbers with real magnitude that will be pretty highly influenced by the number of people in the state. A lot of the other columns are not like this. Wouldn’t it be interesting to take those counting-style numbers and normalize them by the state population? E.g., something like manufacturers shipments per 10k people. Let’s do that now.\n\n# Make a copy of the state demographics data\nnormalized_state_demographics = state_demographics.copy()\n\n# These are the columns that we want to divide by the population\ncolumns = [\n    \"Employment.Firms.Men-Owned\",\n    \"Employment.Firms.Minority-Owned\",\n    \"Employment.Firms.Nonminority-Owned\",\n    \"Employment.Firms.Nonveteran-Owned\",\n    \"Employment.Firms.Total\",\n    \"Employment.Firms.Veteran-Owned\",\n    \"Employment.Firms.Women-Owned\",\n    \"Employment.Nonemployer Establishments\",\n    \"Housing.Households\",\n    \"Housing.Housing Units\",\n    \"Miscellaneous.Manufacturers Shipments\",\n    \"Miscellaneous.Veterans\",\n    \"Sales.Accommodation and Food Services Sales\",\n    \"Sales.Retail Sales\",\n]\n\n# Loop through each of the columns\nfor column in columns:\n    # Normalize the data: X / Population * 10,000 people\n    normalized_data = (\n        normalized_state_demographics[column]\n        / state_demographics[\"Population.2010 Population\"]\n        * 10_000\n    )\n    # Replace the original column with the normalized column\n    normalized_state_demographics[column] = normalized_data\n\nNow that we have normalized data, we can generate the correlation matrix and plot the heatmap.\n\n# Generate a correlation matrix of the numeric columns\ncorrelation_matrix = normalized_state_demographics.corr(numeric_only=True)\n\n# Draw the clustered heatmap\nclustermap_plot = sns.clustermap(\n    correlation_matrix,\n    # Specify the complete linkage for calculating clusters\n    method=\"complete\",\n    # The relative space the dendrograms will occupy\n    dendrogram_ratio=0.05,\n    # Use the \"icefire\" diverging palette\n    cmap=\"icefire\",\n    # Make sure the min color value occurs at -1\n    vmin=-1,\n    # Make sure the max color value occurs at 1\n    vmax=1,\n    figsize=(12, 12),\n    # Remove the x-axis tick labels\n    xticklabels=True,\n    # Remove the y-axis tick labels\n    yticklabels=True,\n    # Set the options for the color palette legend\n    cbar_kws={\n        \"label\": \"Correlation\",  # Set the label for the color palette legend\n        \"location\": \"bottom\",  # Set the location of the color palette legend\n    },\n    # Set the location for the color palette legend\n    cbar_pos=(\n        0.65,  # Distance from the left\n        0.20,  # Distance from the bottom\n        0.25,  # Width\n        0.14,  # Height\n    ),\n)\n\n\n\n\n\n\n\n\nAfter “normalizing out” the effect of population on a bunch of the variables, we can see some trends that were a bit masked before. For example, we can now see some correlation between education, wealth, and income, as well as some potentially interesting trends around ethnicity and age.\n\n\n\n\n\n\nTip 7.7: Stop & Think\n\n\n\n\n\nWhat insights can a correlation heatmap with clustering provide that a simple correlation table cannot?\n\n\n\n\n\n\nState Demographics Wrap-Up\nWe went over a lot of material in this section! I hope it gave you a sense of how exploratory data analysis can go using Pandas and Seaborn: you begin by exploring the data to get a sense of it, identify patterns and trends, and then dive deeper into those patterns to better understand the system you’re analyzing.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#cancer-data",
    "href": "exploratory_data_analysis.html#cancer-data",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Cancer data",
    "text": "Cancer data\nNow that you have a basic understanding of using Pandas for EDA, let’s take a look at some public health data.\nWe’re going to look at the cancer data from CORGIS. This dataset contains information about cancer deaths between 2007 and 2013 in each state. Specifically, these are deaths from breast, lung, and colorectal cancer. In addition to the total death rate, the rates have also been broken down by age, race, and sex.\n\nBasic Info\nTo start, we need to read the CSV file with the cancer data.\n\ncancer = pd.read_csv(\"./_data/cancer.csv\")\ncancer.head()\n\n\n\n\n\n\n\n\nState\nTotal.Rate\nTotal.Number\nTotal.Population\nRates.Age.&lt; 18\nRates.Age.18-45\nRates.Age.45-64\nRates.Age.&gt; 64\nRates.Age and Sex.Female.&lt; 18\nRates.Age and Sex.Male.&lt; 18\n...\nTypes.Lung.Age and Sex.Male.45 - 64\nTypes.Lung.Age and Sex.Female.&gt; 64\nTypes.Lung.Age and Sex.Male.&gt; 64\nTypes.Lung.Race.White\nTypes.Lung.Race.White non-Hispanic\nTypes.Lung.Race.Black\nTypes.Lung.Race.Black non-Hispanic\nTypes.Lung.Race.Asian\nTypes.Lung.Race.Indigenous\nTypes.Lung.Race.Hispanic\n\n\n\n\n0\nAlabama\n214.2\n71529.0\n33387205.0\n2.0\n18.5\n244.7\n1017.8\n2.0\n2.1\n...\n102.9\n221.7\n457.4\n59.9\n60.4\n52.6\n52.8\n23.0\n22.9\n14.8\n\n\n1\nAlaska\n128.1\n6361.0\n4966180.0\n1.7\n11.8\n170.9\n965.2\n0.0\n0.0\n...\n50.3\n268.3\n335.0\n48.7\n49.5\n45.6\n47.9\n33.0\n74.4\n0.0\n\n\n2\nArizona\n165.6\n74286.0\n44845598.0\n2.5\n13.6\n173.6\n840.2\n2.6\n2.5\n...\n47.0\n191.9\n275.8\n39.5\n42.2\n38.2\n40.4\n21.3\n11.1\n21.6\n\n\n3\nArkansas\n223.9\n45627.0\n20382448.0\n2.3\n17.6\n250.1\n1048.3\n2.6\n2.0\n...\n106.5\n248.7\n484.7\n63.4\n64.2\n62.9\n63.0\n18.1\n16.2\n14.6\n\n\n4\nCalifornia\n150.9\n393980.0\n261135696.0\n2.6\n13.7\n163.7\n902.4\n2.4\n2.8\n...\n36.8\n192.5\n269.0\n37.2\n42.6\n46.5\n48.6\n25.8\n18.4\n18.3\n\n\n\n\n5 rows × 75 columns\n\n\n\nLike before, we will use describe() to get a basic overview of the numeric data columns.\n\ncancer.describe().style.format(precision=1)\n\n\n\n\n\n\n \nTotal.Rate\nTotal.Number\nTotal.Population\nRates.Age.&lt; 18\nRates.Age.18-45\nRates.Age.45-64\nRates.Age.&gt; 64\nRates.Age and Sex.Female.&lt; 18\nRates.Age and Sex.Male.&lt; 18\nRates.Age and Sex.Female.18 - 45\nRates.Age and Sex.Male.18 - 45\nRates.Age and Sex.Female.45 - 64\nRates.Age and Sex.Male.45 - 64\nRates.Age and Sex.Female.&gt; 64\nRates.Age and Sex.Male.&gt; 64\nRates.Race.White\nRates.Race.White non-Hispanic\nRates.Race.Black\nRates.Race.Asian\nRates.Race.Indigenous\nRates.Race and Sex.Female.White\nRates.Race and Sex.Female.White non-Hispanic\nRates.Race and Sex.Female.Black\nRates.Race and Sex.Female.Black non-Hispanic\nRates.Race and Sex.Female.Asian\nRates.Race and Sex.Female.Indigenous\nRates.Race and Sex.Male.White\nRates.Race and Sex.Male.White non-Hispanic\nRates.Race and Sex.Male.Black\nRates.Race and Sex.Male.Black non-Hispanic\nRates.Race and Sex.Male.Asian\nRates.Race and Sex.Male.Indigenous\nRates.Race.Hispanic\nRates.Race and Sex.Female.Hispanic\nRates.Race and Sex.Male.Hispanic\nTypes.Breast.Total\nTypes.Breast.Age.18 - 44\nTypes.Breast.Age.45 - 64\nTypes.Breast.Age.&gt; 64\nTypes.Breast.Race.White\nTypes.Breast.Race.White non-Hispanic\nTypes.Breast.Race.Black\nTypes.Breast.Race.Black non-Hispanic\nTypes.Breast.Race.Asian\nTypes.Breast.Race.Indigenous\nTypes.Breast.Race.Hispanic\nTypes.Colorectal.Total\nTypes.Colorectal.Age and Sex.Female.18 - 44\nTypes.Colorectal.Age and Sex.Male.18 - 44\nTypes.Colorectal.Age and Sex.Female.45 - 64\nTypes.Colorectal.Age and Sex.Male.45 - 64\nTypes.Colorectal.Age and Sex.Female.&gt; 64\nTypes.Colorectal.Age and Sex.Male.&gt; 64\nTypes.Colorectal.Race.White\nTypes.Colorectal.Race.White non-Hispanic\nTypes.Colorectal.Race.Black\nTypes.Colorectal.Race.Black non-Hispanic\nTypes.Colorectal.Race.Asian\nTypes.Colorectal.Race.Indigenous\nTypes.Colorectal.Race.Hispanic\nTypes.Lung.Total\nTypes.Lung.Age and Sex.Female.18 - 44\nTypes.Lung.Age and Sex.Male.18 - 44\nTypes.Lung.Age and Sex.Female.45 - 64\nTypes.Lung.Age and Sex.Male.45 - 64\nTypes.Lung.Age and Sex.Female.&gt; 64\nTypes.Lung.Age and Sex.Male.&gt; 64\nTypes.Lung.Race.White\nTypes.Lung.Race.White non-Hispanic\nTypes.Lung.Race.Black\nTypes.Lung.Race.Black non-Hispanic\nTypes.Lung.Race.Asian\nTypes.Lung.Race.Indigenous\nTypes.Lung.Race.Hispanic\n\n\n\n\ncount\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n51.0\n\n\nmean\n190.7\n78723.7\n42401510.5\n2.1\n14.8\n197.6\n980.9\n1.7\n2.0\n16.0\n13.5\n177.8\n218.3\n826.7\n1181.8\n171.0\n173.2\n182.8\n99.0\n111.1\n145.6\n147.7\n142.7\n146.6\n85.0\n88.7\n206.4\n208.7\n222.9\n228.2\n107.2\n129.6\n98.2\n82.2\n115.5\n26.0\n4.0\n34.9\n102.3\n21.2\n21.6\n22.6\n23.2\n6.0\n5.0\n8.6\n17.5\n1.1\n1.5\n14.7\n21.4\n81.8\n101.5\n15.2\n15.4\n17.0\n17.4\n7.0\n6.9\n8.2\n53.2\n1.3\n1.4\n45.6\n64.8\n224.7\n355.0\n48.2\n49.3\n44.1\n44.5\n19.3\n27.3\n16.2\n\n\nstd\n28.6\n80861.3\n47842444.3\n0.5\n2.2\n31.3\n75.2\n0.8\n0.9\n2.5\n2.0\n23.0\n41.0\n65.4\n105.2\n15.0\n15.0\n48.7\n21.6\n68.3\n11.4\n11.5\n58.1\n58.2\n23.2\n61.7\n21.5\n21.7\n83.5\n83.1\n41.7\n90.6\n24.1\n24.5\n36.2\n3.1\n0.9\n4.9\n9.4\n1.2\n1.5\n12.5\n12.7\n5.5\n8.1\n5.8\n2.7\n0.6\n0.7\n2.6\n4.2\n9.5\n10.9\n1.7\n1.8\n9.2\n9.3\n5.4\n9.9\n5.1\n12.6\n0.8\n0.8\n11.2\n21.3\n33.3\n69.9\n9.4\n9.3\n20.5\n21.4\n10.4\n26.4\n8.0\n\n\nmin\n98.5\n6361.0\n3931624.0\n0.0\n10.0\n132.3\n735.8\n0.0\n0.0\n10.3\n9.8\n126.1\n138.5\n611.6\n884.9\n127.8\n129.1\n0.0\n0.0\n0.0\n109.9\n110.5\n0.0\n0.0\n0.0\n0.0\n145.1\n146.2\n0.0\n0.0\n0.0\n0.0\n39.5\n0.0\n0.0\n17.4\n0.0\n27.8\n62.3\n17.5\n18.2\n0.0\n0.0\n0.0\n0.0\n0.0\n9.0\n0.0\n0.0\n10.1\n14.5\n59.7\n72.4\n10.0\n9.8\n0.0\n0.0\n0.0\n0.0\n0.0\n15.6\n0.0\n0.0\n18.7\n26.0\n92.7\n153.2\n20.4\n20.6\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n25%\n176.5\n20631.0\n11869909.5\n2.0\n13.4\n175.0\n943.5\n1.8\n2.0\n14.1\n12.4\n162.9\n187.9\n803.5\n1112.8\n162.8\n165.7\n162.6\n90.7\n58.0\n140.4\n143.6\n135.6\n146.3\n79.5\n43.7\n195.4\n197.0\n200.1\n216.6\n96.4\n66.6\n82.4\n72.5\n91.5\n24.5\n3.5\n31.2\n98.7\n20.4\n20.8\n21.1\n23.6\n0.0\n0.0\n0.0\n16.1\n1.0\n1.4\n13.2\n18.2\n74.5\n93.3\n14.3\n14.5\n15.1\n16.4\n0.0\n0.0\n5.9\n46.5\n0.9\n0.9\n38.8\n49.0\n209.6\n313.3\n42.8\n43.7\n38.2\n40.0\n18.4\n4.5\n13.7\n\n\n50%\n196.1\n54930.0\n30348057.0\n2.2\n14.6\n189.3\n999.6\n2.0\n2.3\n16.0\n13.4\n172.2\n208.3\n845.5\n1195.0\n171.3\n173.9\n194.4\n97.6\n102.2\n146.2\n149.9\n161.7\n165.6\n85.6\n78.7\n205.4\n207.8\n246.2\n252.7\n112.5\n114.2\n98.0\n84.8\n119.4\n26.6\n4.2\n34.6\n102.2\n21.1\n21.4\n27.4\n28.4\n8.5\n0.0\n10.5\n17.7\n1.3\n1.7\n14.3\n21.0\n82.4\n102.1\n15.2\n15.2\n21.2\n21.5\n9.1\n0.0\n9.0\n52.4\n1.4\n1.4\n45.7\n59.7\n225.8\n342.6\n48.1\n48.8\n47.0\n48.6\n22.2\n20.9\n18.1\n\n\n75%\n210.8\n93328.0\n46503256.0\n2.4\n16.2\n217.8\n1031.4\n2.2\n2.5\n17.7\n14.4\n193.3\n243.3\n871.1\n1257.8\n180.2\n183.4\n220.8\n112.7\n162.1\n152.4\n155.2\n181.7\n182.8\n94.8\n127.7\n218.1\n220.2\n278.7\n282.1\n129.1\n198.4\n112.2\n95.5\n136.9\n27.6\n4.7\n37.7\n107.8\n22.1\n22.2\n31.1\n31.9\n10.7\n10.6\n12.2\n19.4\n1.5\n1.9\n15.3\n23.5\n88.7\n110.7\n16.6\n16.7\n23.2\n23.5\n10.6\n13.9\n11.6\n61.8\n1.8\n1.8\n53.4\n77.6\n248.2\n391.0\n54.5\n55.0\n60.0\n60.3\n25.5\n44.2\n20.4\n\n\nmax\n254.6\n393980.0\n261135696.0\n2.7\n20.3\n263.9\n1110.2\n2.9\n3.2\n22.8\n19.0\n233.2\n310.0\n916.2\n1372.7\n204.9\n205.9\n235.1\n149.0\n248.0\n170.5\n171.2\n195.6\n198.5\n130.3\n219.4\n253.6\n255.1\n319.5\n321.2\n172.2\n303.3\n168.5\n140.8\n202.3\n31.8\n5.6\n56.6\n129.6\n24.0\n25.5\n34.7\n35.0\n15.8\n27.4\n18.2\n24.4\n2.0\n3.0\n21.9\n34.1\n99.8\n124.6\n19.1\n19.2\n26.7\n27.2\n18.0\n34.7\n18.0\n81.3\n2.9\n3.1\n77.1\n112.6\n298.3\n519.1\n71.5\n71.9\n73.2\n73.6\n33.9\n87.5\n35.2\n\n\n\n\n\n\n\nEffect of Population\nIn the state demographics data, we saw a strong dependency between some of the variables and state population. We would also expect there to be a pretty strong dependency between the total number of cancer deaths and the total state population. Let’s see if that is the case:\n\ncancer.plot(\n    kind=\"scatter\",\n    x=\"Total.Population\",\n    y=\"Total.Number\",\n    loglog=True,\n)\n\n\n\n\n\n\n\n\nJust to make it super clear, let’s do the same plot, but this time use the rate of cancer deaths per 100k people rather than the raw totals. (I bet you can guess how it will look!)\n\ncancer.plot(\n    kind=\"scatter\",\n    x=\"Total.Population\",\n    y=\"Total.Rate\",\n    logx=True,\n)\n\n\n\n\n\n\n\n\nBecause of this, we will use the rates per 100k people rather than total numbers for this section.\n\n\nComparing States\nLet’s see if there are any high-level differences between individual states and rates of cancer deaths.\n\n(\n    cancer\n    # Sort the values by the rate of cancer deaths\n    .sort_values(\"Total.Rate\").plot(\n        # Make a horizontal bar chart\n        kind=\"barh\",\n        x=\"State\",\n        y=\"Total.Rate\",\n        # Adjust the figure size so the labels print nicely\n        figsize=(5, 8),\n        # Give an informative x-axis label\n        xlabel=\"Cancer Rate (per 100k people)\",\n        # Don't bother with the legend as we only have one data series to plot\n        legend=False,\n    )\n)\n\n\n\n\n\n\n\n\nThere is about a 2.5 times difference between the state with the highest rate of cancer deaths (West Virginia) as compared to the state with the lowest (Utah).\nLet’s make a boxplot to see the spread of the data.\n\ncancer.plot(kind=\"box\", y=\"Total.Rate\")\n\n\n\n\n\n\n\n\nCool, so we see some variation in the rate of cancer deaths across states. Let’s try and find out if there are any variables in the state demographics data that are correlated with death rates for different types of cancer.\n\n\nCancer Deaths and Demographics\nThis dataset has data for three types of cancer, breast cancer, colorectal cancer, and lung cancer, so we will want to pull out those columns.\n\ncancer_death_rates = cancer[\n    [\"State\", \"Types.Breast.Total\", \"Types.Colorectal.Total\", \"Types.Lung.Total\"]\n]\ncancer_death_rates.head()\n\n\n\n\n\n\n\n\nState\nTypes.Breast.Total\nTypes.Colorectal.Total\nTypes.Lung.Total\n\n\n\n\n0\nAlabama\n27.4\n19.4\n66.4\n\n\n1\nAlaska\n17.8\n11.9\n36.6\n\n\n2\nArizona\n23.3\n14.9\n42.3\n\n\n3\nArkansas\n27.9\n21.2\n73.3\n\n\n4\nCalifornia\n23.0\n14.0\n34.5\n\n\n\n\n\n\n\nWe want to include the normalized state demographic data in with the cancer data, but we don’t want all the columns. Earlier, we saw that we can use filter() to select columns using regular expressions. We will do that again here to select only the categories of variables that we are interested in.\n\n# Create a filtered version of normalized_state_demographics data frame containing:\n# - State column\n# - Columns starting with \"Age\"\n# - Columns starting with \"Education\"\n# - Columns starting with \"Ethnicities\"\n# - Columns starting with \"Housing\"\n# - Columns starting with \"Income\"\n# This uses regex patterns with filter() to select columns,\n# then combines them using pd.concat()\nfiltered_normalized_state_demographics = pd.concat(\n    [\n        normalized_state_demographics.filter([\"State\"]),\n        normalized_state_demographics.filter(regex=r\"^Age\"),\n        normalized_state_demographics.filter(regex=r\"^Education\"),\n        normalized_state_demographics.filter(regex=r\"^Ethnicities\"),\n        normalized_state_demographics.filter(regex=r\"^Housing\"),\n        normalized_state_demographics.filter(regex=r\"^Income\"),\n    ],\n    axis=\"columns\",\n)\nfiltered_normalized_state_demographics.head()\n\n\n\n\n\n\n\n\nState\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nEducation.High School or Higher\nEducation.Bachelor's Degree or Higher\nEthnicities.White Alone\nEthnicities.Black Alone\nEthnicities.American Indian and Alaska Native Alone\nEthnicities.Asian Alone\n...\nHousing.Housing Units\nHousing.Homeownership Rate\nHousing.Median Value of Owner-Occupied Units\nHousing.Households\nHousing.Persons per Household\nHousing.Households with a computer\nHousing.Households with a Internet\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\n\n\n\n\n0\nConnecticut\n5.1\n20.4\n17.7\n90.6\n39.3\n79.7\n12.2\n0.6\n5.0\n...\n4266.789625\n66.1\n275400\n3835.223275\n2.53\n90.8\n85.5\n78444\n44496\n10.0\n\n\n1\nDelaware\n5.6\n20.9\n19.4\n90.0\n32.0\n69.2\n23.2\n0.7\n4.1\n...\n4942.245198\n71.2\n251100\n4046.199387\n2.57\n91.6\n85.0\n68287\n35450\n11.3\n\n\n2\nDistrict of Columbia\n6.4\n18.2\n12.4\n90.9\n58.5\n46.0\n46.0\n0.6\n4.5\n...\n5364.478340\n41.6\n601500\n4726.194611\n2.30\n91.8\n82.6\n86420\n56147\n13.5\n\n\n3\nFlorida\n5.3\n19.7\n20.9\n88.2\n29.9\n77.3\n16.9\n0.5\n3.0\n...\n5145.217009\n65.4\n215300\n4114.772322\n2.65\n91.5\n83.0\n55660\n31619\n12.7\n\n\n4\nGeorgia\n6.2\n23.6\n14.3\n87.1\n31.3\n60.2\n32.6\n0.5\n4.4\n...\n4519.558039\n63.3\n176000\n3879.988270\n2.70\n90.2\n81.3\n58700\n31067\n13.3\n\n\n\n\n5 rows × 24 columns\n\n\n\nNow, we can merge the data:\n\ncancer_demographics = cancer_death_rates.merge(\n    filtered_normalized_state_demographics, on=\"State\", how=\"inner\"\n)\ncancer_demographics.head()\n\n\n\n\n\n\n\n\nState\nTypes.Breast.Total\nTypes.Colorectal.Total\nTypes.Lung.Total\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nEducation.High School or Higher\nEducation.Bachelor's Degree or Higher\nEthnicities.White Alone\n...\nHousing.Housing Units\nHousing.Homeownership Rate\nHousing.Median Value of Owner-Occupied Units\nHousing.Households\nHousing.Persons per Household\nHousing.Households with a computer\nHousing.Households with a Internet\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\n\n\n\n\n0\nAlabama\n27.4\n19.4\n66.4\n6.0\n22.2\n17.3\n86.2\n25.5\n69.1\n...\n4780.278660\n68.8\n142700\n3907.941778\n2.55\n85.5\n76.4\n50536\n27928\n15.5\n\n\n1\nAlaska\n17.8\n11.9\n36.6\n7.0\n24.6\n12.5\n92.8\n29.6\n65.3\n...\n4503.520686\n64.3\n270400\n3567.092960\n2.80\n94.1\n85.5\n77640\n36787\n10.1\n\n\n2\nArizona\n23.3\n14.9\n42.3\n5.9\n22.5\n18.0\n87.1\n29.5\n82.6\n...\n4812.222809\n64.4\n225500\n4022.623845\n2.68\n91.7\n84.1\n58945\n30694\n13.5\n\n\n3\nArkansas\n27.9\n21.2\n73.3\n6.2\n23.2\n17.4\n86.6\n23.0\n79.0\n...\n4763.950838\n65.6\n127800\n3971.548583\n2.52\n86.2\n73.0\n47597\n26577\n16.2\n\n\n4\nCalifornia\n23.0\n14.0\n34.5\n6.0\n22.5\n14.8\n83.3\n33.9\n71.9\n...\n3856.324950\n54.8\n505000\n3501.444518\n2.95\n93.0\n86.7\n75235\n36955\n11.8\n\n\n\n\n5 rows × 27 columns\n\n\n\nLet’s do another correlation matrix:\n\ncancer_demographics_full_correlation_matrix = cancer_demographics.corr(\n    numeric_only=True\n)\ncancer_demographics_full_correlation_matrix\n\n\n\n\n\n\n\n\nTypes.Breast.Total\nTypes.Colorectal.Total\nTypes.Lung.Total\nAge.Percent Under 5 Years\nAge.Percent Under 18 Years\nAge.Percent 65 and Older\nEducation.High School or Higher\nEducation.Bachelor's Degree or Higher\nEthnicities.White Alone\nEthnicities.Black Alone\n...\nHousing.Housing Units\nHousing.Homeownership Rate\nHousing.Median Value of Owner-Occupied Units\nHousing.Households\nHousing.Persons per Household\nHousing.Households with a computer\nHousing.Households with a Internet\nIncome.Median Houseold Income\nIncome.Per Capita Income\nIncome.Persons Below Poverty Level\n\n\n\n\nTypes.Breast.Total\n1.000000\n0.820604\n0.738585\n-0.388781\n-0.498962\n0.451119\n-0.131677\n-0.019331\n0.051875\n0.419682\n...\n0.262656\n0.042882\n-0.288228\n0.375702\n-0.654101\n-0.541995\n-0.398493\n-0.275447\n0.054492\n0.331260\n\n\nTypes.Colorectal.Total\n0.820604\n1.000000\n0.861898\n-0.314083\n-0.381218\n0.600918\n-0.158118\n-0.361382\n0.097213\n0.156050\n...\n0.280906\n0.186077\n-0.418381\n0.264358\n-0.618531\n-0.753502\n-0.612936\n-0.502661\n-0.269516\n0.443643\n\n\nTypes.Lung.Total\n0.738585\n0.861898\n1.000000\n-0.435992\n-0.411047\n0.609561\n-0.216806\n-0.437903\n0.146942\n0.188569\n...\n0.303359\n0.331816\n-0.530434\n0.203042\n-0.571173\n-0.706384\n-0.577125\n-0.574831\n-0.363581\n0.450317\n\n\nAge.Percent Under 5 Years\n-0.388781\n-0.314083\n-0.435992\n1.000000\n0.879837\n-0.753746\n-0.070116\n-0.171279\n-0.169575\n0.064308\n...\n-0.205294\n-0.165581\n-0.069280\n-0.084758\n0.401366\n0.095203\n-0.098394\n-0.037176\n-0.190507\n0.113660\n\n\nAge.Percent Under 18 Years\n-0.498962\n-0.381218\n-0.411047\n0.879837\n1.000000\n-0.617604\n-0.128687\n-0.408223\n0.064120\n-0.122481\n...\n-0.343985\n0.172119\n-0.309091\n-0.300107\n0.515735\n0.062107\n-0.115255\n-0.207411\n-0.458687\n0.107673\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nHousing.Households with a computer\n-0.541995\n-0.753502\n-0.706384\n0.095203\n0.062107\n-0.364809\n0.471508\n0.575901\n-0.000697\n-0.280283\n...\n-0.127756\n-0.264749\n0.589177\n-0.010558\n0.370020\n1.000000\n0.923669\n0.749979\n0.565945\n-0.791368\n\n\nHousing.Households with a Internet\n-0.398493\n-0.612936\n-0.577125\n-0.098394\n-0.115255\n-0.201042\n0.516615\n0.637379\n0.045414\n-0.323449\n...\n-0.215326\n-0.206746\n0.602584\n-0.039747\n0.257074\n0.923669\n1.000000\n0.820815\n0.656077\n-0.878474\n\n\nIncome.Median Houseold Income\n-0.275447\n-0.502661\n-0.574831\n-0.037176\n-0.207411\n-0.332490\n0.433715\n0.825304\n-0.311793\n-0.017949\n...\n-0.301793\n-0.429184\n0.799816\n-0.114904\n0.247522\n0.749979\n0.820815\n1.000000\n0.895940\n-0.755590\n\n\nIncome.Per Capita Income\n0.054492\n-0.269516\n-0.363581\n-0.190507\n-0.458687\n-0.233495\n0.384923\n0.925678\n-0.274597\n0.169388\n...\n-0.059754\n-0.547328\n0.734335\n0.185974\n-0.099105\n0.565945\n0.656077\n0.895940\n1.000000\n-0.586132\n\n\nIncome.Persons Below Poverty Level\n0.331260\n0.443643\n0.450317\n0.113660\n0.107673\n0.068212\n-0.743634\n-0.566515\n-0.176359\n0.434675\n...\n0.074541\n-0.024846\n-0.435180\n-0.057847\n-0.051614\n-0.791368\n-0.878474\n-0.755590\n-0.586132\n1.000000\n\n\n\n\n26 rows × 26 columns\n\n\n\nThis will give every variable against all other variables, but in this case we don’t want to plot all that. We just want to see the correlation of the state demographic data to the cancer death data, and not the state demographic data with itself again.\nSo, let’s filter out the rows and columns that we don’t need.\n\ncancer_columns = [\"Types.Breast.Total\", \"Types.Colorectal.Total\", \"Types.Lung.Total\"]\ncancer_demographics_correlation_matrix = cancer_demographics_full_correlation_matrix[\n    cancer_columns\n].drop(cancer_columns, axis=\"rows\")\ncancer_demographics_correlation_matrix\n\n\n\n\n\n\n\n\nTypes.Breast.Total\nTypes.Colorectal.Total\nTypes.Lung.Total\n\n\n\n\nAge.Percent Under 5 Years\n-0.388781\n-0.314083\n-0.435992\n\n\nAge.Percent Under 18 Years\n-0.498962\n-0.381218\n-0.411047\n\n\nAge.Percent 65 and Older\n0.451119\n0.600918\n0.609561\n\n\nEducation.High School or Higher\n-0.131677\n-0.158118\n-0.216806\n\n\nEducation.Bachelor's Degree or Higher\n-0.019331\n-0.361382\n-0.437903\n\n\n...\n...\n...\n...\n\n\nHousing.Households with a computer\n-0.541995\n-0.753502\n-0.706384\n\n\nHousing.Households with a Internet\n-0.398493\n-0.612936\n-0.577125\n\n\nIncome.Median Houseold Income\n-0.275447\n-0.502661\n-0.574831\n\n\nIncome.Per Capita Income\n0.054492\n-0.269516\n-0.363581\n\n\nIncome.Persons Below Poverty Level\n0.331260\n0.443643\n0.450317\n\n\n\n\n23 rows × 3 columns\n\n\n\nAnd now we can generate another heatmap.\n\nsns.clustermap(\n    cancer_demographics_correlation_matrix,\n    # Specify the complete linkage for calculating clusters\n    method=\"complete\",\n    # The relative space the dendrograms will occupy\n    dendrogram_ratio=0.15,\n    # Use the \"icefire\" diverging palette\n    cmap=\"icefire\",\n    # Make sure the min color value occurs at -1\n    vmin=-1,\n    # Make sure the max color value occurs at 1\n    vmax=1,\n    # figsize=(12, 12),\n    # Remove the x-axis tick labels\n    xticklabels=True,\n    # Remove the y-axis tick labels\n    yticklabels=True,\n    # Set the options for the color palette legend\n    cbar_kws={\n        \"label\": \"Correlation\",  # Set the label for the color palette legend\n        \"location\": \"bottom\",  # Set the location of the color palette legend\n    },\n    # Set the location for the color palette legend\n    # This is for top left\n    cbar_pos=(\n        0.03,  # Distance from the left\n        0.92,  # Distance from the bottom\n        0.10,  # Width\n        0.05,  # Height\n    ),\n)\n\n\n\n\n\n\n\n\nWe can definitely see some patterns emerging. The groups most highly correlated with cancer deaths were people 65 and older, followed by people below the poverty line. In contrast, groups that were least correlated with cancer deaths included those who lived in households with a computer and fewer people per household.\nThere are also some interesting patterns related to ethnicity. For example, there is almost no correlation between ethnicity and colorectal or lung cancer death for Pacific Islanders, but a negative correlation for breast cancer. Meanwhile, there is a higher correlation between breast cancer deaths and ethnicity for Black people than for colorectal or lung cancer, which are both close to zero.\nThese patterns bring up some interesting questions. Are Pacific Islanders less likely to develop breast cancer than Black people? Or do the two groups tend to develop different types of breast cancer? Or are there social determinants or biases in healthcare that make breast cancer more deadly for Black people?\nExploratory data analysis, as you’ve seen, will show trends, but it is up to scientists and other domain experts to interpret the data and to determine causes.\n\n\nCancer Data Wrap-Up\nIn this section we learned some tricks about how to combine multiple datasets, and how to look for interesting data trends by including more metadata into our analysis.\nYou may have noticed that there are a lot more variables in the cancer dataset. You could definitely imagine doing a lot more with this data!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#public-health-data",
    "href": "exploratory_data_analysis.html#public-health-data",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Public Health Data",
    "text": "Public Health Data\nLet’s check out some public health data next. This data is a bit different in that it has data for the states over many years, and it includes multiple diseases. This makes it a neat resource to learn a few more Pandas tricks!\n\nBasics\nWe start by importing the data.\n\ndisease = pd.read_csv(\"./_data/health.csv\")\ndisease.head()\n\n\n\n\n\n\n\n\ndisease\nincrease\nloc\nnumber\npopulation\nyear\n\n\n\n\n0\nMEASLES\n334.99\nALABAMA\n8843\n2640000\n1928\n\n\n1\nMEASLES\n200.75\nARIZONA\n847\n422000\n1928\n\n\n2\nMEASLES\n481.77\nARKANSAS\n8899\n1847000\n1928\n\n\n3\nMEASLES\n69.22\nCALIFORNIA\n3698\n5344000\n1928\n\n\n4\nMEASLES\n206.98\nCOLORADO\n2099\n1014000\n1928\n\n\n\n\n\n\n\nThe first thing I want to do is clean it up a little bit. I like columns to use title case and to avoid abbreviations that aren’t in common usage. Additionally, the other datasets we looked at didn’t have entries in all caps, so I would like to fix that as well.\n\ndisease = (\n    # Start with the disease DataFrame\n    disease\n    # Rename the columns to more readable format with capital letters\n    .rename(\n        columns={\n            # Change 'disease' to 'Disease'\n            \"disease\": \"Disease\",\n            # Change 'increase' to 'Increase'\n            \"increase\": \"Increase\",\n            # Change 'loc' to 'State'\n            \"loc\": \"State\",\n            # Change 'number' to 'Cases'\n            \"number\": \"Cases\",\n            # Change 'population' as 'Population'\n            \"population\": \"Population\",\n            # Change 'year' to 'Year'\n            \"year\": \"Year\",\n        }\n    )\n    # Use assign() to create or modify columns without modifying the original\n    # DataFrame\n    .assign(\n        # Convert the Disease column text to title case\n        # (first letter of each word capitalized)\n        Disease=lambda df: df[\"Disease\"].str.title(),\n        # Convert the State column text to title case\n        # (first letter of each word capitalized)\n        State=lambda df: df[\"State\"].str.title(),\n    )\n)\ndisease\n\n\n\n\n\n\n\n\nDisease\nIncrease\nState\nCases\nPopulation\nYear\n\n\n\n\n0\nMeasles\n334.99\nAlabama\n8843\n2640000\n1928\n\n\n1\nMeasles\n200.75\nArizona\n847\n422000\n1928\n\n\n2\nMeasles\n481.77\nArkansas\n8899\n1847000\n1928\n\n\n3\nMeasles\n69.22\nCalifornia\n3698\n5344000\n1928\n\n\n4\nMeasles\n206.98\nColorado\n2099\n1014000\n1928\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n14260\nPertussis\n2.06\nVirginia\n171\n8096604\n2011\n\n\n14261\nPertussis\n10.68\nWashington\n731\n6830038\n2011\n\n\n14262\nPertussis\n1.99\nWest Virginia\n37\n1855364\n2011\n\n\n14263\nPertussis\n6.20\nWisconsin\n351\n5711767\n2011\n\n\n14264\nPertussis\n1.25\nWyoming\n7\n568158\n2011\n\n\n\n\n14265 rows × 6 columns\n\n\n\nWe use the method chaining style again: each method returns a DataFrame that the next method operates on.\nThat lambda usage in assign() might be a bit obscure. Let’s break it down.\n.assign(\n    Disease=lambda df: df[\"Disease\"].str.title()\n)\n\nDisease will be the column that holds the result of the evaluation of the lambda function\nlambda df:\n\nThis creates a small anonymous function that takes a DataFrame named df as input.\nWhich data frame? Well, it’s the data frame that we are currently working on. If that is too mind-bending, just roll with it for now.\n\ndf[\"Disease\"] selects just the Disease column from the data frame df\n.str.title() is a Pandas string method that converts text to “Title Case” (capitalizes the first letter of each word).\n\nSo when you see Disease=lambda df: df[\"Disease\"].str.title() in the assign() method, it means “create a new Disease column by taking the values from the existing Disease column and converting each value to title case.”\nNote: Pandas has a special way of working with string data. To learn more, check out the working with text data docs.\n\n\nDisease Rates\nSimilar to the other datasets, we will want to look at disease rates and not raw numbers since we don’t want the state population to influence our results. This time, let’s modify the existing data frame directly. We could have used assign() again, but I want to give you a little variety, in case you see something similar in the wild.\n\ndisease[\"CasesPer100k\"] = disease[\"Cases\"] / disease[\"Population\"] * 100_000\ndisease.head()\n\n\n\n\n\n\n\n\nDisease\nIncrease\nState\nCases\nPopulation\nYear\nCasesPer100k\n\n\n\n\n0\nMeasles\n334.99\nAlabama\n8843\n2640000\n1928\n334.962121\n\n\n1\nMeasles\n200.75\nArizona\n847\n422000\n1928\n200.710900\n\n\n2\nMeasles\n481.77\nArkansas\n8899\n1847000\n1928\n481.808338\n\n\n3\nMeasles\n69.22\nCalifornia\n3698\n5344000\n1928\n69.199102\n\n\n4\nMeasles\n206.98\nColorado\n2099\n1014000\n1928\n207.001972\n\n\n\n\n\n\n\nSome of these columns are no longer needed, so let’s drop them.\n\ndisease = disease.drop(columns=[\"Increase\", \"Population\"])\ndisease\n\n\n\n\n\n\n\n\nDisease\nState\nCases\nYear\nCasesPer100k\n\n\n\n\n0\nMeasles\nAlabama\n8843\n1928\n334.962121\n\n\n1\nMeasles\nArizona\n847\n1928\n200.710900\n\n\n2\nMeasles\nArkansas\n8899\n1928\n481.808338\n\n\n3\nMeasles\nCalifornia\n3698\n1928\n69.199102\n\n\n4\nMeasles\nColorado\n2099\n1928\n207.001972\n\n\n...\n...\n...\n...\n...\n...\n\n\n14260\nPertussis\nVirginia\n171\n2011\n2.111997\n\n\n14261\nPertussis\nWashington\n731\n2011\n10.702722\n\n\n14262\nPertussis\nWest Virginia\n37\n2011\n1.994218\n\n\n14263\nPertussis\nWisconsin\n351\n2011\n6.145209\n\n\n14264\nPertussis\nWyoming\n7\n2011\n1.232052\n\n\n\n\n14265 rows × 5 columns\n\n\n\n\n\nGrouping Data\nNow that we have cleaned things up a little, let’s summarize the data.\n\ndisease.describe()\n\n\n\n\n\n\n\n\nCases\nYear\nCasesPer100k\n\n\n\n\ncount\n14265.000000\n14265.000000\n14265.000000\n\n\nmean\n1680.833719\n1969.932983\n57.365462\n\n\nstd\n6219.396618\n23.050123\n165.211789\n\n\nmin\n0.000000\n1928.000000\n0.000000\n\n\n25%\n17.000000\n1948.000000\n0.718467\n\n\n50%\n109.000000\n1973.000000\n3.928048\n\n\n75%\n677.000000\n1989.000000\n25.368169\n\n\nmax\n132342.000000\n2011.000000\n2974.789916\n\n\n\n\n\n\n\nHuh, that really didn’t give us useful information did it? The way this data is structured means that multiple columns are needed to identify unique observations. In this case a single observation is uniquely identified by a combination of state-year-disease. When we use describe, or other summarizing methods which we will see in a bit, we will first need to group the observations by subsets of the columns that uniquely identify them.\nFor example, to see averages for diseases per year across all states, we would group by Year and Disease:\n\ndisease.groupby(by=[\"Year\", \"Disease\"]).describe()\n\n\n\n\n\n\n\n\n\nCases\nCasesPer100k\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nYear\nDisease\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1928\nMeasles\n47.0\n10283.765957\n16529.355868\n0.0\n1236.50\n3698.0\n8656.50\n75391.0\n47.0\n360.119785\n355.941940\n0.000000\n130.758467\n256.223176\n508.753856\n1823.718365\n\n\nPolio\n48.0\n99.083333\n155.322154\n6.0\n24.00\n45.0\n89.75\n947.0\n48.0\n4.816878\n4.951962\n0.413365\n1.563007\n2.666954\n6.917508\n20.760799\n\n\nSmallpox\n49.0\n744.285714\n924.561878\n0.0\n83.00\n341.0\n1285.00\n3917.0\n49.0\n41.261417\n49.954931\n0.000000\n2.793296\n18.525449\n73.181818\n204.166667\n\n\n1929\nMeasles\n46.0\n7370.891304\n11334.755452\n211.0\n1102.50\n2164.5\n7100.75\n47605.0\n46.0\n262.191202\n233.787770\n12.133410\n73.116765\n182.629793\n370.208359\n1016.530334\n\n\nPolio\n48.0\n57.208333\n91.662879\n3.0\n9.00\n20.5\n63.25\n530.0\n48.0\n2.040845\n1.718013\n0.215983\n0.839060\n1.532462\n2.824737\n9.938144\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2009\nPertussis\n51.0\n175.294118\n208.747244\n1.0\n54.50\n109.0\n194.00\n1081.0\n51.0\n3.254531\n2.049602\n0.166762\n1.685645\n2.778082\n4.144554\n10.021862\n\n\n2010\nHepatitis A\n50.0\n81.200000\n396.252599\n0.0\n4.75\n12.5\n30.50\n2815.0\n50.0\n0.667417\n1.991235\n0.000000\n0.244203\n0.397353\n0.514911\n14.378582\n\n\nPertussis\n51.0\n276.705882\n373.632830\n6.0\n43.00\n132.0\n326.00\n1810.0\n51.0\n4.811068\n4.255464\n0.095149\n1.927061\n3.394460\n5.986279\n20.872745\n\n\n2011\nHepatitis A\n48.0\n22.958333\n34.008108\n0.0\n4.00\n8.5\n28.00\n176.0\n48.0\n0.289574\n0.169517\n0.000000\n0.168690\n0.278355\n0.404930\n0.798173\n\n\nPertussis\n51.0\n217.647059\n261.697522\n5.0\n46.50\n111.0\n275.00\n1145.0\n51.0\n4.099466\n3.043221\n0.306022\n1.679363\n3.037787\n6.325108\n11.821290\n\n\n\n\n315 rows × 16 columns\n\n\n\nOr, to see averages for diseases per state across all years, we would group by State and Disease:\n\ndisease.groupby(by=[\"State\", \"Disease\"]).describe()\n\n\n\n\n\n\n\n\n\nCases\nYear\nCasesPer100k\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nState\nDisease\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlabama\nHepatitis A\n46.0\n136.913043\n130.868182\n6.0\n35.50\n73.5\n216.50\n467.0\n46.0\n1988.500000\n...\n1999.75\n2011.0\n46.0\n3.648793\n3.729064\n0.132973\n0.860860\n1.717768\n5.708617\n12.877434\n\n\nMeasles\n66.0\n3175.484848\n4196.365912\n0.0\n23.50\n1774.0\n3784.75\n18118.0\n66.0\n1961.166667\n...\n1977.75\n2002.0\n66.0\n107.593367\n143.192331\n0.000000\n0.600964\n53.786844\n129.980033\n590.279330\n\n\nMumps\n34.0\n185.735294\n285.659185\n1.0\n7.25\n19.0\n363.75\n1025.0\n34.0\n1984.529412\n...\n1992.75\n2002.0\n34.0\n5.102450\n7.967464\n0.025053\n0.167004\n0.476587\n9.596426\n28.959711\n\n\nPertussis\n55.0\n464.163636\n679.201295\n1.0\n24.00\n65.0\n996.50\n2249.0\n55.0\n1977.654545\n...\n1997.50\n2011.0\n55.0\n15.573160\n23.568290\n0.025173\n0.587737\n1.552425\n33.401600\n76.444596\n\n\nPolio\n39.0\n155.230769\n186.117176\n0.0\n50.50\n67.0\n188.50\n796.0\n39.0\n1947.076923\n...\n1956.50\n1968.0\n39.0\n5.265726\n6.262218\n0.000000\n1.685470\n2.348485\n6.273355\n27.429359\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWyoming\nMumps\n28.0\n61.750000\n142.390861\n0.0\n2.00\n3.5\n7.25\n436.0\n28.0\n1983.821429\n...\n1992.25\n2001.0\n28.0\n17.632509\n41.088007\n0.000000\n0.402704\n0.726743\n1.753256\n123.148148\n\n\nPertussis\n50.0\n98.660000\n148.652534\n0.0\n4.00\n12.0\n154.50\n559.0\n50.0\n1977.120000\n...\n1998.75\n2011.0\n50.0\n36.635964\n58.913155\n0.000000\n0.846719\n2.319264\n55.790924\n227.235772\n\n\nPolio\n36.0\n41.805556\n60.885757\n1.0\n6.75\n19.0\n42.00\n273.0\n36.0\n1945.527778\n...\n1954.25\n1964.0\n36.0\n15.125636\n20.958616\n0.296736\n2.869318\n6.709717\n15.650277\n93.174061\n\n\nRubella\n21.0\n66.714286\n191.147624\n1.0\n2.00\n6.0\n12.00\n861.0\n21.0\n1978.095238\n...\n1984.00\n1995.0\n21.0\n19.367791\n55.357490\n0.203371\n0.440929\n1.242236\n2.547296\n247.880350\n\n\nSmallpox\n24.0\n74.250000\n105.328162\n1.0\n2.00\n9.5\n119.00\n334.0\n24.0\n1939.625000\n...\n1945.25\n1952.0\n24.0\n31.957904\n45.915825\n0.341297\n0.779701\n3.886957\n50.388562\n149.775785\n\n\n\n\n355 rows × 24 columns\n\n\n\nAnd so on, with any combination required.\nNote: We didn’t show it here, but you can also group by single columns, too.\nYou may have noticed that there is too much data to really get a sense of it in the interactive Quarto prompt. The quickest way around this is to use the to_clipboard() method, and then paste the data into your favorite spreadsheet program and you can get a better look.\n\ndisease.groupby(by=[\"State\", \"Disease\"]).describe().to_clipboard()\n\nThe to_clipboard() function can be very handy!\n\n\nPlotting the Data\nHaving gotten ourselves acquainted with the data, we should make some plots! Let’s see how common the diseases were across all states and all years for which we have data.\n\n(\n    # Select only the columns we need for this chart\n    disease[[\"Disease\", \"Cases\"]]\n    # Group the data by disease\n    .groupby(by=\"Disease\")\n    # Take the sum of the cases\n    .agg({\"Cases\": \"sum\"})\n    # The groupby() operation will make Disease the row names,\n    # but we want them as a column.\n    .reset_index(\"Disease\")\n    # Sort the rows by number of cases\n    .sort_values(\"Cases\", ascending=False)\n    # Plot the data\n    .plot(\n        # Do a bar chart\n        kind=\"bar\",\n        # The x-axis is disease name\n        x=\"Disease\",\n        # The y-axis is the number of cases\n        y=\"Cases\",\n        # We want to log transform the y-axis\n        logy=True,\n        # Drop the legend since we only have a single series of data to plot\n        legend=False,\n        # Give the y-axis a label\n        ylabel=\"Cases\",\n    )\n)\n\n\n\n\n\n\n\n\nMeasles is by far the most common disease in the dataset. Let’s see if the prevalence of different diseases changes over time.\n\nplot_data = (\n    disease[[\"Disease\", \"Year\", \"Cases\"]]\n    .groupby(by=[\"Disease\", \"Year\"])\n    .agg({\"Cases\": \"sum\"})\n    .reset_index([\"Disease\", \"Year\"])\n)\n\nplot_data.plot(\n    kind=\"line\",\n    x=\"Year\",\n    y=\"Cases\",\n)\n\n\n\n\n\n\n\n\nOops! That’s not what we want. Pandas uses matplotlib under the hood, and by default, it works more naturally with a slightly different data format. For reference, here is the data we just tried to plot:\n\nplot_data\n\n\n\n\n\n\n\n\nDisease\nYear\nCases\n\n\n\n\n0\nHepatitis A\n1966\n32416\n\n\n1\nHepatitis A\n1967\n38280\n\n\n2\nHepatitis A\n1968\n45204\n\n\n3\nHepatitis A\n1969\n46012\n\n\n4\nHepatitis A\n1970\n55188\n\n\n...\n...\n...\n...\n\n\n310\nSmallpox\n1948\n446\n\n\n311\nSmallpox\n1949\n67\n\n\n312\nSmallpox\n1950\n42\n\n\n313\nSmallpox\n1951\n19\n\n\n314\nSmallpox\n1952\n16\n\n\n\n\n315 rows × 3 columns\n\n\n\nLet’s adjust the format to one that works better for the plotting function:\n\nplot_data = (\n    # Select the columns we need\n    disease[[\"Disease\", \"Year\", \"Cases\"]]\n    # Group by disease-year pairs\n    .groupby(by=[\"Disease\", \"Year\"])\n    # Get the sum of the cases across the grouping variables\n    .agg({\"Cases\": \"sum\"})\n    # Convert the row names back to columns\n    .reset_index([\"Disease\", \"Year\"])\n    # Pivot! (See below for an explanation)\n    .pivot(\n        # Set the Year column to be the row index of the resulting table\n        index=\"Year\",\n        # Take the new columns from the unique values in the Disease column\n        columns=\"Disease\",\n        # Take the values for the new columns from the data in the Cases column\n        values=\"Cases\",\n    )\n    # Pivoting sets Year as the row names, so convert it back to a column\n    .reset_index(\"Year\")\n)\n\n# Don't worry about this--it's just to make the data print in a nicer way.\nplot_data.columns.name = None\n\nplot_data\n\n\n\n\n\n\n\n\nYear\nHepatitis A\nMeasles\nMumps\nPertussis\nPolio\nRubella\nSmallpox\n\n\n\n\n0\n1928\nNaN\n483337.0\nNaN\nNaN\n4756.0\nNaN\n36470.0\n\n\n1\n1929\nNaN\n339061.0\nNaN\nNaN\n2746.0\nNaN\n38389.0\n\n\n2\n1930\nNaN\n384597.0\nNaN\nNaN\n8964.0\nNaN\n45728.0\n\n\n3\n1931\nNaN\n438435.0\nNaN\nNaN\n15743.0\nNaN\n28708.0\n\n\n4\n1932\nNaN\n390114.0\nNaN\nNaN\n3829.0\nNaN\n10740.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n79\n2007\n2243.0\nNaN\nNaN\n6999.0\nNaN\nNaN\nNaN\n\n\n80\n2008\n2030.0\nNaN\nNaN\n6754.0\nNaN\nNaN\nNaN\n\n\n81\n2009\n1580.0\nNaN\nNaN\n8940.0\nNaN\nNaN\nNaN\n\n\n82\n2010\n4060.0\nNaN\nNaN\n14112.0\nNaN\nNaN\nNaN\n\n\n83\n2011\n1102.0\nNaN\nNaN\n11100.0\nNaN\nNaN\nNaN\n\n\n\n\n84 rows × 8 columns\n\n\n\nAs you can see, we’ve taken the original data and made it wider. (If you’re familiar with R’s Tidyverse, you may recognize this operation as taking tidy data and turning into so-called “messy” data.)\nThe only new thing here is the pivot() function. Pivoting data can be a little tricky to get used to. The basic operation is taking unique values from a column and using each of those unique values to create a new column. (It’s actually more flexible than that, but that’s the general idea.)\nIn this case, we want to make new columns from the unique values in the Disease column (e.g., Measles, Mumps, and Rubella). The values of those new columns will be taken from the Cases column.\nNow we can plot it!\n\ndisease_by_year_plot = plot_data.plot(\n    # We want a line chart\n    kind=\"line\",\n    # Put Year on the x-axis\n    # (The y-axis is set implicitly as the other columns)\n    x=\"Year\",\n    # Log transform the y-axis\n    logy=True,\n    # Give the y-axis a label\n    ylabel=\"Cases\",\n)\n\n\n\n\n\n\n\n\nWhat do you notice about this data? Let me list out some of the questions/observations that might come to mind when first looking at this plot:\n\nWhy do many of the diseases have sharp declines and eventually drop out of the plot after a while?\nWhy is there a gap in the Pertussis (whooping cough) data?\nWhy is Pertussis the only one that has been on a steady increase since the 1970s?\nWhat’s with the big spike in measles cases in the 1980s?\nWhy don’t all the diseases have data going back to the earliest years in the dataset?\nAre there any state-by-state trends, or do they mainly follow the national trends?\nHang on, these are all diseases that we have good vaccination programs for….\n\nNote: The corgis page with this data doesn’t provide much info, but I think it’s fairly safe to assume that when there are no cases reported for a given disease in a given year, that probably means there were no observed cases of that disease in that year.\nOnce you get to that last point, a lot of the other questions are probably pretty easy to explain! Let’s see if the introduction of the vaccine for these diseases corresponds with the decline in cases.\nThe polio vaccine was tested in 1954 and introduced in 1955 by Jonas Salk, becoming a significant breakthrough in the fight against polio. Let’s draw a little vertical dashed line at the year 1955 and see what it looks like.\n\nplot_data.plot(kind=\"line\", x=\"Year\", logy=True, ylabel=\"Cases\")\n# Draw a gray, dashed, vertical line at year 1955\nplt.axvline(x=1955, color=\"#999999\", linestyle=\"--\")\n\n\n\n\n\n\n\n\nYep! As you might have guessed, the polio cases really start a sharp decline after the introduction of the vaccine.\nWhat about smallpox? The history of smallpox vaccination is a bit more circuitous than polio, but two key dates I found were 1939, which saw the start of egg-based smallpox vaccine production by the Texas Department of Health, and 1948, which was when they began to be used in vaccination campaigns (see Wikipedia’s Smallpox vaccine article). So let’s draw the plot again with those two dates highlighted.\n\nplot_data.plot(kind=\"line\", x=\"Year\", logy=True, ylabel=\"Cases\")\nplt.axvline(x=1939, color=\"#999999\", linestyle=\"--\")\nplt.axvline(x=1948, color=\"#999999\", linestyle=\"--\")\n\n\n\n\n\n\n\n\nAgain, that sharp decline we see lines up pretty well with those key dates in the history of the smallpox vaccination program.\nSo, we have now gotten a pretty solid working hypothesis that the drop offs in cases for these diseases are probably due to the introduction of vaccination programs! But what about the interesting variability shown in the measles and pertussis data? I will leave that as an exercise for the reader….\n\n\nPublic Health Data Wrap-Up\nBy examining public health data, we’ve gained experience working with time series data. We’ve also seen how the patterns we observe in this data can help direct our research questions and approaches.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#pandas-tour",
    "href": "exploratory_data_analysis.html#pandas-tour",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Pandas Tour",
    "text": "Pandas Tour\nTo wrap up, let’s summarize everything by giving you a “little” reference that you can use for guidance.\nFor this, we can use the classic coffee data.\n\ncoffee = pd.read_csv(\"./_data/coffee_small.csv\")\ncoffee\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n415 rows × 6 columns\n\n\n\n\nCreating Data Frames\nThere are a ton of ways to create data frames in Pandas. (E.g., check out the docs for DataFrame and the pages linked from that page.) However, to keep it simple, we will stick with read_csv() and creating data frames from Python dictionaries.\nReading from a CSV file:\ndf = read_csv(\"/path/to/data.csv\")\nUsing a dictionary to specify columns:\n\ndf = pd.DataFrame(\n    {\n        \"Gene\": [\"gene_1\", \"gene_2\", \"gene_3\"],\n        \"Sequence\": [\"ACTG\", \"AAGT\", \"GGCT\"],\n        \"Sample\": [\"Sample 1\", \"Sample 2\", \"Sample 2\"],\n    }\n)\ndf\n\n\n\n\n\n\n\n\nGene\nSequence\nSample\n\n\n\n\n0\ngene_1\nACTG\nSample 1\n\n\n1\ngene_2\nAAGT\nSample 2\n\n\n2\ngene_3\nGGCT\nSample 2\n\n\n\n\n\n\n\n\n\nSubsetting and Filtering Rows and Columns\n\nSelecting Columns\nTo get individual columns, use the bracket notation.\n\ncoffee[\"Country\"]\n\n0       Colombia\n1       Colombia\n2       Colombia\n3       Colombia\n4       Colombia\n         ...    \n410    Guatemala\n411    Guatemala\n412    Guatemala\n413       Mexico\n414       Mexico\nName: Country, Length: 415, dtype: object\n\n\nTo select multiple columns, use the bracket notation, but pass in a list of column names rather than a single value.\n\ncoffee[[\"Country\", \"Year\"]]\n\n\n\n\n\n\n\n\nCountry\nYear\n\n\n\n\n0\nColombia\n2012\n\n\n1\nColombia\n2012\n\n\n2\nColombia\n2012\n\n\n3\nColombia\n2012\n\n\n4\nColombia\n2012\n\n\n...\n...\n...\n\n\n410\nGuatemala\n2015\n\n\n411\nGuatemala\n2015\n\n\n412\nGuatemala\n2015\n\n\n413\nMexico\n2015\n\n\n414\nMexico\n2015\n\n\n\n\n415 rows × 2 columns\n\n\n\nIt is often a good idea to save the columns that you want to get in a list, and pass that instead. This can help keep things clear and neat, especially if you have a ton of columns that you’re selecting. Additionally, it can be useful if you build up the list of columns to subset programmatically.\n\ncolumns = [\"Country\", \"Year\"]\ncoffee[columns]\n\n\n\n\n\n\n\n\nCountry\nYear\n\n\n\n\n0\nColombia\n2012\n\n\n1\nColombia\n2012\n\n\n2\nColombia\n2012\n\n\n3\nColombia\n2012\n\n\n4\nColombia\n2012\n\n\n...\n...\n...\n\n\n410\nGuatemala\n2015\n\n\n411\nGuatemala\n2015\n\n\n412\nGuatemala\n2015\n\n\n413\nMexico\n2015\n\n\n414\nMexico\n2015\n\n\n\n\n415 rows × 2 columns\n\n\n\nSometimes, you may only want a single row, but you want the output to be a DataFrame rather than a Series.\n\ncoffee[[\"Country\"]]\n\n\n\n\n\n\n\n\nCountry\n\n\n\n\n0\nColombia\n\n\n1\nColombia\n\n\n2\nColombia\n\n\n3\nColombia\n\n\n4\nColombia\n\n\n...\n...\n\n\n410\nGuatemala\n\n\n411\nGuatemala\n\n\n412\nGuatemala\n\n\n413\nMexico\n\n\n414\nMexico\n\n\n\n\n415 rows × 1 columns\n\n\n\nSometimes you want to select columns based on patterns. For example, to take all the columns that start with Score, we can use filter():\n\ncoffee.filter(regex=r\"^Score\")\n\n\n\n\n\n\n\n\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n0\n7.83\n8.00\n7.75\n7.75\n\n\n1\n7.75\n7.92\n7.83\n7.75\n\n\n2\n7.67\n7.83\n7.83\n8.00\n\n\n3\n7.42\n7.67\n7.75\n7.75\n\n\n4\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n\n\n410\n7.50\n7.25\n7.00\n7.25\n\n\n411\n7.50\n7.33\n7.00\n7.42\n\n\n412\n7.25\n7.17\n6.75\n7.25\n\n\n413\n7.75\n7.67\n7.42\n7.42\n\n\n414\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n415 rows × 4 columns\n\n\n\nThat is using a regular expression (regex) to specify matching any column whose name starts with Score. (If you need an introduction to regular expressions see Appendix I.)\nYou can also use a substring matching version:\n\ncoffee.filter(like=\"Score\")\n\n\n\n\n\n\n\n\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n0\n7.83\n8.00\n7.75\n7.75\n\n\n1\n7.75\n7.92\n7.83\n7.75\n\n\n2\n7.67\n7.83\n7.83\n8.00\n\n\n3\n7.42\n7.67\n7.75\n7.75\n\n\n4\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n\n\n410\n7.50\n7.25\n7.00\n7.25\n\n\n411\n7.50\n7.33\n7.00\n7.42\n\n\n412\n7.25\n7.17\n6.75\n7.25\n\n\n413\n7.75\n7.67\n7.42\n7.42\n\n\n414\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n415 rows × 4 columns\n\n\n\n\n\nFiltering Data\nA common operation is to keep or reject rows of data based on their values in certain columns. For example, to keep study participants aged 65 and over, or to reject any counties with median income greater than $35,000. We can do this using query().\nHere are some examples with the coffee data.\nSelect rows where the country is Mexico:\n\ncoffee.query(\"Country == 'Mexico'\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n75\nMexico\n2012\n7.75\n8.00\n7.75\n7.83\n\n\n76\nMexico\n2012\n7.83\n7.58\n7.67\n7.92\n\n\n77\nMexico\n2012\n8.00\n7.92\n7.42\n8.08\n\n\n78\nMexico\n2012\n7.92\n7.83\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n381\nMexico\n2014\n7.58\n7.75\n7.42\n7.50\n\n\n382\nMexico\n2014\n7.08\n7.08\n6.92\n7.08\n\n\n383\nMexico\n2014\n6.92\n7.00\n6.83\n6.92\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n195 rows × 6 columns\n\n\n\nSelect rows where the country is Colombia and the year is 2014:\n\ncoffee.query(\"Country == 'Colombia' and Year == 2014\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n327\nColombia\n2014\n7.92\n7.75\n7.67\n7.92\n\n\n328\nColombia\n2014\n7.92\n7.75\n7.67\n7.75\n\n\n329\nColombia\n2014\n7.67\n7.75\n7.67\n7.67\n\n\n330\nColombia\n2014\n7.67\n7.67\n7.50\n7.50\n\n\n331\nColombia\n2014\n7.83\n7.75\n7.58\n7.58\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n337\nColombia\n2014\n7.58\n7.50\n7.50\n7.67\n\n\n338\nColombia\n2014\n7.58\n7.75\n7.50\n7.58\n\n\n339\nColombia\n2014\n7.83\n7.67\n7.50\n7.58\n\n\n340\nColombia\n2014\n7.50\n7.58\n7.58\n7.75\n\n\n341\nColombia\n2014\n7.67\n7.42\n7.42\n7.58\n\n\n\n\n15 rows × 6 columns\n\n\n\nSelect rows where the aroma and flavor scores are both at least 8:\n\ncoffee.query(\"`Score.Aroma` &gt;= 8 and `Score.Aroma` &gt;= 8\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n31\nGuatemala\n2012\n8.00\n8.00\n7.83\n8.33\n\n\n32\nGuatemala\n2012\n8.25\n8.00\n7.67\n7.92\n\n\n33\nGuatemala\n2012\n8.00\n7.83\n7.67\n7.83\n\n\n38\nGuatemala\n2012\n8.17\n8.17\n7.83\n7.67\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n344\nGuatemala\n2014\n8.00\n7.92\n7.67\n7.58\n\n\n350\nGuatemala\n2014\n8.00\n7.50\n7.33\n8.00\n\n\n384\nColombia\n2015\n8.17\n7.92\n7.83\n8.00\n\n\n385\nColombia\n2015\n8.17\n7.83\n7.58\n8.08\n\n\n391\nColombia\n2015\n8.00\n7.50\n7.50\n7.67\n\n\n\n\n20 rows × 6 columns\n\n\n\nDo you see how we put backticks around the column names? This is because it had a dot (.) in the name. Whenever you have special characters, like dot or spaces, you will need to use backticks around the variable name.\nSelect rows where the aftertaste score was at least 8 and the acidity was below 7:\n\ncoffee.query(\"`Score.Aftertaste` &gt;= 8 or `Score.Acidity` &lt; 7\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n30\nGuatemala\n2012\n7.83\n8.00\n8.00\n8.17\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n109\nMexico\n2012\n8.08\n8.17\n8.00\n8.00\n\n\n193\nMexico\n2012\n7.33\n7.17\n7.00\n6.75\n\n\n214\nMexico\n2012\n7.25\n7.17\n7.00\n6.83\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n230\nMexico\n2012\n7.00\n6.58\n6.67\n6.83\n\n\n233\nMexico\n2012\n6.92\n6.92\n6.92\n6.92\n\n\n237\nColombia\n2013\n8.00\n8.00\n8.00\n8.17\n\n\n383\nMexico\n2014\n6.92\n7.00\n6.83\n6.92\n\n\n387\nColombia\n2015\n7.75\n7.67\n8.00\n7.67\n\n\n\n\n13 rows × 6 columns\n\n\n\nSelect rows where the year is prior to 2015, or the Flavor and the Aftertaste are above 7.75.\n\ncoffee.query(\"Year &lt; 2015 or (`Score.Flavor` &gt; 7.75 and `Score.Aftertaste` &gt; 7.75)\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n380\nMexico\n2014\n7.25\n7.17\n7.17\n7.25\n\n\n381\nMexico\n2014\n7.58\n7.75\n7.42\n7.50\n\n\n382\nMexico\n2014\n7.08\n7.08\n6.92\n7.08\n\n\n383\nMexico\n2014\n6.92\n7.00\n6.83\n6.92\n\n\n384\nColombia\n2015\n8.17\n7.92\n7.83\n8.00\n\n\n\n\n385 rows × 6 columns\n\n\n\nYou can also use the bracket notation for filtering data:\n\ncoffee[coffee[\"Year\"] &gt;= 2014]\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n327\nColombia\n2014\n7.92\n7.75\n7.67\n7.92\n\n\n328\nColombia\n2014\n7.92\n7.75\n7.67\n7.75\n\n\n329\nColombia\n2014\n7.67\n7.75\n7.67\n7.67\n\n\n330\nColombia\n2014\n7.67\n7.67\n7.50\n7.50\n\n\n331\nColombia\n2014\n7.83\n7.75\n7.58\n7.58\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n88 rows × 6 columns\n\n\n\nYou will find that Pandas has many ways to achieve the same thing. While this flexibility can be helpful especially if you are doing some exploratory analysis for your research, it is often a good idea to stick to one way of doing things, especially within a single script or data pipeline.\n\n\nSelecting Rows and Columns\nSometimes, you may need to select rows and columns at the same time. For this, you can use loc() and iloc(). These tend to feel more natural in situations in which you have a row index. Our data does not, so let’s use this little data frame instead to illustrate what I mean.\n\ndf = pd.DataFrame(\n    {\"A\": [1, 3, 5, 6], \"B\": [2, 1, 4, 7], \"C\": [4, 3, 3, 1], \"D\": [9, 7, 4, 2]},\n    index=[\"Patient 1\", \"Patient 2\", \"Patient 3\", \"Patient 4\"],\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nPatient 1\n1\n2\n4\n9\n\n\nPatient 2\n3\n1\n3\n7\n\n\nPatient 3\n5\n4\n3\n4\n\n\nPatient 4\n6\n7\n1\n2\n\n\n\n\n\n\n\nIn this data frame, row names are “Patient 1” to “Patient 4” and the column names are A-D.\nGetting a single value:\n\ndf.loc[\"Patient 2\", \"C\"]\n\nnp.int64(3)\n\n\nGetting the values for a single row as a series:\n\ndf.loc[\"Patient 4\", :]\n\nA    6\nB    7\nC    1\nD    2\nName: Patient 4, dtype: int64\n\n\nGetting a single column as a series:\n\ndf.loc[:, \"A\"]\n\nPatient 1    1\nPatient 2    3\nPatient 3    5\nPatient 4    6\nName: A, dtype: int64\n\n\nGetting multiple rows for a single column:\n\ndf.loc[[\"Patient 1\", \"Patient 3\"], \"B\"]\n\nPatient 1    2\nPatient 3    4\nName: B, dtype: int64\n\n\nGetting multiple contiguous rows for a single column. (We can use Python’s slice notation for this. Just be aware that the end of the slice is included in the output when you use loc(), unlike the usual Python slicing.)\n\ndf.loc[\"Patient 1\":\"Patient 3\", \"B\"]\n\nPatient 1    2\nPatient 2    1\nPatient 3    4\nName: B, dtype: int64\n\n\nGetting multiple columns for a single row:\n\ndf.loc[\"Patient 2\", [\"B\", \"D\"]]\n\nB    1\nD    7\nName: Patient 2, dtype: int64\n\n\nGetting multiple contiguous columns for a single row using slicing.\n\ndf.loc[\"Patient 2\", \"B\":\"D\"]\n\nB    1\nC    3\nD    7\nName: Patient 2, dtype: int64\n\n\nGetting multiple rows and multiple columns:\n\ndf.loc[[\"Patient 1\", \"Patient 4\"], [\"B\", \"D\"]]\n\n\n\n\n\n\n\n\nB\nD\n\n\n\n\nPatient 1\n2\n9\n\n\nPatient 4\n7\n2\n\n\n\n\n\n\n\nAnd of course, you can mix and match slicing as required to get exactly the data you need:\n\ndf.loc[\"Patient 1\":\"Patient 3\", [\"A\", \"C\"]]\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\nPatient 1\n1\n4\n\n\nPatient 2\n3\n3\n\n\nPatient 3\n5\n3\n\n\n\n\n\n\n\n\n\nSubsetting Rows\nThere are a few nice functions for sampling and downsizing your data frames. This can be helpful if you’re working with huge data, or if you just want to get a better handle on things before working on the full data.\nGet the first few rows of a table:\n\ncoffee.head()\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n\n\n\n\n\n\n\nGet the last few rows of a table:\n\ncoffee.tail()\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n\n\n\nSample 5% of the rows of a table:\n\ncoffee.sample(frac=0.05)\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n105\nMexico\n2012\n7.58\n7.67\n7.50\n7.58\n\n\n406\nGuatemala\n2015\n7.83\n7.67\n7.25\n7.67\n\n\n204\nMexico\n2012\n7.33\n7.00\n6.92\n7.25\n\n\n291\nGuatemala\n2013\n7.83\n7.67\n7.33\n7.33\n\n\n137\nMexico\n2012\n7.67\n7.33\n7.25\n7.50\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n246\nColombia\n2013\n7.67\n7.67\n7.67\n7.67\n\n\n51\nGuatemala\n2012\n7.67\n7.50\n7.33\n7.67\n\n\n289\nGuatemala\n2013\n7.50\n7.50\n7.50\n7.67\n\n\n194\nMexico\n2012\n6.92\n7.17\n7.17\n7.17\n\n\n300\nGuatemala\n2013\n7.50\n7.00\n6.83\n7.08\n\n\n\n\n21 rows × 6 columns\n\n\n\nRandomly select 10 rows from a table:\n\ncoffee.sample(n=10)\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n161\nMexico\n2012\n7.58\n7.42\n7.25\n7.33\n\n\n63\nGuatemala\n2012\n7.58\n7.33\n7.50\n7.33\n\n\n357\nGuatemala\n2014\n7.67\n7.67\n7.75\n7.67\n\n\n316\nMexico\n2013\n7.58\n7.50\n7.25\n7.58\n\n\n288\nGuatemala\n2013\n7.83\n7.67\n7.17\n7.33\n\n\n56\nGuatemala\n2012\n7.50\n7.67\n7.33\n7.33\n\n\n391\nColombia\n2015\n8.00\n7.50\n7.50\n7.67\n\n\n220\nMexico\n2012\n7.33\n7.00\n6.67\n7.00\n\n\n390\nColombia\n2015\n7.42\n7.67\n7.83\n7.75\n\n\n163\nMexico\n2012\n7.42\n7.42\n7.33\n7.33\n\n\n\n\n\n\n\nSelect the top 10 highest entries for a column:\n\ncoffee.nlargest(10, \"Score.Flavor\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n38\nGuatemala\n2012\n8.17\n8.17\n7.83\n7.67\n\n\n39\nGuatemala\n2012\n7.67\n8.17\n7.50\n7.42\n\n\n109\nMexico\n2012\n8.08\n8.17\n8.00\n8.00\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n29\nColombia\n2012\n7.42\n8.00\n7.83\n7.75\n\n\n30\nGuatemala\n2012\n7.83\n8.00\n8.00\n8.17\n\n\n31\nGuatemala\n2012\n8.00\n8.00\n7.83\n8.33\n\n\n32\nGuatemala\n2012\n8.25\n8.00\n7.67\n7.92\n\n\n35\nGuatemala\n2012\n7.67\n8.00\n7.67\n8.00\n\n\n\n\n\n\n\nSelect the 10 smallest entries for a column:\n\ncoffee.nsmallest(10, \"Score.Aroma\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n234\nMexico\n2012\n6.50\n6.67\n6.42\n7.17\n\n\n306\nGuatemala\n2013\n6.50\n6.33\n6.50\n7.50\n\n\n70\nGuatemala\n2012\n6.67\n6.50\n6.33\n7.00\n\n\n71\nGuatemala\n2012\n6.75\n6.50\n6.17\n7.00\n\n\n72\nGuatemala\n2012\n6.75\n6.67\n6.17\n7.17\n\n\n176\nMexico\n2012\n6.92\n7.17\n7.08\n7.50\n\n\n194\nMexico\n2012\n6.92\n7.17\n7.17\n7.17\n\n\n211\nMexico\n2012\n6.92\n6.92\n6.67\n7.25\n\n\n231\nMexico\n2012\n6.92\n6.42\n6.17\n7.33\n\n\n233\nMexico\n2012\n6.92\n6.92\n6.92\n6.92\n\n\n\n\n\n\n\n\n\n\nReshaping Data\nYou will often find yourself needing to change the layout of your data. This can include sorting, reindexing (i.e., adding or changing indices), renaming columns, concatenating, pivoting, and melting data frames.\n\nSorting Rows and Columns\nSorting a data frame by a specific column:\n\ncoffee.sort_values(\"Score.Aftertaste\")\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n231\nMexico\n2012\n6.92\n6.42\n6.17\n7.33\n\n\n66\nGuatemala\n2012\n7.17\n6.08\n6.17\n7.25\n\n\n72\nGuatemala\n2012\n6.75\n6.67\n6.17\n7.17\n\n\n67\nGuatemala\n2012\n7.17\n6.17\n6.17\n7.08\n\n\n71\nGuatemala\n2012\n6.75\n6.50\n6.17\n7.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n237\nColombia\n2013\n8.00\n8.00\n8.00\n8.17\n\n\n387\nColombia\n2015\n7.75\n7.67\n8.00\n7.67\n\n\n30\nGuatemala\n2012\n7.83\n8.00\n8.00\n8.17\n\n\n109\nMexico\n2012\n8.08\n8.17\n8.00\n8.00\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n\n\n415 rows × 6 columns\n\n\n\nThe default sort order is ascending. If you want descending order, you must ask for it:\n\ncoffee.sort_values(\"Score.Aftertaste\", ascending=False)\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n237\nColombia\n2013\n8.00\n8.00\n8.00\n8.17\n\n\n30\nGuatemala\n2012\n7.83\n8.00\n8.00\n8.17\n\n\n387\nColombia\n2015\n7.75\n7.67\n8.00\n7.67\n\n\n109\nMexico\n2012\n8.08\n8.17\n8.00\n8.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n231\nMexico\n2012\n6.92\n6.42\n6.17\n7.33\n\n\n71\nGuatemala\n2012\n6.75\n6.50\n6.17\n7.00\n\n\n66\nGuatemala\n2012\n7.17\n6.08\n6.17\n7.25\n\n\n67\nGuatemala\n2012\n7.17\n6.17\n6.17\n7.08\n\n\n72\nGuatemala\n2012\n6.75\n6.67\n6.17\n7.17\n\n\n\n\n415 rows × 6 columns\n\n\n\nYou can sort values based on multiple columns as well. In addition, you can specify multiple values to the ascending option so that you can have some columns sorted ascending and others sorted descending. Let’s first sort by ascending year, and then by descending flavor (that is within a year, put the best scoring coffees at the top).\n\ncoffee.sort_values([\"Year\", \"Score.Flavor\"], ascending=[True, False])\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n74\nMexico\n2012\n8.17\n8.25\n8.17\n8.00\n\n\n38\nGuatemala\n2012\n8.17\n8.17\n7.83\n7.67\n\n\n39\nGuatemala\n2012\n7.67\n8.17\n7.50\n7.42\n\n\n109\nMexico\n2012\n8.08\n8.17\n8.00\n8.00\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n408\nGuatemala\n2015\n7.50\n7.42\n7.17\n7.83\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n\n\n\n\n415 rows × 6 columns\n\n\n\nFinally, you can sort based on the names of row and column indices. Here is an example that sorts by the names of the columns.\n\ncoffee.sort_index(axis=\"columns\")\n\n\n\n\n\n\n\n\nCountry\nScore.Acidity\nScore.Aftertaste\nScore.Aroma\nScore.Flavor\nYear\n\n\n\n\n0\nColombia\n7.75\n7.75\n7.83\n8.00\n2012\n\n\n1\nColombia\n7.75\n7.83\n7.75\n7.92\n2012\n\n\n2\nColombia\n8.00\n7.83\n7.67\n7.83\n2012\n\n\n3\nColombia\n7.75\n7.75\n7.42\n7.67\n2012\n\n\n4\nColombia\n7.75\n7.75\n7.75\n7.75\n2012\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n7.25\n7.00\n7.50\n7.25\n2015\n\n\n411\nGuatemala\n7.42\n7.00\n7.50\n7.33\n2015\n\n\n412\nGuatemala\n7.25\n6.75\n7.25\n7.17\n2015\n\n\n413\nMexico\n7.42\n7.42\n7.75\n7.67\n2015\n\n\n414\nMexico\n7.50\n7.33\n7.58\n7.50\n2015\n\n\n\n\n415 rows × 6 columns\n\n\n\nIn pandas, every row and column needs a unique identifier called an index. While pandas will automatically assign numeric indexes if none are provided, you can also specify custom row and column names to better organize and access your data.\n\n\nRenaming Columns\nRenaming columns is very useful for dealing with your colleague’s messy data! You will learn this one like the back of your hand.\n\ncoffee.rename(\n    columns={\n        \"Score.Acidity\": \"Acidity\",\n        \"Score.Aftertaste\": \"Aftertaste\",\n        \"Score.Aroma\": \"Aroma\",\n        \"Score.Flavor\": \"Flavor\",\n    }\n)\n\n\n\n\n\n\n\n\nCountry\nYear\nAroma\nFlavor\nAftertaste\nAcidity\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n415 rows × 6 columns\n\n\n\nIn cases like this, where there is a pattern in the column names that you want to get rid of, you can use a Python function.\n\ncoffee.rename(columns=lambda name: name.replace(\"Score.\", \"\"))\n\n\n\n\n\n\n\n\nCountry\nYear\nAroma\nFlavor\nAftertaste\nAcidity\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n415 rows × 6 columns\n\n\n\nNice!\n\n\nDropping Columns\nDropping columns is easy with drop():\n\ncoffee.drop(columns=[\"Country\", \"Year\"])\n\n\n\n\n\n\n\n\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\n\n\n\n\n0\n7.83\n8.00\n7.75\n7.75\n\n\n1\n7.75\n7.92\n7.83\n7.75\n\n\n2\n7.67\n7.83\n7.83\n8.00\n\n\n3\n7.42\n7.67\n7.75\n7.75\n\n\n4\n7.75\n7.75\n7.75\n7.75\n\n\n...\n...\n...\n...\n...\n\n\n410\n7.50\n7.25\n7.00\n7.25\n\n\n411\n7.50\n7.33\n7.00\n7.42\n\n\n412\n7.25\n7.17\n6.75\n7.25\n\n\n413\n7.75\n7.67\n7.42\n7.42\n\n\n414\n7.58\n7.50\n7.33\n7.50\n\n\n\n\n415 rows × 4 columns\n\n\n\n\n\nReindexing\n“Reindexing” sounds a bit weird, but it is all about editing row and column names. A common operation involves turning row names into an explicit column, or turning a column into row names. Let’s go back to our tiny data frame again.\n\ndf = pd.DataFrame(\n    {\"A\": [1, 3, 5, 6], \"B\": [2, 1, 4, 7], \"C\": [4, 3, 3, 1], \"D\": [9, 7, 4, 2]},\n    index=[\"Patient 1\", \"Patient 2\", \"Patient 3\", \"Patient 4\"],\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nPatient 1\n1\n2\n4\n9\n\n\nPatient 2\n3\n1\n3\n7\n\n\nPatient 3\n5\n4\n3\n4\n\n\nPatient 4\n6\n7\n1\n2\n\n\n\n\n\n\n\nCurrently, the row names are the patient IDs. Let’s change those to a column called \"PatientID\":\n\ndf = df.reset_index(names=\"PatientID\")\n\ndf\n\n\n\n\n\n\n\n\nPatientID\nA\nB\nC\nD\n\n\n\n\n0\nPatient 1\n1\n2\n4\n9\n\n\n1\nPatient 2\n3\n1\n3\n7\n\n\n2\nPatient 3\n5\n4\n3\n4\n\n\n3\nPatient 4\n6\n7\n1\n2\n\n\n\n\n\n\n\nNow the row index is the numbers 0-3. If we want to turn the PatientID column back into the row index, we use set_index() like this:\n\ndf = df.set_index(\"PatientID\")\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\nPatientID\n\n\n\n\n\n\n\n\nPatient 1\n1\n2\n4\n9\n\n\nPatient 2\n3\n1\n3\n7\n\n\nPatient 3\n5\n4\n3\n4\n\n\nPatient 4\n6\n7\n1\n2\n\n\n\n\n\n\n\nThere are times when having a row index can be convenient, but generally I have data as columns only as it can make for cleaner data manipulation.\n\n\nConcatenating\nSometimes you have multiple data frames that you want to glue together. This can be done either by glueing rows and keeping columns the same, or by glueing columns and keeping rows the same.\nTo see it in action, let’s make two small data frames. One for group A:\n\ngroup_a = pd.DataFrame({\"Treatment\": [1, 2, 3], \"Control\": [4, 5, 6]})\ngroup_a\n\n\n\n\n\n\n\n\nTreatment\nControl\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n3\n6\n\n\n\n\n\n\n\nAnd one for group B:\n\ngroup_b = pd.DataFrame({\"Treatment\": [10, 20, 30], \"Control\": [40, 50, 60]})\ngroup_b\n\n\n\n\n\n\n\n\nTreatment\nControl\n\n\n\n\n0\n10\n40\n\n\n1\n20\n50\n\n\n2\n30\n60\n\n\n\n\n\n\n\n\nGlue Rows\nGlue the rows (this also works for more than two data frames):\n\npd.concat([group_a, group_b])\n\n\n\n\n\n\n\n\nTreatment\nControl\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n3\n6\n\n\n0\n10\n40\n\n\n1\n20\n50\n\n\n2\n30\n60\n\n\n\n\n\n\n\nThat’s fine, but you probably still want the group info. You can add in the column before concatenating:\n\npd.concat(\n    [\n        group_a.assign(Group=\"A\"),\n        group_b.assign(Group=\"B\"),\n    ]\n)\n\n\n\n\n\n\n\n\nTreatment\nControl\nGroup\n\n\n\n\n0\n1\n4\nA\n\n\n1\n2\n5\nA\n\n\n2\n3\n6\nA\n\n\n0\n10\n40\nB\n\n\n1\n20\n50\nB\n\n\n2\n30\n60\nB\n\n\n\n\n\n\n\nThere we go! Now we haven’t lost the info.\n\n\nGlue Columns\nGlue the columns:\n\nprint(pd.concat([group_a, group_b], axis=\"columns\"))\n\n   Treatment  Control  Treatment  Control\n0          1        4         10       40\n1          2        5         20       50\n2          3        6         30       60\n\n\nNow that is a little silly to glue the columns in this way since it results in duplicate column names. You might want to follow it with a rename operation. Let’s pretend that the two data frames represent two study groups, each having values for treatment and control. Then you might do something like this:\n\npd.concat(\n    [\n        group_a.add_prefix(\"GroupA\"),\n        group_b.add_prefix(\"GroupB\"),\n    ],\n    axis=\"columns\",\n)\n\n\n\n\n\n\n\n\nGroupATreatment\nGroupAControl\nGroupBTreatment\nGroupBControl\n\n\n\n\n0\n1\n4\n10\n40\n\n\n1\n2\n5\n20\n50\n\n\n2\n3\n6\n30\n60\n\n\n\n\n\n\n\nNice!\n\n\n\nPivoting and Melting\nAlright, now this can get a little bit tricky. These operations are sometimes called pivot wider and pivot longer. Let’s just talk about the basics, but know that you can get pretty fancy with this if required.\n(We will use the renamed columns so the output looks a little nicer.)\n\ncoffee_renamed = coffee.rename(columns=lambda name: name.replace(\"Score.\", \"\"))\ncoffee_tidy = coffee_renamed.melt(\n    # These variables identify observations\n    id_vars=[\"Country\", \"Year\"],\n    # The column name for the variables\n    var_name=\"Category\",\n    # The column name for the values of those variables\n    value_name=\"Score\",\n)\n\ncoffee_tidy\n\n\n\n\n\n\n\n\nCountry\nYear\nCategory\nScore\n\n\n\n\n0\nColombia\n2012\nAroma\n7.83\n\n\n1\nColombia\n2012\nAroma\n7.75\n\n\n2\nColombia\n2012\nAroma\n7.67\n\n\n3\nColombia\n2012\nAroma\n7.42\n\n\n4\nColombia\n2012\nAroma\n7.75\n\n\n...\n...\n...\n...\n...\n\n\n1655\nGuatemala\n2015\nAcidity\n7.25\n\n\n1656\nGuatemala\n2015\nAcidity\n7.42\n\n\n1657\nGuatemala\n2015\nAcidity\n7.25\n\n\n1658\nMexico\n2015\nAcidity\n7.42\n\n\n1659\nMexico\n2015\nAcidity\n7.50\n\n\n\n\n1660 rows × 4 columns\n\n\n\nCertain operations require tidy data or are more natural when the data is in this format. For example, making plots using seaborn.\n\nimport seaborn as sns\n\nsns.boxplot(coffee_tidy.sort_values(\"Category\"), x=\"Category\", y=\"Score\", hue=\"Country\")\n\n\n\n\n\n\n\n\n(Converting the coffee_tidy data frame back to the messy form is a bit trickier because Country-Year pairs don’t form unique pairs. So we will use a different example for this.)\n\ndf = pd.DataFrame(\n    {\n        \"Group\": [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"],\n        \"Condition\": [\n            \"Treatment\",\n            \"Treatment\",\n            \"Control\",\n            \"Control\",\n            \"Treatment\",\n            \"Treatment\",\n            \"Control\",\n            \"Control\",\n        ],\n        \"Replicate\": [1, 2, 1, 2, 1, 2, 1, 2],\n        \"Result\": [53.6, 57.2, 66.3, 61.4, 48.6, 49.2, 63.5, 67.8],\n    }\n)\n\ndf\n\n\n\n\n\n\n\n\nGroup\nCondition\nReplicate\nResult\n\n\n\n\n0\nA\nTreatment\n1\n53.6\n\n\n1\nA\nTreatment\n2\n57.2\n\n\n2\nA\nControl\n1\n66.3\n\n\n3\nA\nControl\n2\n61.4\n\n\n4\nB\nTreatment\n1\n48.6\n\n\n5\nB\nTreatment\n2\n49.2\n\n\n6\nB\nControl\n1\n63.5\n\n\n7\nB\nControl\n2\n67.8\n\n\n\n\n\n\n\nNow, we use pivot() to convert it back to the so-called “messy” format:\n\ndf_messy = df.pivot(\n    index=[\"Group\", \"Replicate\"],\n    columns=\"Condition\",\n    values=\"Result\",\n).reset_index()\ndf_messy.columns.name = None\ndf_messy\n\n\n\n\n\n\n\n\nGroup\nReplicate\nControl\nTreatment\n\n\n\n\n0\nA\n1\n66.3\n53.6\n\n\n1\nA\n2\n61.4\n57.2\n\n\n2\nB\n1\n63.5\n48.6\n\n\n3\nB\n2\n67.8\n49.2\n\n\n\n\n\n\n\nTo go back to the tidy format:\n\ndf_tidy = df_messy.melt(\n    id_vars=[\"Group\", \"Replicate\"],\n    var_name=\"Condition\",\n    value_name=\"Result\",\n)\ndf_tidy\n\n\n\n\n\n\n\n\nGroup\nReplicate\nCondition\nResult\n\n\n\n\n0\nA\n1\nControl\n66.3\n\n\n1\nA\n2\nControl\n61.4\n\n\n2\nB\n1\nControl\n63.5\n\n\n3\nB\n2\nControl\n67.8\n\n\n4\nA\n1\nTreatment\n53.6\n\n\n5\nA\n2\nTreatment\n57.2\n\n\n6\nB\n1\nTreatment\n48.6\n\n\n7\nB\n2\nTreatment\n49.2\n\n\n\n\n\n\n\nCheck it out:\n\na = (\n    df.sort_values(by=[\"Group\", \"Condition\", \"Replicate\"])\n    .sort_index(axis=\"columns\")\n    .reset_index(drop=True)\n)\n\ndisplay(a)\n\nb = (\n    df_tidy.sort_values(by=[\"Group\", \"Condition\", \"Replicate\"])\n    .sort_index(axis=\"columns\")\n    .reset_index(drop=True)\n)\n\ndisplay(b)\n\nassert a.equals(b)\n\n\n\n\n\n\n\n\nCondition\nGroup\nReplicate\nResult\n\n\n\n\n0\nControl\nA\n1\n66.3\n\n\n1\nControl\nA\n2\n61.4\n\n\n2\nTreatment\nA\n1\n53.6\n\n\n3\nTreatment\nA\n2\n57.2\n\n\n4\nControl\nB\n1\n63.5\n\n\n5\nControl\nB\n2\n67.8\n\n\n6\nTreatment\nB\n1\n48.6\n\n\n7\nTreatment\nB\n2\n49.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCondition\nGroup\nReplicate\nResult\n\n\n\n\n0\nControl\nA\n1\n66.3\n\n\n1\nControl\nA\n2\n61.4\n\n\n2\nTreatment\nA\n1\n53.6\n\n\n3\nTreatment\nA\n2\n57.2\n\n\n4\nControl\nB\n1\n63.5\n\n\n5\nControl\nB\n2\n67.8\n\n\n6\nTreatment\nB\n1\n48.6\n\n\n7\nTreatment\nB\n2\n49.2\n\n\n\n\n\n\n\nNote: we use display() here rather than print to get nice looking tables when we need to “print” multiple items in a single codeblock.\nThis is a bit of a trickier topic, but I still wanted to give you an idea of how it works for when you run into it in your own research.\n\n\n\nMaking New Columns\nThere are a couple of ways to make new columns in a data frame.\nThe first way is with assign() which creates a new data frame with the requested column added to it.\nLet’s say that you wanted to make a score summary that is a linear combination of a few of the score columns.\n\ncoffee.assign(\n    Score=lambda df: 1.5 * df[\"Score.Aroma\"]\n    + 2 * df[\"Score.Flavor\"]\n    + 0.75 * df[\"Score.Aftertaste\"]\n)\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\nScore\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n33.5575\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n33.3375\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n33.0375\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n32.2825\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n32.9375\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n31.0000\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n31.1600\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n30.2775\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n32.5300\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n31.8675\n\n\n\n\n415 rows × 7 columns\n\n\n\nIf you check the original data frame, you will see that the Score column that we just created is not there:\n\ntry:\n    coffee[\"Score\"]\nexcept KeyError:\n    print(\"no Score column!\")\n\nno Score column!\n\n\nIf you want to actually save that column in your original data frame, you need to use the bracket notation again.\n\ncoffee[\"Score\"] = (\n    1.5 * coffee[\"Score.Aroma\"]\n    + 2 * coffee[\"Score.Flavor\"]\n    + 0.75 * coffee[\"Score.Aftertaste\"]\n)\ncoffee\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\nScore\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\n33.5575\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\n33.3375\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\n33.0375\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\n32.2825\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\n32.9375\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\n31.0000\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\n31.1600\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\n30.2775\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\n32.5300\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\n31.8675\n\n\n\n\n415 rows × 7 columns\n\n\n\nAnd now the Score column is part of the coffee data frame. Let’s drop it out so it doesn’t stick around for the rest of the tutorial.\n\ncoffee = coffee.drop(columns=\"Score\")\n\n\n\nGrouping Data\nGrouping and aggregating data is a very common operation. For example, you might want to run aggregation functions on groups of data like, what’s the mean Flavor score for countries in this data set?\n\ncoffee.groupby(\"Country\").agg({\"Score.Flavor\": \"mean\"})\n# coffee.groupby(\"Country\").agg({\"Score.Aroma\": [\"mean\", \"min\", \"mean\"]})\n\n\n\n\n\n\n\n\nScore.Flavor\n\n\nCountry\n\n\n\n\n\nColombia\n7.602347\n\n\nGuatemala\n7.461803\n\n\nMexico\n7.374513\n\n\n\n\n\n\n\nYou can run the aggregations on multiple columns at once. Here we take the mean flavor and the sum of the aroma. (Yes, that’s a weird thing to do, it’s just here to show you that you don’t have to run the same function for each column.)\n\ncoffee.groupby(\"Country\").agg({\"Score.Flavor\": \"mean\", \"Score.Aroma\": \"sum\"})\n\n\n\n\n\n\n\n\nScore.Flavor\nScore.Aroma\n\n\nCountry\n\n\n\n\n\n\nColombia\n7.602347\n752.53\n\n\nGuatemala\n7.461803\n920.60\n\n\nMexico\n7.374513\n1454.48\n\n\n\n\n\n\n\nYou can even run multiple aggregation functions on each column:\n\ncoffee.groupby(\"Country\").agg(\n    {\"Score.Flavor\": [\"mean\", \"min\", \"max\"], \"Score.Aroma\": \"sum\"}\n)\n\n\n\n\n\n\n\n\nScore.Flavor\nScore.Aroma\n\n\n\nmean\nmin\nmax\nsum\n\n\nCountry\n\n\n\n\n\n\n\n\nColombia\n7.602347\n7.25\n8.00\n752.53\n\n\nGuatemala\n7.461803\n6.08\n8.17\n920.60\n\n\nMexico\n7.374513\n6.42\n8.25\n1454.48\n\n\n\n\n\n\n\nIn many cases you need to group based on multiple columns. In this dataset, it makes sense to group by Country-Year pairs:\n\ncoffee.groupby([\"Country\", \"Year\"]).agg(\n    {\"Score.Flavor\": [\"mean\", \"std\"], \"Score.Aroma\": [\"mean\", \"std\"]}\n)\n\n\n\n\n\n\n\n\n\nScore.Flavor\nScore.Aroma\n\n\n\n\nmean\nstd\nmean\nstd\n\n\nCountry\nYear\n\n\n\n\n\n\n\n\nColombia\n2012\n7.621667\n0.166900\n7.637333\n0.124677\n\n\n2013\n7.546579\n0.184028\n7.646053\n0.170493\n\n\n2014\n7.617333\n0.128478\n7.728667\n0.142672\n\n\n2015\n7.690000\n0.115388\n7.795333\n0.213972\n\n\nGuatemala\n2012\n7.401364\n0.501006\n7.508182\n0.363237\n\n\n...\n...\n...\n...\n...\n\n\n2015\n7.595714\n0.230508\n7.625000\n0.172571\n\n\nMexico\n2012\n7.370741\n0.324318\n7.457284\n0.275460\n\n\n2013\n7.369500\n0.236609\n7.486500\n0.161156\n\n\n2014\n7.400909\n0.234668\n7.394545\n0.239097\n\n\n2015\n7.585000\n0.120208\n7.665000\n0.120208\n\n\n\n\n12 rows × 4 columns\n\n\n\nThat would be the mean and standard deviation of flavor and aroma for all country-year pairs.\n\n\nCombining Data Sets\nAnother useful bit of functionality is merging datasets that have overlapping columns. These is a lot like doing joins in a relational database. While Pandas does have a join() function, we are mainly going to be using merge() instead, as it doesn’t require that the tables have meaningful row indices.\nLet’s say we have a data frame that includes useful information about the countries for which we have coffee data. (Okay, we’re not using necessarily “useful” data in this example, but work with me a bit.)\n\ncountry_capitals = pd.DataFrame(\n    {\n        \"Country\": [\"Colombia\", \"Guatemala\", \"Mexico\"],\n        \"Capital\": [\"Bogotá\", \"Guatemala City\", \"Mexico City\"],\n    }\n)\ncountry_capitals\n\n\n\n\n\n\n\n\nCountry\nCapital\n\n\n\n\n0\nColombia\nBogotá\n\n\n1\nGuatemala\nGuatemala City\n\n\n2\nMexico\nMexico City\n\n\n\n\n\n\n\nOkay, now let’s say we wanted that info in our coffee data frame. We can join them using merge().\n\ncoffee.merge(country_capitals)\n\n\n\n\n\n\n\n\nCountry\nYear\nScore.Aroma\nScore.Flavor\nScore.Aftertaste\nScore.Acidity\nCapital\n\n\n\n\n0\nColombia\n2012\n7.83\n8.00\n7.75\n7.75\nBogotá\n\n\n1\nColombia\n2012\n7.75\n7.92\n7.83\n7.75\nBogotá\n\n\n2\nColombia\n2012\n7.67\n7.83\n7.83\n8.00\nBogotá\n\n\n3\nColombia\n2012\n7.42\n7.67\n7.75\n7.75\nBogotá\n\n\n4\nColombia\n2012\n7.75\n7.75\n7.75\n7.75\nBogotá\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n410\nGuatemala\n2015\n7.50\n7.25\n7.00\n7.25\nGuatemala City\n\n\n411\nGuatemala\n2015\n7.50\n7.33\n7.00\n7.42\nGuatemala City\n\n\n412\nGuatemala\n2015\n7.25\n7.17\n6.75\n7.25\nGuatemala City\n\n\n413\nMexico\n2015\n7.75\n7.67\n7.42\n7.42\nMexico City\n\n\n414\nMexico\n2015\n7.58\n7.50\n7.33\n7.50\nMexico City\n\n\n\n\n415 rows × 7 columns\n\n\n\nThere is actually a lot to the merge() function which we won’t get into too much here, but one important thing is how missing data is handled between data frames. Let’s make some more data to look at this.\nFirst, a data frame for jobs:\n\njobs = pd.DataFrame(\n    {\n        \"Name\": [\"Rahaf\", \"Nanjin\", \"Lujain\", \"Lovisa\"],\n        \"Job\": [\"Chef\", \"Courier\", \"Engineer\", \"Bookmaker\"],\n    }\n)\njobs\n\n\n\n\n\n\n\n\nName\nJob\n\n\n\n\n0\nRahaf\nChef\n\n\n1\nNanjin\nCourier\n\n\n2\nLujain\nEngineer\n\n\n3\nLovisa\nBookmaker\n\n\n\n\n\n\n\nAnd another one for ages:\n\nages = pd.DataFrame(\n    {\n        \"Name\": [\"Rahaf\", \"Nanjin\", \"Lujain\", \"Lovisa\"],\n        \"Age\": [\"47\", \"26\", \"31\", \"61\"],\n    }\n)\nages\n\n\n\n\n\n\n\n\nName\nAge\n\n\n\n\n0\nRahaf\n47\n\n\n1\nNanjin\n26\n\n\n2\nLujain\n31\n\n\n3\nLovisa\n61\n\n\n\n\n\n\n\nTo demonstrate the joins, we will need to take subsets of the data frames in which some of the rows overlap and some of them don’t:\n\njobs_subset = jobs.head(3)\nages_subset = ages.tail(3)\n\ndisplay(jobs_subset)\ndisplay(ages_subset)\n\n\n\n\n\n\n\n\nName\nJob\n\n\n\n\n0\nRahaf\nChef\n\n\n1\nNanjin\nCourier\n\n\n2\nLujain\nEngineer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nAge\n\n\n\n\n1\nNanjin\n26\n\n\n2\nLujain\n31\n\n\n3\nLovisa\n61\n\n\n\n\n\n\n\n\nInner Join\nThe inner join keeps rows that only belong to both sets of data. (This is like the inner join in SQL.)\n\njobs_subset.merge(ages_subset, how=\"inner\")\n\n\n\n\n\n\n\n\nName\nJob\nAge\n\n\n\n\n0\nNanjin\nCourier\n26\n\n\n1\nLujain\nEngineer\n31\n\n\n\n\n\n\n\n\n\nOuter Join\nThe outer join keeps all the rows, even if some of the rows are only present in one data frame or the other. Missing values will be NaN by default. (This is like the full outer join in SQL.)\n\njobs_subset.merge(ages_subset, how=\"outer\")\n\n\n\n\n\n\n\n\nName\nJob\nAge\n\n\n\n\n0\nLovisa\nNaN\n61\n\n\n1\nLujain\nEngineer\n31\n\n\n2\nNanjin\nCourier\n26\n\n\n3\nRahaf\nChef\nNaN\n\n\n\n\n\n\n\n\n\nLeft Join\nThe left join keeps all the rows in the left data frame, even if they are not present in the right data frame. Missing values will be NaN by default. (This is like the left outer join in SQL.)\n\njobs_subset.merge(ages_subset, how=\"left\")\n\n\n\n\n\n\n\n\nName\nJob\nAge\n\n\n\n\n0\nRahaf\nChef\nNaN\n\n\n1\nNanjin\nCourier\n26\n\n\n2\nLujain\nEngineer\n31\n\n\n\n\n\n\n\n\n\nRight Join\nThe right join keeps all the rows in the right data frame, even if they are not present in the left data frame. Missing values will be NaN by default. (This is like the right outer join in SQL.)\n\njobs_subset.merge(ages_subset, how=\"right\")\n\n\n\n\n\n\n\n\nName\nJob\nAge\n\n\n\n\n0\nNanjin\nCourier\n26\n\n\n1\nLujain\nEngineer\n31\n\n\n2\nLovisa\nNaN\n61\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 7.8: Stop & Think\n\n\n\n\n\nWhen merging datasets, what factors should you consider when choosing between inner, outer, left, and right joins?\n\n\n\n\n\n\nPandas Summary\nPandas is a huge and fairly complex library. But you can of get a lot of real work done by getting comfortable with a fairly small subset, and then working your way through more advanced concepts over time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#wrap-up",
    "href": "exploratory_data_analysis.html#wrap-up",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nIn this chapter, we went over the basics of exploratory data analysis (EDA) in Python using the Pandas library. We went through three examples datasets together letting our curiosity guide us. Then, we went through a big tour of the Pandas library for you to use as a reference to some of the most frequently used Pandas functions. Pandas is a big library with a lot of different functions and different ways to do things. This chapter should give you enough Pandas skills to start using it in your own research, and give you the basics you need to get more in-depth with the library later in your career.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#suggested-readings",
    "href": "exploratory_data_analysis.html#suggested-readings",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nPandas is a massive library. These resources can help get a handle on it:\n\nPandas cheat sheet Pandas docs\nPandas User Guide\nThe pandas DataFrame: Make Working With Data Delightful",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis.html#practice-problems",
    "href": "exploratory_data_analysis.html#practice-problems",
    "title": "7  Intro to Exploratory Data Analysis with Python",
    "section": "Practice Problems",
    "text": "Practice Problems\nConsider the following data when answering the following problems.\n\nstate_cancer_data = {\n    \"State\": [\n        \"Delaware\",\n        \"Maryland\",\n        \"Virginia\",\n        \"Pennsylvania\",\n        \"New York\",\n        \"New Jersey\",\n    ],\n    \"Cancer Deaths\": [\n        13_000,\n        72_000,\n        99_000,\n        202_000,\n        249_000,\n        117_000,\n    ],\n    \"Population\": [\n        6_300_000,\n        40_500_000,\n        56_100_000,\n        88_800_000,\n        135_700_000,\n        61_500_000,\n    ],\n    \"Percent Aged 65+\": [\n        19,\n        16,\n        16,\n        19,\n        17,\n        17,\n    ],\n    \"Median Household Income\": [\n        68_000,\n        85_000,\n        74_000,\n        62_000,\n        69_000,\n        83_000,\n    ],\n}\n\n\n7.1\nCreate a Pandas DataFrame to represent the state cancer and demographic data given above.\n\n\n7.2\nCreate a new column called Cancer Deaths Per 100k that represents the number of cancer cases per 100,000 people for each state.\n\n\n7.3\nWhich states have a cancer rate of at least 180 cases per 100k and a median household income less than $68,500?\n\n\n7.4\nWhat is the relationship between median household income and the rate of cancer deaths?\n\n\n7.5\nWhat is the relationship between the percent of the population aged 65 and older and the rate of cancer deaths?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "stats_models.html",
    "href": "stats_models.html",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "",
    "text": "Introduction\nPython has become a cornerstone tool for statistical analysis in life sciences, offering powerful yet accessible ways to understand biological data. Statistical analysis helps scientists move beyond simple observations to make reliable inferences about experimental results and natural phenomena. Python’s data science libraries (like NumPy, Pandas, and SciPy) provide ready-to-use functions that handle complex statistical calculations without requiring advanced mathematical knowledge. Modeling biological processes mathematically allows scientists to test hypotheses, predict outcomes, and discover patterns that might not be visible through direct observation. For life scientists, building statistical literacy is as important as lab techniques–it’s how we determine if results are meaningful or merely coincidental. Python’s visualization capabilities (through libraries like Matplotlib and Seaborn) help make abstract statistical concepts more concrete and interpretable. Understanding basic statistics in Python empowers biologists to design better experiments, analyze results more critically, and communicate findings more effectively.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "stats_models.html#setup",
    "href": "stats_models.html#setup",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "Setup",
    "text": "Setup\nEnsure that you have the following packages installed:\n\nMatplotlib\nNumPy\nPandas\nSciPy\nSeaborn\nstatsmodels\n\nNext, import all the required items for the chapter. We will use the common abbreviations when appropriate.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.formula.api\nimport statsmodels.stats.multicomp\n\nfrom collections import Counter\nfrom scipy import cluster\nfrom scipy import stats\nfrom statsmodels.multivariate.pca import PCA\n\npd.set_option(\"display.max_rows\", 10)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "stats_models.html#statistical-tests",
    "href": "stats_models.html#statistical-tests",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nStatistical tests are mathematical methods that help researchers determine whether patterns in their data represent genuine relationships or simply random chance.\nNote: Statistical tests all have caveats and other considerations that you need to be aware of before running the test and while interpreting the output. I suggest that you carefully read the docs for each of these functions as well as consult any relevant statistical literature when designing and analyzing your experiments.\n\nHypothesis Testing and p-values\nThe goal of a statistical test is to determine whether a specific hypothesis about data is likely true. When conducting such a test, we set up two hypotheses: the null hypothesis (H0) and the alternative hypothesis (HA). The null hypothesis typically states that there’s no effect or relationship in our data (essentially, “nothing interesting is happening”), while the alternative hypothesis suggests that some effect or relationship does exist. If our test results show statistical significance, we reject the null hypothesis in favor of the alternative. However, if we don’t find significance, we fail to reject the null hypothesis (rather than “accepting” it, a subtle but important distinction). To determine significance, we calculate a p-value, which tells us the probability of observing our test results (or something more extreme) if the null hypothesis were actually true. Before conducting the test, we choose a significance level (commonly 0.05) as our threshold. This means we’re comfortable with a 5% risk of rejecting a null hypothesis that is actually true (known as a Type I error or false positive). On the flip side, we might also fail to reject a null hypothesis that’s actually false. This is called a Type II error, and it happens when our test misses a real effect that exists in the data (false negative).\nFor more on hypothesis testing and p-values, see Introduction to Hypothesis Testing and Steps for Hypothesis Tests.\nThis is probably a good time to mention that p-values have been controversial for quite a while now, for examples, see:\n\nMoving to a World Beyond “p &lt; 0.05”\nThe ongoing tyranny of statistical significance testing in biomedical research\nA Dirty Dozen: Twelve P-Value Misconceptions\n\nThere are many more examples to be found in the literature about this topic!\nNote: For this document, we will use a p-value cutoff of 0.05, and talk about significance and rejecting null hypotheses in as basic a way as possible, but you should keep the above criticism in mind.\n\n\n\n\n\n\nTip 8.1: Stop & Think\n\n\n\n\n\nWhat’s one potential problem with relying solely on p-values when interpreting experimental results?\n\n\n\n\n\nBrief Glossary\nWe’re going to touch briefly on some common statistical tests. Before we do, let’s review a few terms:\n\nNormal distribution: Data is normally distributed if it fits a symmetrical bell curve and the mean and median are both at the center of the distribution.\nIndependent sample: Sample groups are independent if observations do not influence each other.\nPaired sample: Samples are paired (or dependent) when there is a meaningful relationship between observations in two or more groups. These can be measurements of the same individual over time or related measurements of two or more individuals. There are good examples of when this might happen here and here.\nAssumption: All statistical tests assume things about your data in their calculations. The big ones are usual the distribution, independence, skewness, and linearity. Performing statistical tests on data that does not meet the correct assumptions can lead to erroneous results, though this may vary with sample size. Statistical tests generally fall into two categories (see the articles Parametric and Nonparametric: Demystifying the Terms and Nonparametric statistical tests: friend or foe? for more thorough explanations and comparisons):\n\nParametric tests: These assume specific data characteristics, generally a normal distribution. These assumptions matter most with smaller sample sizes.\nNonparametric tests: These make fewer assumptions about your data’s shape and other parameters. They often work by analyzing ranks rather than raw values.\n\n\n\n\nComparing Two Groups\nThe statistical comparison of two groups is one of the more common operations that you will need to perform. This covers everything from comparing gene expression levels between healthy and diseased tissue to measuring enzyme activity before and after drug treatment. Comparing two groups is mostly done with the venerable t test, as well as the Mann-Whitney and Wilcoxon tests when the underlying data is not sampled from a normal distribution.\nOne distinction that needs to be made is in the experimental design, that is, whether your groups are paired or not. Paired groups are measurements taken from the same subjects under different conditions (like before and after treatment). Alternatively, unpaired groups are measurements from different subjects (like control group versus experimental group).\nWe will start with unpaired groups, since dealing with them is a bit simpler.\n\nUnpaired Groups\nComparing two unpaired groups is a classic problem, and you have probably needed this operation many times in your own research. Let’s start by looking at the unpaired t test.\n\nUnpaired t Test\nAn unpaired t test, also known as a two-sample t test, compares means between two independent groups. This is the classic t test that you may remember from your statistics courses. Let’s create some example data and visualize it to see how it works.\n\n# Ensures the same random numbers are generated each time.\nnp.random.seed(9382741)\n\n# Create a DataFrame with two columns of random data:\ndf = pd.DataFrame(\n    {\n        # \"Group A\": Generate 40 random values from a normal distribution\n        # with mean (loc) of 5 and standard deviation (scale) of 3\n        \"Group A\": stats.norm(loc=5, scale=3).rvs(size=40),\n        # \"Group B\": Generate 40 random values from a normal distribution\n        # with mean (loc) of 8 and standard deviation (scale) of 3\n        \"Group B\": stats.norm(loc=8, scale=3).rvs(size=40),\n    }\n)\n\n# Create a categorical strip plot (jittered points) using seaborn\n# - 'kind=\"strip\"' specifies we want individual data points with jitter\n# - 'height=3' sets the height of the plot in inches\n# - 'aspect=1.6' sets the width-to-height ratio\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\n\n# Overlay a boxplot on the same axes\n# - 'width=0.25' makes the boxes narrower than default\n# - 'fill=False' creates unfilled boxes (just outlines)\n# - 'color=\"#555555\"' sets the box color to a dark gray\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n\n\n\n\n\n\n\n\nNow that we have seen the data, let’s run the t test. To do it, we use the ttest_ind() function from SciPy.\n\n# Perform an unpaired t test comparing Group A and Group B\n# stats.ttest_ind() performs the independent samples t test\n# df[\"Group A\"] and df[\"Group B\"] are the two samples we're comparing\n# .pvalue extracts just the p-value from the test results\nresult = stats.ttest_ind(df[\"Group A\"], df[\"Group B\"]).pvalue\n\n# Calculate the mean value for each column in the data frame\n# - df.mean() calculates the mean of each column in the original data frame\n# - columns=[\"Mean\"] sets the name of the resulting column to \"Mean\"\nmeans = pd.DataFrame(df.mean(), columns=[\"Mean\"])\n\ndisplay(means)\nprint(f\"Unpaired t test p-value: {result:.1e}\")\n\n\n\n\n\n\n\n\nMean\n\n\n\n\nGroup A\n5.345036\n\n\nGroup B\n7.455855\n\n\n\n\n\n\n\nUnpaired t test p-value: 1.5e-03\n\n\nThe p-value of the t test is 1.5e-03 so we reject the null hypothesis that the group means are the same.\nThe t test comes with several important assumptions that should be met for the results to be valid:\n\nSamples are independent\nSamples have equal variance (homoscedasticity or equal spread of data)\nData follows a normal distribution\nRandom sampling from the population\n\n\n\nMann-Whitney Test\nIf your data doesn’t follow a normal distribution, you may need a nonparametric alternative like the Mann-Whitney Test. Instead of comparing means, this test determines if values from one group tend to be larger than values from another group.\nFirst, we generate and visualize some data. We will generate some data from a uniform distribution (non-normal), and each of the samples will be small (10 observations each). This is a pretty good use-case for the Mann-Whitney test.\n\n\n\n\n\n\nTip 8.2: Stop & Think\n\n\n\n\n\nWhen might you choose a nonparametric test like Mann-Whitney over a parametric test like the t test?\n\n\n\n\nnp.random.seed(9382741)\ndf = pd.DataFrame(\n    {\n        \"Group A\": stats.uniform(loc=5, scale=10).rvs(size=6),\n        \"Group B\": stats.uniform(loc=8, scale=10).rvs(size=6),\n    }\n)\n\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n\n\n\n\n\n\n\n\nJust from looking at the data, it appears that there might be a real difference between the two groups. Let’s run the Mann-Whitney test to check if they are significantly different.\n\n# Perform the Mann-Whitney U test comparing Groups A and B\n# (We extract just the p-value from the test results with the `.pvalue` at the end)\nresult = stats.mannwhitneyu(df[\"Group A\"], df[\"Group B\"]).pvalue\n\n# Calculate the median value for each column in the data frame\nmedians = pd.DataFrame(df.median(), columns=[\"Median\"])\n\ndisplay(medians)\nprint(f\"Mann-Whitney test p-value: {result:.2f}\")\n\n\n\n\n\n\n\n\nMedian\n\n\n\n\nGroup A\n8.414715\n\n\nGroup B\n12.917969\n\n\n\n\n\n\n\nMann-Whitney test p-value: 0.03\n\n\nCool! So we see that the Mann-Whitney test returned a p-value &lt; 0.05 so we reject the null hypothesis that the distribution underlying group A is the same as the distribution underlying group B.\nWhile the Mann-Whitney test doesn’t assume that the samples come from a normal distribution, it does still have assumptions that should be met and that can affect your interpretation of the results. For more info see here.\n\n\n\nPaired Groups\n\nFor comparing the means of paired samples, you will want to use a paired t test.\nThe nonparametric companion to the paired t test is the Wilcoxon test.\n\n\nPaired t test\nWe use the scipy.stats.ttest_rel() function from SciPy to perform a paired t-test. This statistical test examines whether two related samples (like before-and-after measurements from the same subjects) have the same average values. The test evaluates the null hypothesis that the means of the two paired groups are identical.\nSame as before, let’s create some data and make some box plots to check it out. In this dataset, we would expect to see about a 2 unit increase in the post treatment measurements.\n\n# Set the random seed for reproducibility\nnp.random.seed(964832)\n\n# Generate random pre-treatment measurements from a normal distribution\n# with mean=50 and standard deviation=10 for 40 subjects\nbefore_treatment = stats.norm(loc=50, scale=10).rvs(size=40)\n\n# Create a DataFrame with two columns:\n# - 'Pre': the pre-treatment measurements\n# - 'Post': the pre-treatment values plus a small random effect\n#   (representing treatment effect) from a normal distribution\n#   with mean=2 and standard deviation=1\ndf = pd.DataFrame(\n    {\n        \"Pre\": before_treatment,\n        \"Post\": before_treatment + stats.norm(loc=2, scale=1).rvs(size=40),\n    }\n)\n\n# Create a strip plot (jittered points) to visualize the individual data points\n# in both Pre and Post groups\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\n\n# Add box plots over the strip plots to show summary statistics\n# width=0.25 makes the boxes narrow\n# fill=False means the boxes are transparent\n# color=\"#555555\" sets the box color to a dark gray\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n\n\n\n\n\n\n\n\nNext, we run the paired t test.\n\n# Run the t test\nresult = stats.ttest_rel(df[\"Pre\"], df[\"Post\"]).pvalue\n\n# This is to make a nice table with the means.\nmeans = pd.DataFrame(df.mean(), columns=[\"Mean\"])\n\ndisplay(means)\nprint(f\"Unpaired t test p-value: {result:.1e}\")\n\n\n\n\n\n\n\n\nMean\n\n\n\n\nPre\n50.150032\n\n\nPost\n52.008654\n\n\n\n\n\n\n\nUnpaired t test p-value: 3.3e-12\n\n\nIt is statistically significant! But wait, while our p-value confirms this, we need to consider whether 2 units (whatever those units represent in your experiment) actually matters in your biological system. This is where your expertise as a researcher becomes crucial. It’s important not to rely solely on p-values when interpreting results. In our example, we had sufficient sample size to detect a difference, but that difference might still be too small to have any meaningful biological impact.\n\n\n\n\n\n\nTip 8.3: Stop & Think\n\n\n\n\n\nWhy is it important to consider both statistical significance and biological relevance when interpreting results?\n\n\n\nAs you can see, this analysis is quite similar to the unpaired test we discussed earlier. The main difference to keep in mind is how you interpret the results: with paired tests, you’re looking at changes within the same subjects (like before and after treatment), rather than differences between separate groups.\n\nAside: Effect Size and p-values\nScientists have a tendency to focus strongly on p-values, but there is another measure that arguably deserves more attention: effect size. Effect size tells us something different than a p-value: how large or meaningful that effect actually is. It measures the magnitude of differences between groups or the strength of relationships between variables in practical terms. In studies with large sample sizes, it’s very common to find statistically significant results (tiny p-values) alongside tiny effect sizes. This creates a situation where a difference might be technically “real” but practically meaningless. This is precisely why researchers often report both p-values and effect sizes together. P-values answer “Is there an effect?”, while effect sizes answer the equally crucial question: “Does this effect actually matter?” Together, they provide a much more complete and actionable understanding of research findings.\nFor more on effect sizes, see Using Effect Size–or Why the P Value Is Not Enough.\n\n\n\n\n\n\nTip 8.4: Stop & Think\n\n\n\n\n\nCan you think of a scenario in the life sciences where a statistically significant result might have a negligible effect size?\n\n\n\n\n\n\nWilcoxon test\nThe Wilcoxon signed-rank test is used to test the null hypothesis that two related paired samples come from the same distribution. Here is what the SciPy docs have to say about the Wilcoxon signed-rank test:\n\nThe Wilcoxon signed-rank test tests the null hypothesis that two related paired samples come from the same distribution. In particular, it tests whether the distribution of the differences x - y is symmetric about zero. It is a non-parametric version of the paired t test.\n\nNote: this is pretty similar to the Mann-Whitney test…we will leave the distinction to your stats professors :)\nAgain, let’s make some fake data and see how it looks.\n\n# Set a random seed for reproducibility\nnp.random.seed(9382741)\n\n# Create a DataFrame with two columns:\n# 1. \"Pre-Treatment\": 6 random values from a uniform distribution\n#    between 5 and 11 (loc=5, scale=6)\n# 2. \"Post-Treatment\": 6 random values from a uniform distribution\n#    between 6 and 12 (loc=6, scale=6)\ndf = pd.DataFrame(\n    {\n        \"Pre-Treatment\": stats.uniform(loc=5, scale=6).rvs(size=6),\n        \"Post-Treatment\": stats.uniform(loc=6, scale=6).rvs(size=6),\n    }\n)\n\n# Create a strip plot (individual data points) using Seaborn's catplot function\n# - height=3 and aspect=1.6 control the dimensions of the plot\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\n\n# Add a boxplot on top of the strip plot to show distribution statistics\n# - width=0.25: makes the boxes narrower than default\n# - fill=False: creates transparent boxes so we can still see the strip plot points\n# - color=\"#555555\": sets the box and whisker color to a dark gray\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n\n\n\n\n\n\n\n\nLooks like a bit of difference there, but not too strong. Time for a statistical test!\n\n# Run the Wilcoxon signed-rank test\nresult = stats.wilcoxon(df[\"Pre-Treatment\"], df[\"Post-Treatment\"]).pvalue\n\n# This is to make a nice table with the medians.\nmedians = pd.DataFrame(df.median(), columns=[\"Median\"])\n\ndisplay(medians)\nprint(f\"Mann-Whitney test p-value: {result:.2f}\")\n\n\n\n\n\n\n\n\nMedian\n\n\n\n\nPre-Treatment\n7.048829\n\n\nPost-Treatment\n8.950781\n\n\n\n\n\n\n\nMann-Whitney test p-value: 0.16\n\n\nIn this case we fail to reject the null hypothesis that the paired samples com from the same distribution (i.e., that the difference between them is symmetric about zero).\nSimilar to the Mann-Whitney test, though the Wilcoxon signed-rank test doesn’t assume your data comes from a normal population it does s other assumptions to be aware of. Always consult the relevant statistical literature prior to using these tests in your research.\n\n\n\n\nComparing Multiple Groups\nIn the previous sections, we have been focused on comparing two groups. You will often run into cases in which you have more than two groups, or other experimental designs which preclude you from using more straightforward techniques. Let’s see how we can deal with that now.\nTo compare multiple groups, we most commonly use an Analysis of Variance (ANOVA) test. The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean. It’s nonparametric partner is the Kruskal-Wallis test, which uses ranks instead of raw values, and tests the null hypothesis that the population median of all of the groups are equal.\nLet’s revisit the cancer example from last chapter Chapter 7. For the purposes here, we will generate some fake numbers for deaths per 100k people for each of the geographic regions specified by the U.S. Bureau of Labor Statistics.\nImagine we have data points for three locations per state within each region (just assume these are some relevant locations for public health: counties, metro areas, whatever). In this example, we are counting the District of Columbia as a state.\nLet’s write a function to generate the data:\n\n# Dictionary mapping each U.S. region to the number of states it contains\nREGIONS = {\n    \"Northeast\": 7,\n    \"Mid-Atlantic\": 7,\n    \"Southeast\": 8,\n    \"Midwest\": 10,\n    \"Mountain-Plains\": 6,\n    \"Southwest\": 5,\n    \"West\": 8,\n}\n\n# Number of sampling locations we'll collect data from in each state\nSAMPLING_LOCATIONS_PER_STATE = 3\n\n\ndef generate_cancer_data(region, mean, sd):\n    \"\"\"\n    Generate simulated cancer death data for a specific region.\n\n    Parameters:\n        region (str): The name of the region from the REGIONS dictionary\n        mean (float): The mean cancer death count for this region\n        sd (float): The standard deviation of cancer deaths for this region\n\n    Returns:\n        list: A list of dictionaries containing region and death count data\n    \"\"\"\n    # Calculate how many states are in this region\n    state_count = REGIONS[region]\n\n    # Calculate total number of observations needed\n    # (states in region * sampling locations per state)\n    observation_count = state_count * SAMPLING_LOCATIONS_PER_STATE\n\n    # Generate random cancer death counts from a normal distribution\n    # with the specified mean and standard deviation\n    observations = stats.norm(loc=mean, scale=sd).rvs(observation_count)\n\n    # Convert the raw numbers into a list of dictionaries with region labels\n    result = [{\"Region\": region, \"Deaths\": observation} for observation in observations]\n\n    return result\n\n\n# Create a pandas DataFrame by combining cancer data from all regions\n# Each region is assigned different mean death rates to simulate regional variations\n# The standard deviation is kept constant at 20 across all regions\ncancer = pd.DataFrame(\n    generate_cancer_data(\"Northeast\", mean=200, sd=20)\n    + generate_cancer_data(\"Mid-Atlantic\", mean=200, sd=20)\n    + generate_cancer_data(\"Southeast\", mean=225, sd=20)  # Highest mean death rate\n    + generate_cancer_data(\"Midwest\", mean=195, sd=20)\n    + generate_cancer_data(\"Mountain-Plains\", mean=175, sd=20)\n    + generate_cancer_data(\"Southwest\", mean=175, sd=20)\n    + generate_cancer_data(\"West\", mean=150, sd=20)  # Lowest mean death rate\n)\n\n# Display the resulting DataFrame\ncancer\n\n\n\n\n\n\n\n\nRegion\nDeaths\n\n\n\n\n0\nNortheast\n232.569070\n\n\n1\nNortheast\n171.141188\n\n\n2\nNortheast\n196.646856\n\n\n3\nNortheast\n185.546208\n\n\n4\nNortheast\n215.395855\n\n\n...\n...\n...\n\n\n148\nWest\n187.082183\n\n\n149\nWest\n164.184395\n\n\n150\nWest\n123.225502\n\n\n151\nWest\n199.988325\n\n\n152\nWest\n159.141206\n\n\n\n\n153 rows × 2 columns\n\n\n\nCheck out the resulting data table. We generate the data to look like this because it is a pretty good representation of how real data might be structured in a CSV or other document. Now let’s plot it:\n\nsns.catplot(cancer, x=\"Deaths\", y=\"Region\", hue=\"Region\", kind=\"box\")\n\n\n\n\n\n\n\n\nLooks like there may be some significant differences there. Let’s run some statistical tests to see if we can assume that some of the means are different from the others.\n\nOne-Way ANOVA\nWe can use SciPy’s f_oneway() function to run the one-way ANOVA. Here is the blurb from the docs about f_oneway():\n\nThe one-way ANOVA tests the null hypothesis that two or more groups have the same population mean. The test is applied to samples from two or more groups, possibly with differing sizes.\n\nThe f_oneway() function doesn’t really want to work with data in the format we have it, so we have to pull out each group and pass it in separately:\n\n# Create a list of Series objects, where each Series contains the 'Deaths' values\n# for a specific region from the cancer dataset\ndeaths = [cancer.query(f\"Region == '{region}'\")[\"Deaths\"] for region in REGIONS.keys()]\n\n# Perform a one-way ANOVA test using the deaths data from all regions\n# - The * operator unpacks the deaths list so each Series becomes a separate argument\n# - stats.f_oneway compares means of two or more independent samples to determine if they're significantly different\nresult = stats.f_oneway(*deaths)\n\n# Print the results of the ANOVA test\n# - The :.1f formats the F statistic to 1 decimal place\n# - The :.1e formats the p-value in scientific notation with 1 decimal place\nprint(f\"F stat: {result.statistic:.1f}, p-value: {result.pvalue:.1e}\")\n\nF stat: 21.4, p-value: 6.2e-18\n\n\nThat’s a significant value, so we reject the null hypothesis that the groups have the same population mean. (Like all the other tests mentioned, it’s a good idea to make sure you are familiar with any assumptions of the test, to ensure you are using and interpreting the results correctly.)\nJust for fun, let’s write some “fancy” code:\n\n# Reshape the cancer data frame from long to wide format, with regions as columns\ncancer_wide = cancer.pivot(columns=\"Region\")\n\n# Create a list of Series, each containing the cancer deaths for one region\n# The dropna() removes any missing values from each region's data\ndeaths_by_region = [cancer_wide[region].dropna() for region in cancer_wide.columns]\n\n# Perform a one-way ANOVA test to compare means across all regions\n# - The * operator unpacks the list so each region's data is passed as a separate argument\nresult = stats.f_oneway(*deaths_by_region)\n\n# Print the results, formatting F-statistic to 1 decimal place and p-value in scientific notation\nprint(f\"F stat: {result.statistic:.1f}, p-value: {result.pvalue:.1e}\")\n\nF stat: 21.4, p-value: 6.2e-18\n\n\nPretty neat!\n\n\n\n\n\n\nTip 8.5: Stop & Think\n\n\n\n\n\nWhich version of the code do you like better? Which do you think is more readable?\n\n\n\n\n\nKruskal-Wallis\nThe ANOVA’s non-parametric partner is the Kruskal-Wallis test. It uses ranks instead of raw values, and tests the null hypothesis that the population median of all of the groups are equal.\nThe SciPy function for running the Kruskal-Wallis test is called kruskal(). The blurb from the SciPy docs about the test is pretty informative:\n\nThe Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.\n\nThe format is pretty much the same as for the ANOVA.\n\nresult = stats.kruskal(*deaths)\nprint(f\"H stat: {result.statistic:.1f}, p-value: {result.pvalue:.1e}\")\n\nH stat: 72.5, p-value: 1.3e-13\n\n\nThe p-value is less than our critical threshold of 0.05, so we reject the null hypothesis that median of all of the groups are equal. Just like with the ANOVA, it doesn’t tell us which of the groups is different. We will tackle that question in the next section.\nI’m going to start to sound like a broken record here, but, like every other test, make sure you know the assumptions and are using the test correctly if you are using it in your own research!\n\n\nPost-Hoc Testing\nAn ANOVA or Kruskal-Wallis test will tell you if there is a significant difference somewhere among multiple groups, but they don’t say which specific groups differ from each other. For this, we need to perform a follow up or post-hoc test. Tukey’s Honestly Significant Difference (Tukey’s HSD) is one of the most popular post-hoc methods for ANOVA (Dunn’s test, which we won’t cover in this chapter, is the most popular for Kruskal-Wallis). It systematically compares all pairs of groups and adjusts the resulting p-values (more on this in the next section). By using a post-hoc test, we can identify which groups differ significantly from each other, transforming a general “something’s different” conclusion into specific insights about where those differences lie.\nWe can run Tukey’s HSD using the tukey_hsd() from the SciPy package. Here is SciPy’s docs have to say about the function:\n\nThe null hypothesis is that the distributions underlying the samples all have the same mean. The test statistic, which is computed for every possible pairing of samples, is simply the difference between the sample means. For each pair, the p-value is the probability under the null hypothesis (and other assumptions; see notes) of observing such an extreme value of the statistic, considering that many pairwise comparisons are being performed. Confidence intervals for the difference between each pair of means are also available.\n\nNote: See how they mention being aware of the assumptions of the test? Make sure to check those out before using Tukey’s HSD in your own research!\n\nTukey’s HSD with SciPy\nYou call the tukey_hsd() function just like the f_oneway() and kruskal() functions. We will use the same data again:\n\nresult = stats.tukey_hsd(*deaths)\nprint(result)\n\nTukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\nComparison  Statistic  p-value  Lower CI  Upper CI\n (0 - 1)      2.357     1.000   -16.875    21.588\n (0 - 2)    -16.746     0.108   -35.367     1.875\n (0 - 3)      1.341     1.000   -16.390    19.071\n (0 - 4)     28.182     0.001     8.165    48.199\n (0 - 5)     26.825     0.004     5.757    47.892\n (0 - 6)     40.853     0.000    22.232    59.474\n (1 - 0)     -2.357     1.000   -21.588    16.875\n (1 - 2)    -19.103     0.040   -37.724    -0.482\n (1 - 3)     -1.016     1.000   -18.747    16.715\n (1 - 4)     25.825     0.003     5.808    45.842\n (1 - 5)     24.468     0.012     3.401    45.535\n (1 - 6)     38.496     0.000    19.875    57.117\n (2 - 0)     16.746     0.108    -1.875    35.367\n (2 - 1)     19.103     0.040     0.482    37.724\n (2 - 3)     18.087     0.030     1.021    35.153\n (2 - 4)     44.928     0.000    25.497    64.359\n (2 - 5)     43.571     0.000    23.060    64.082\n (2 - 6)     57.599     0.000    39.609    75.588\n (3 - 0)     -1.341     1.000   -19.071    16.390\n (3 - 1)      1.016     1.000   -16.715    18.747\n (3 - 2)    -18.087     0.030   -35.153    -1.021\n (3 - 4)     26.841     0.001     8.261    45.420\n (3 - 5)     25.484     0.003     5.777    45.190\n (3 - 6)     39.512     0.000    22.446    56.578\n (4 - 0)    -28.182     0.001   -48.199    -8.165\n (4 - 1)    -25.825     0.003   -45.842    -5.808\n (4 - 2)    -44.928     0.000   -64.359   -25.497\n (4 - 3)    -26.841     0.001   -45.420    -8.261\n (4 - 5)     -1.357     1.000   -23.143    20.429\n (4 - 6)     12.671     0.451    -6.760    32.102\n (5 - 0)    -26.825     0.004   -47.892    -5.757\n (5 - 1)    -24.468     0.012   -45.535    -3.401\n (5 - 2)    -43.571     0.000   -64.082   -23.060\n (5 - 3)    -25.484     0.003   -45.190    -5.777\n (5 - 4)      1.357     1.000   -20.429    23.143\n (5 - 6)     14.028     0.391    -6.483    34.539\n (6 - 0)    -40.853     0.000   -59.474   -22.232\n (6 - 1)    -38.496     0.000   -57.117   -19.875\n (6 - 2)    -57.599     0.000   -75.588   -39.609\n (6 - 3)    -39.512     0.000   -56.578   -22.446\n (6 - 4)    -12.671     0.451   -32.102     6.760\n (6 - 5)    -14.028     0.391   -34.539     6.483\n\n\n\nThe tukey_hsd() function returns an instance of the TukeyHSDResult class. You may find that the TukeyHSDResult class is not exactly the most user-friendly class. To get the data to put on a chart, you have to do a little bit of work:\n\n# Perform Tukey's HSD (Honest Significant Difference) test on the death data\n# This test compares all possible pairs of regions to find significant differences\nresult = stats.tukey_hsd(*deaths)\n\n# Generate 95% confidence intervals for the differences between means\nconfidence_interval = result.confidence_interval(0.95)\n\n# Get the list of region names from the REGIONS dictionary\nregions = list(REGIONS.keys())\n\n# Create an empty list to store the results for each pair of regions\nrows = []\n\n# Loop through all pairs of regions using numpy's ndenumerate to get indices and p-values\nfor (i, j), p_value in np.ndenumerate(result.pvalue):\n    # Skip comparing a region to itself (i.e., the diagonal elements)\n    if i != j:\n        # Create a dictionary with all the information for this pair of regions\n        row = {\n            \"Region 1\": regions[i],\n            \"Region 2\": regions[j],\n            # Create a label for the pair\n            \"Pair\": regions[i] + \"-\" + regions[j],\n            # The mean difference between regions\n            \"Difference in Means\": result.statistic[i, j],\n            \"p-value\": p_value,\n            \"Significant?\": p_value &lt; 0.05,\n            # Lower bound of confidence interval\n            \"CI Low\": confidence_interval.low[i, j],\n            # Upper bound of confidence interval\n            \"CI High\": confidence_interval.high[i, j],\n        }\n\n        # Add this pair's results to our list\n        rows.append(row)\n\n# Convert the list of dictionaries to a pandas DataFrame\n# Sort by the difference in means and reset the index\ntukey_df = pd.DataFrame(rows).sort_values(\"Difference in Means\").reset_index()\ntukey_df  # Display the DataFrame\n\n# Create a categorical plot using seaborn\n# This will show the difference in means for each pair of regions\nfacet_grid = sns.catplot(\n    tukey_df,\n    y=\"Pair\",\n    x=\"Difference in Means\",\n    hue=\"Significant?\",\n    height=7,\n    aspect=0.8,\n)\n\n# Get the current axis object to add more elements to the plot\n# - 'ax' is a common abbreviation you will see when using matplotlib\nax = facet_grid.facet_axis(0, 0)\n\n# Add confidence interval lines for each pair\nfor idx, row in tukey_df.iterrows():\n    # Plot a horizontal line for each confidence interval\n    ax.plot([row[\"CI Low\"], row[\"CI High\"]], [idx, idx], color=\"#333333\", linewidth=1)\n\n# Add a vertical line at x=0\n# This helps visualize which differences are positive vs. negative\n# If a confidence interval crosses this line, the difference is not significant\nax.axvline(0, linestyle=\"--\", color=\"#BBBBBB\")\n\n\n\n\n\n\n\n\nNote: You may want to check out the docs for the numpy.ndenumerate() function as we haven’t covered it before now.\nWe added in the confidence intervals to the plot. As you can see, the pairs whose confidence interval overlaps with zero are not-significantly different by the test, while those whose confidence interval does not overlap with zero are significantly different by the test.\nThis code is a bit finicky, and we would like to avoid that sort of code whenever possible. Additionally, the plot itself is fairly tricky to read since there are so many pairs. To help with both these issues, let’s pull in the statsmodels package as it has a nice, built-in way to visualize Tukey’s HSD results.\n\n\nTukey’s HSD with statsmodels\nTo run the test using the statsmodels package, we use pairwise_tukeyhsd(), and to generate the plot we use the plot_simultaneous() function. Let’s see how easy it is to run:\n\nresult = statsmodels.stats.multicomp.pairwise_tukeyhsd(cancer[\"Deaths\"], groups=cancer[\"Region\"], alpha=0.05)\n# There is a small issue where if you don't set the result of this to a variable,\n# you get two charts instead of one.\nfigure = result.plot_simultaneous()\n\n\n\n\n\n\n\n\nThat’s pretty cool! Now, instead of looking at all those pairs, we can look at confidence intervals for each region. If they do not overlap with another region, then those two regions have significantly different deaths per 100k people. Not only did we get a plot that more effectively shows the results, we also got to chop out a ton of code!\n\n\n\n\n\n\nTip 8.6: Stop & Think\n\n\n\n\n\nLooking at the Tukey’s HSD plot, how would you explain which regions have significantly different cancer death rates?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "stats_models.html#linear-regression",
    "href": "stats_models.html#linear-regression",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is incredibly common, and I’m absolutely positive that you all have already used it in your research. Let’s see how it goes in Python!\nLinear regression uses one or more explanatory (independent) variables to predict a response (dependent) variable. Simple linear regression refers to a model with only one explanatory variable, while models with two or more explanatory variables are multiple linear regression models. In both cases, we fit a linear model to the data, meaning that we are expecting a straight, non-vertical line. (There’s a really snazzy visual guide to linear regression here.)\nLet’s generate and take a look at some data.\n\n# This ensures that random numbers generated will be the same every time the\n# code runs\nnp.random.seed(254370)\n\n# Define how many data points we want to generate\nobservation_count = 100\n\n# Generate X values from a uniform distribution\n# - loc=-50: The lower bound of the distribution\n# - scale=100: The range of the distribution (from -50 to -50+100=50)\n# - size=observation_count: Generate 100 random values\nx = stats.uniform.rvs(loc=-50, scale=100, size=observation_count)\n\n# Generate Y values based on X following a linear relationship: Y = 2X + noise\n# 2 * x: This creates a linear relationship where Y increases by 2 for every\n#   unit increase in X\n# stats.norm.rvs(...): This adds random noise from a normal distribution\n# - loc=0: The noise is centered around 0 (no bias)\n# - scale=20: The standard deviation of the noise is 20\n#   (determines how scattered the points are)\n# - size=observation_count: Generate 100 random noise values\ny = 2 * x + stats.norm.rvs(loc=0, scale=20, size=observation_count)\n\ndf = pd.DataFrame({\"X\": x, \"Y\": y})\n\n# Create a scatter plot to visualize the relationship between X and Y\ndf.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\n\n\n\n\n\n\n\n\nLooks like a pretty strong trend there. Let’s build a linear model using ordinary least squares fitting. We can use the ols() function from the statsmodels package for this. This package has some neat features like the ability to specify models in a way that looks a lot like how we might do it in R.\n\n# Import necessary libraries (assumed to be done before this code)\n# statsmodels is used for statistical modeling\n# sns (seaborn) is used for data visualization\n\n# Describe the model using the formula interface\n# \"Y ~ X\" means we want to predict Y based on X (simple linear regression)\n# This creates a model object but doesn't fit it to the data yet\nmodel = statsmodels.formula.api.ols(\"Y ~ X\", data=df)\n\n# Fit the model to the data\n# This performs the actual regression calculations and returns a results object\n# containing coefficients, p-values, R-squared, etc.\nresult = model.fit()\n\n# Display a summary of the regression results\n# The summary() method provides a comprehensive report of the model performance\n# 'slim=True' parameter gives a condensed version of the output with just essential\n#   statistics\n# This will show coefficients, standard errors, t-values, p-values, R-squared, etc.\nprint(result.summary(slim=True))\n\n# Create a scatter plot with regression line using seaborn\n# lmplot automatically:\n# - Creates a scatter plot of the raw data (X vs Y)\n# - Fits a regression line\n# - Adds a shaded confidence interval around the line\n# This provides a visual representation of the relationship we just modeled\n# statistically\nsns.lmplot(df, x=\"X\", y=\"Y\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.877\nModel:                            OLS   Adj. R-squared:                  0.875\nNo. Observations:                 100   F-statistic:                     696.5\nCovariance Type:            nonrobust   Prob (F-statistic):           2.51e-46\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.5727      2.091      1.230      0.221      -1.576       6.722\nX              1.9263      0.073     26.391      0.000       1.781       2.071\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nThe summary table gives you the main regression result info, including fitted coefficients, standard errors, p-values, R-squared values, etc. We won’t get into the details of how regression is calculated or how exactly to interpret it in this chapter, so just focus on the coefficients and the p-values for now.\nIn this case, we have a coefficient for X of 1.9, which is pretty close to the true model, and it is significant! Additionally, the R-squared value is 0.877, which means this model captures a lot of the variability in our data. In other words, it’s a very good model!\n\nWith No Relationship in the Data\nLet’s try another dataset, but this time the response is not related to the predictors.\n\n# Set the seed\nnp.random.seed(962378)\n\n# Generate the data\nobservation_count = 100\nx = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\n# The response values are basically random noise\ny = stats.norm(loc=0, scale=10).rvs(size=observation_count)\ndf = pd.DataFrame({\"X\": x, \"Y\": y})\n\n# Create the model\nmodel = statsmodels.formula.api.ols(\"Y ~ X\", data=df)\n\n# Fit the model\nresult = model.fit()\n\n# Print the summary and plot the data\nprint(result.summary(slim=True))\nsns.lmplot(df, x=\"X\", y=\"Y\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.001\nModel:                            OLS   Adj. R-squared:                 -0.009\nNo. Observations:                 100   F-statistic:                   0.08288\nCovariance Type:            nonrobust   Prob (F-statistic):              0.774\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.3901      1.057      0.369      0.713      -1.707       2.487\nX             -0.0115      0.040     -0.288      0.774      -0.091       0.068\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nThis time, we have pretty much the opposite result. The fitted coefficient is around zero, the line on the plot is basically horizontal, and the R-squared value is very low. So it is very likely that there is no relationship that we can measure between these two variables. And of course, we expected this result, as that is how we set up the data!\n\n\n\n\n\n\nTip 8.7: Stop & Think\n\n\n\n\n\nWhat does an R-squared value close to zero tell you about a linear regression model?\n\n\n\n\n\nMultiple Predictors\nMultiple predictors can be given as well. Let’s make another data set with three predictors, \\(x_1\\), \\(x_2\\), and \\(x_3\\). The response \\(y\\) will only depend on the first two predictors, however.\n\nnp.random.seed(526347)\n\nobservation_count = 100\n\n# Generate 3 independent variables from uniform distributions\n# Each variable ranges from -50 to 50 (loc=-50, scale=100)\nx1 = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\nx2 = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\nx3 = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\n\n# Generate random noise from a normal distribution with mean 0 and standard\n# deviation 10\nnoise = stats.norm(loc=0, scale=10).rvs(size=observation_count)\n\n# Create the dependent variable y as a linear combination of x1 and x2,\n#   plus noise\n# True model: y = 2*x1 - 3*x2 + noise\n# - Note that x3 is not used in generating y!\ny = 2 * x1 - 3 * x2 + noise\n\n# Create a DataFrame with all variables\n# Sort it by the y values and reset the index\ndf = (\n    pd.DataFrame({\"X1\": x1, \"X2\": x2, \"X3\": x3, \"Y\": y})\n    .sort_values(\"Y\")\n    .reset_index(drop=True)\n)\n\n# Fit a linear regression model using all three predictors\n# - Formula notation: Y ~ X1 + X2 + X3\nmodel = statsmodels.formula.api.ols(\"Y ~ X1 + X2 + X3\", data=df)\nresult = model.fit()\n\n# Display a summary of the regression results\n# - slim=True omits some of the more detailed statistics\nprint(result.summary(slim=True))\n\n# Create a pairplot showing relationships between Y and each predictor\n# - kind=\"reg\" adds regression lines to the scatter plots\nsns.pairplot(df, kind=\"reg\", y_vars=\"Y\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.990\nModel:                            OLS   Adj. R-squared:                  0.989\nNo. Observations:                 100   F-statistic:                     3044.\nCovariance Type:            nonrobust   Prob (F-statistic):           5.20e-95\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.1886      0.986      1.205      0.231      -0.769       3.146\nX1             2.0538      0.033     62.303      0.000       1.988       2.119\nX2            -2.9983      0.037    -81.967      0.000      -3.071      -2.926\nX3             0.0445      0.034      1.318      0.191      -0.023       0.112\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nThe coefficients of the variables, p-values, and scatter plots are all about what we would expect given the way we generated the data.\nOne cool thing to call out is the pairplot() function from seaborn. It’s a nice way to visualize multiple variables at the same time. Here, we can see a positive association between X1 and y, a negative association between X2 and y, and no association between X3 and y. Cool!\nLinear regression assumes some things about your data (are you tired of me saying this yet…). For example, the JMP docs put it this way:\n\n\nThe true relationship is linear\nErrors are normally distributed\nHomoscedasticity of errors (or, equal variance around the line).\nIndependence of the observations\n\n\nIt’s a good idea to check these assumptions before interpreting your results. There are many common diagnostic plots for doing so, like the normal Q-Q plot, and the scale-location plot, but that is a story for another time!\nNote: There is a really cool site that talks about how most common statistical models are basically linear models (or at least, very close to them). It’s a pretty neat way to think of statistical modeling that I encourage you to check out!\n\n\n\n\n\n\nTip 8.8: Stop & Think\n\n\n\n\n\nIn our multiple regression example, how could you determine which predictors are most important for the model?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "stats_models.html#dimensionality-reduction-clustering",
    "href": "stats_models.html#dimensionality-reduction-clustering",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "Dimensionality Reduction & Clustering",
    "text": "Dimensionality Reduction & Clustering\nLet’s switch gears a little bit and introduce dimensionality reduction and clustering. Dimensionality reduction techniques help transform complex high-dimensional data (like genomic or proteomic datasets with thousands of features) into simpler representations that capture the essential patterns. Clustering methods then help identify natural groupings within this data, allowing you to discover hidden structures without prior labeling. These techniques are common tools in bioinformatics that allow researchers to visualize complex biological relationships, identify subtypes of diseases, or group similar protein structures together when manual classification would be impractical.\n\nPrincipal Components Analysis (PCA)\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique that transforms your original variables into a new set of uncorrelated variables called principal components. These principal components are ordered by how much of the original data’s variance they capture, with the first component accounting for the most variance, the second component capturing the second most, and so on. PCA is more or less finding the directions (or axes) along which your data varies the most, allowing you to reduce dimensionality while preserving as much information as possible.\nNote: there is another way to think of PCA, and that is as a form of indirect gradient analysis or as biplots of compositional data, which are two of my favorite topics. Unfortunately, or maybe fortunately depending on your interests, we won’t be covering that here!\nFirst, let’s read in the data. You might recognize this dataset, its Fisher’s famous iris dataset!\n\niris = pd.read_csv(\"./_data/iris.csv\")\ndisplay(iris)\n\nsns.pairplot(iris, hue=\"Species\")\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\n\n\n\n\n\nWe can use the PCA class from the statsmodels package to compute the PCA:\n\nresult = PCA(\n    # The data to perform PCA on - our iris dataset without the 'Species' column\n    # We're only using the numeric feature columns\n    iris.drop(columns=\"Species\"),\n    # demean=True means that the mean will be subtracted from each feature\n    # This centers the data around zero\n    # R's `prcomp()` calls this parameter \"center\"\n    demean=True,\n    # standardize=False means that we are NOT scaling each feature to have unit variance\n    # If True, each feature would be divided by its standard deviation\n    # R's `prcomp()` would use both center = TRUE and scale. = TRUE for standardization\n    standardize=False,\n    # We won't get into the math behind this parameter.  Just know that setting it to\n    # False is the more typical use case for our purposes.\n    normalize=False,\n    # ncomp=2 specifies that we want to keep only the first 2 principal components\n    # These will be the 2 directions that capture the most variance in the data\n    ncomp=2,\n)\nresult\n\nPrincipal Component Analysis(nobs: 150, nvar: 4, transformation: Demean (Covariance), normalization: False, number of components: 2, SVD, id: 0x12f82f0e0)\n\n\nOnce PCA is performed, we can visualize the data by plotting the projections onto the first two principal components:\n\n# Prepare the data for plotting\nplot_data = (\n    result.scores.rename(\n        columns={\n            # Rename the first principal component for better readability\n            \"comp_0\": \"PC 1\",\n            # Rename the second principal component for better readability\n            \"comp_1\": \"PC 2\",\n        }\n    )\n    # Add the Species column from the original dataset for coloring points\n    .assign(Species=iris[\"Species\"])\n)\n\n# Create a scatter plot using seaborn\n# - relplot creates a relational plot (scatter plot in this case)\n# - x and y specify which columns to use for the x and y axes\n#   (the first two principal components)\n# - hue colors the points based on the Species column\nsns.relplot(plot_data, x=\"PC 1\", y=\"PC 2\", hue=\"Species\")\n\n\n\n\n\n\n\n\nAs you can see, the three species are fairly well separated along the x-axis, which represents the first principal component.\n\n\n\n\n\n\nTip 8.9: Stop & Think\n\n\n\n\n\nWhat biological interpretation might you give to the first principal component in this iris dataset analysis?\n\n\n\nThere is a lot more to say about PCA, including differences between using the covariance vs. correlation matrix, biplots, interpretation, etc., but we will leave it there for now.\n\n\nk-means Clustering\nK-means clustering is a common unsupervised machine learning technique that partitions your data into a specified number, k, of distinct, non-overlapping groups, without requiring labeled training data. The algorithm works by iteratively assigning data points to the nearest cluster center, then recalculating those centers until convergence is reached. Some use cases for k-means clustering include grouping genes given expression data, identifying protein function similarities, or finding patient subgroups with similar clinical profiles.\nWe can use SciPy’s kmeans2() function to run the k-means algorithm on our iris data:\n\n# Set the random seed for reproducibility\nnp.random.seed(238974)\n\n# Perform K-means clustering on the iris dataset\n# - We're using the numeric columns of the iris data frame\n# - We specify k=3 to create 3 clusters (matching the number of iris species)\n# - We ignore the _centroids\n# - `labels` will contain the cluster assignment (0, 1, or 2) for each iris sample\n_centroids, labels = cluster.vq.kmeans2(iris.drop(columns=\"Species\"), k=3)\n\n# Create a scatter plot to visualize the clustering results\n# - We're plotting the first two principal components (PC 1 vs PC 2)\n# - Each point is colored by its actual species (using 'hue')\n# - Each point's marker style is determined by its cluster assignment (using 'style')\n# - This allows us to compare the actual species classifications with the clustering\n#   results\n# - 'plot_data.assign(Cluster=labels)' adds the cluster labels as a new column to the\n#   data frame\nsns.relplot(\n    plot_data.assign(Cluster=labels),\n    x=\"PC 1\",\n    y=\"PC 2\",\n    hue=\"Species\",\n    style=\"Cluster\",\n    legend=\"full\",\n)\n\n\n\n\n\n\n\n\nWe can see that there is pretty good overlap between the clusters assigned by the k-means algorithm with the true species groupings. In most cases, you won’t get clusters that perfectly represent your data. For this reason, there are a ton of techniques to help you evaluate and optimize clustering and other classification methods that we won’t get into here. Instead, we will keep it super simple and just look at the proportion of cluster labels for each species. This will give us a rough idea about the quality of the clustering results.\n\ndef proportions(clusters):\n    # Count occurrences of each cluster value\n    counts = Counter(clusters)\n    # Get total number of items in clusters\n    total = len(clusters)\n    # Calculate proportion for each cluster and sort by cluster id\n    proportions = {cluster: count / total for cluster, count in sorted(counts.items())}\n    return proportions\n\n\n# For each species in the iris dataset:\n# - Assign the cluster labels to a new column 'Cluster'\n# - Group by 'Species'\n# - Aggregate the 'Cluster' column using the proportions function\n#\n# This shows the distribution of cluster assignments within each species\niris.assign(Cluster=labels).groupby(\"Species\").agg({\"Cluster\": proportions})\n\n\n\n\n\n\n\n\nCluster\n\n\nSpecies\n\n\n\n\n\nsetosa\n{1: 1.0}\n\n\nversicolor\n{0: 0.06, 2: 0.94}\n\n\nvirginica\n{0: 0.74, 2: 0.26}\n\n\n\n\n\n\n\nThat’s pretty good! The k-means clustering pretty well describes the setosa and versicolor species, but the virginica species gets mislabelled about 25% of the time.\n\n\n\n\n\n\nTip 8.10: Stop & Think\n\n\n\n\n\nWhat might explain why k-means clustering performed well for setosa and versicolor but less well for virginica?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "stats_models.html#summary",
    "href": "stats_models.html#summary",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "Summary",
    "text": "Summary\nThis chapter covered key statistical and modeling techniques in Python used in life sciences. We looked at methods for comparing groups, including t-tests, ANOVA, and non-parametric alternatives, with a focus on understanding p-values and effect sizes. We also introduced linear regression for exploring relationships between variables, as well as dimensionality reduction and clustering for identifying patterns in data. Throughout, we highlighted the importance of understanding the assumptions behind these methods and interpreting results carefully. Effective data analysis goes beyond running code–it involves choosing the right methods, making sense of the results, and linking them to biological questions. Python offers a range of tools that, when used thoughtfully, support meaningful and clear analysis of experimental data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "stats_models.html#practice-problems",
    "href": "stats_models.html#practice-problems",
    "title": "8  Intro to Statistics & Modeling with Python",
    "section": "Practice Problems",
    "text": "Practice Problems\n\n8.1\nWrite code to generate two groups of 30 samples each, where group A has a mean of 10 and standard deviation of 2, and group B has a mean of 15 and standard deviation of 2. Then perform an unpaired t-test to compare them.\n\n\n8.2\nAdd a third group (mean 11, standard deviation of 2) to the dataset from the last problem. Then perform an ANOVA to determine if there are statistically significant differences between them.\n\n\n8.3\nRun Tukey’s HSD on the data from Section 8.7.2.\n\n\n8.4\nGenerate a dataset with one response variable and two predictor variables, where only one predictor has a relationship with the response. Fit a multiple regression model and determine which predictor is significant.\n\n\n8.5\nPerform k-means clustering with k=2 on the iris dataset. Does this grouping make biological sense? Justify your answer with a visualization.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistics & Modeling</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html",
    "href": "io_files_contexts.html",
    "title": "9  I/O, Files, & Contexts",
    "section": "",
    "text": "Install & Import Needed Libraries\nInput and output (I/O) operations are how your programs interact with the outside world. Whether you are taking command line arguments or reading files, you will need to get data into and out of your programs. In this chapter, we will cover the basic concepts (I/O) operations, file handling, and context managers with a focus on bioinformatics applications.\nFor this module, you well need to ensure that you have biopython, pandas, and seaborn installed.\nThen, you can run the imports:\nimport csv\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom Bio import SeqIO\nfrom Bio import SeqUtils",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#file-handling-basics",
    "href": "io_files_contexts.html#file-handling-basics",
    "title": "9  I/O, Files, & Contexts",
    "section": "File Handling Basics",
    "text": "File Handling Basics\nLet’s start with some file handling basics: reading, writing, and appending to files.\n\nReading from Files\nReading files is a common method for importing data into programs, allowing access to pre-existing information such as sequencing reads, experimental data, configuration settings, or user information. Python provides various techniques to control data reading, whether all at once, line by line, or in chunks. Reading a file is non-destructive to the source, as it only creates a memory copy without affecting the original file.\nFor the next couple of code blocks, we will be using this file:\n\nexample_text_file = \"./_data/sample.txt\"\n\n\nLine-by-Line\nFirst, let’s see how to read a file line-by-line. The with statement creates a context manager that automatically closes the file when the block ends. We will talk more about context managers later in the tutorial, but for now, know that this is generally the recommended way to handle files in Python as it ensures proper resource cleanup.\n\n# Open a file `example_text_file` in read mode\nwith open(example_text_file) as file:\n    # Iterate through each line in the file\n    # - enumerate() returns both the index (`i`) and the value (`line`)\n    #   for each iteration\n    # - `i` will start at 0 for the first line, 1 for the second line, etc.\n    for i, line in enumerate(file):\n        # strip() removes whitespace characters (like newlines) from both\n        # ends of the string\n        print(i, line.strip(), sep=\" =&gt; \")\n\n0 =&gt; Hello, world!\n1 =&gt; This text will be added at the end\n\n\n\n\nAll at Once\nRather than read the data line-by-line, we can read all the data of a file with one function call using the read() method.\nRead an entire file at once:\n\n# Open a file `example_text_file` in read mode\nwith open(example_text_file) as file:\n    # Read the entire contents of the file and store it in the variable\n    # `content`\n    content = file.read()\n    print(content)\n\nHello, world!\nThis text will be added at the end\n\n\n\n\nReading Chunks\nThe read() method can also be used to read chunks of a given size:\n\nwith open(example_text_file, \"r\") as file:\n    # Reads first 5 characters\n    hello = file.read(5)\n    print(hello)\n\n    # Then read the next 2 characters\n    comma_space = file.read(2)\n    # Use the f-string so you can see the space character\n    print(f\"'{comma_space}'\")\n\n    # And finally the next 6\n    world = file.read(6)\n    print(world)\n\nHello\n', '\nworld!\n\n\n\n\n\nWriting to Files\nWriting operations generate new files or completely replace existing ones. This enables programs to save results, create logs, or generate reports.\nWhen writing to an existing file, previous content is erased unless append mode is used. Because of this, you have to be careful with writing operations to prevent unintended data loss. It’s easy to accidentally delete files that you didn’t intend to, so it’s important to be careful!\nFor the next few examples, we will be writing to this file:\n\noutput_file_name = \"./_tmp/output.txt\"\n\nLet’s also write a little helper function to print out the contents of a file. This way, we can see the effect of each of the next few code blocks without cluttering them up:\n\ndef print_file_contents(file_name):\n    \"\"\"Print the entire contents of the given file to the console.\"\"\"\n    with open(file_name) as file:\n        contents = file.read()\n        print(contents)\n\n\nWriting to a File\nTo write to a file, we need to open the file in write mode. This is done by passing \"w\" to the open function. Then we need to call the write() method on the file object and pass it in some data. Note that this will overwrite the existing file, so be careful!\n\nwith open(output_file_name, \"w\") as file:\n    file.write(\"Hello, this is my first file!\")\n\nprint_file_contents(output_file_name)\n\nHello, this is my first file!\n\n\nYou can call write() multiple times on the same file object to write multiple times to the same file:\n\nwith open(output_file_name, \"w\") as file:\n    file.write(\"Line 1: Introduction\\n\")\n    file.write(\"Line 2: Main content\\n\")\n    file.write(\"Line 3: Conclusion\")\n\nprint_file_contents(output_file_name)\n\nLine 1: Introduction\nLine 2: Main content\nLine 3: Conclusion\n\n\n\n\n\n\n\n\nTip 9.1: Stop & Think\n\n\n\n\n\nIn the last two examples, we wrote to the same file both times. After the second example, the file no longer included the text Hello, this is my first file!. Why is that?\n\n\n\n\n\nWriting Lines with a Loop\nIt is pretty common to have some data in a collection, like a list or dictionary, that you want to write to a file. One way to do this is with a for loop:\n\n# Initialize a list of strings\nlines = [\"First line\", \"Second line\", \"Third line\"]\n\n# Create a dictionary mapping protein names to their lengths\nprotein_length = {\"Protein_1\": 500, \"Protein_2\": 750}\n\n# Open a file for writing.\n# - The `\"w\"` specifies that we open the file in \"write\" mode.\n# - The `with` statement ensures file is properly closed when we're done.\nwith open(output_file_name, \"w\") as file:\n    # Iterate through each line in our list\n    for line in lines:\n        # Don't forget to add a newline.\n        # file.write will not add one for you!\n        file.write(line + \"\\n\")\n\n    # Iterate through each key-value pair in the dictionary\n    for protein, length in protein_length.items():\n        # Format a string with protein name and length, including a newline\n        line = f\"{protein} =&gt; {length}\\n\"\n        # Write the formatted string to the file\n        file.write(line)\n\n# Display the contents of the file we just created\nprint_file_contents(output_file_name)\n\nFirst line\nSecond line\nThird line\nProtein_1 =&gt; 500\nProtein_2 =&gt; 750\n\n\n\n\n\n\nAppending to Files\nAppending to a file is similar to writing, except that it preserves existing content while adding new information to the end of a file, rather than overwriting the existing data present in the file. This can be useful for logging, data collection over time, or building cumulative reports. Anything were you need to persist some data, and then go back and add more stuff over time. In a way, append operations can be safer than regular write operations because they won’t overwrite the file to which you’re appending.\nHere’s how to do it. It’s very similar to the writing examples, except that you pass \"a\" to the open() function rather than \"w\". This gives you a file object in “append mode” rather than one in “write mode”.\n\nwith open(output_file_name, \"a\") as file:\n    file.write(\"New line added\\n\")\n\nprint_file_contents(output_file_name)\n\nFirst line\nSecond line\nThird line\nProtein_1 =&gt; 500\nProtein_2 =&gt; 750\nNew line added\n\n\n\nSee how the previous lines we wrote are still in the file output? That’s because we’re in append mode!\nJust like with write mode, you can also append multiple times by looping through some lines:\n\nnew_data = [\"Entry 4\", \"Entry 5\", \"Entry 6\"]\n\nwith open(output_file_name, \"a\") as file:\n    for item in new_data:\n        file.write(item + \"\\n\")\n\nprint_file_contents(output_file_name)\n\nFirst line\nSecond line\nThird line\nProtein_1 =&gt; 500\nProtein_2 =&gt; 750\nNew line added\nEntry 4\nEntry 5\nEntry 6",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#file-operation-details",
    "href": "io_files_contexts.html#file-operation-details",
    "title": "9  I/O, Files, & Contexts",
    "section": "File Operation Details",
    "text": "File Operation Details\nNow that you have seen the basics of reading, writing, and appending, let’s go over a few details about file operations.\n\nOpening and Closing Files\nWhen working with files in Python, you’ll typically use the open() function with the syntax file = open(filename, mode). The filename parameter can be either a relative or absolute path to your target file. What you get back is a file object that serves as your interface to the file’s contents. The mode parameter is particularly important as it determines what operations you’re allowed to perform, including reading, writing, appending, or some combination of these actions (we’ll talk more about modes shortly).\nAn important aspect of file handling that is easy to overlook is properly closing files when you’re done with them. If you’re not using Python’s with statement (which automatically handles closing), you need to explicitly call file.close() when your operations are complete. This step is more important than it might seem at first glance: it releases system resources, ensures all data is properly written to disk, and prevents issues like file corruption and memory leaks. In long-running programs, failing to close files can even lead to running out of file descriptors, which can cause your program to crash. Well-behaved Python programs should ensure that file objects are closed when you are finished with them!\n\n\nFile Modes\nThe main file modes you will probably be using are read, write, append, and binary. You can even mix some of the modes when required!\nNote: There are some more modes, like update (\"+\"), that we won’t go over here. Check them out in the docs if you’re interested!\n\nRead: \"r\"\nThe most common way to open a file in Python is in read mode, which is represented by the letter “r”. It is the default mode if you don’t specify a mode. When you open a file in read mode, Python lets you read the content but doesn’t allow you to modify it. The reading automatically starts at the beginning of the file.\nOne thing to watch out for though: if you try to open a file that doesn’t exist in read mode, Python will raise a FileNotFoundError – you can’t read something that isn’t there! It’s generally a good idea to handle this potential error in your code, especially when working with user-specified file paths.\nCheck it out:\n\nwith open(example_text_file, \"r\") as file:\n    content = file.read()\n    print(content)\n\nHello, world!\nThis text will be added at the end\n\n\nSince read-mode is the default, we don’t have to pass in the \"r\":\n\nwith open(example_text_file) as file:\n    content = file.read()\n    print(content)\n\nHello, world!\nThis text will be added at the end\n\n\nHere’s an example of catching the file not found error:\n\ntry:\n    with open(\"imaginary_file.txt\") as file:\n        content = file.read()\n        print(content)\nexcept FileNotFoundError as error:\n    print(f\"{error=}\")\n\nerror=FileNotFoundError(2, 'No such file or directory')\n\n\n\n\nWrite: \"w\"\nYou use write mode (\"w\") when you need to create a file from scratch, or overwrite the contents of an existing file. Write mode gives you a “fresh start” on the given file each time it is opened.\nYou should be careful when opening a file in write mode. It doesn’t ask for confirmation before erasing existing content, so you’ll want to be absolutely sure you’re passing the correct file name before running your code!\n\n# Writing to a file (creates new or overwrites existing)\nwith open(example_text_file, \"w\") as file:\n    file.write(\"Hello, world!\")\n\nprint_file_contents(example_text_file)\n\nHello, world!\n\n\n\n\nAppend: \"a\"\nUnlike write mode, append opens a file for writing without erasing what’s already there. This lets you add data to existing files. If you open a file that doesn’t exist yet in append mode, a new file will be created automatically.\n\n# Appending to a file\nwith open(example_text_file, \"a\") as file:\n    file.write(\"\\nThis text will be added at the end\")\n\nprint_file_contents(example_text_file)\n\nHello, world!\nThis text will be added at the end\n\n\nSee how the output includes both Hello, world! and This text will be added at the end? That’s what append does!\n\n\nBinary: \"b\"\nThe “b” mode specifies binary operations. Adding it to your file mode (like “rb” for read-binary or “wb” for write-binary) tells Python to handle data as raw bytes rather than text. This is handy when you’re dealing with non-text files such as images, audio files, or custom binary formats.\nIn binary mode, no encoding or decoding processes occur: what you write is exactly what gets stored. Additionally, Python won’t perform any line ending translations that normally happen in text mode, ensuring your data is stored in the file exactly as written.\nYou can use binary mode to read the bytes from a PNG image:\n\nwith open(\"./_data/star.png\", \"rb\") as file:\n    image_data = file.read()\n    print(image_data[:11])\n    # Process the PNG data...\n\nb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00'\n\n\nUsing, \"wb\" let’s you write raw bytes to an output file:\n\n# Some mysterious bytes data\ndata = b\"\\x48\\x65\\x6c\\x6c\\x6f\\x2c\\x20\\x57\\x6f\\x72\\x6c\\x64\\x21\"\n\nwith open(\"./_tmp/binary_output\", \"wb\") as file:\n    # Write raw bytes to the file\n    file.write(data)\n\nprint_file_contents(\"./_tmp/binary_output\")\n\nHello, World!\n\n\nBinary mode might be a bit mysterious, so let me copy in a paragraph straight from the Python docs for the open() function that might help to make it more clear:\n\nAs mentioned in the Overview, Python distinguishes between binary and text I/O. Files opened in binary mode (including ‘b’ in the mode argument) return contents as bytes objects without any decoding. In text mode (the default, or when ‘t’ is included in the mode argument), the contents of the file are returned as str, the bytes having been first decoded using a platform-dependent encoding or using the specified encoding if given.\n\n\n\nText: \"t\"\nText mode is sort of the opposite of binary mode in a way. It handles data as strings with the default encoding/decoding scheme, automatically manages line ending differences between operating systems, and is most appropriate for human-readable text files like FASTA files, CSV, and other plain text files.\nNote: You generally don’t have to specify \"t\" manually as it is the default mode (as opposed to binary mode).\n\n# Text mode is the default, so the \"t\" is optional here.\n# Also...read mode is the default, so technically both \"r\" and \"t\"\n# are optional here!\nwith open(example_text_file, \"rt\") as file:\n    text = file.read()\n    print(text)\n\nHello, world!\nThis text will be added at the end\n\n\n\n\n\nFile Objects\nPython’s file handling system centers around file objects – interfaces that provide methods like read() and write() – to interact with underlying resources. Though named “file” objects, these abstractions extend beyond disk files.\n\nWhat Are File Objects?\nFile objects provide a file-like API (reading, writing) to some underlying resource (like an on-disk file, an in-memory buffer, standard input/output).\nThere are three categories of file objects: raw binary files, buffered binary files and text files.\nAll these interfaces are defined in the io module, though you typically don’t need to interact with this module directly. Rather, you generally create file objects using the open() function.\n\n\nCreating File Objects\nThe standard way to create a file object is through the built-in open() function, as in the examples above. This function determines which type of file object to create based on the mode and other parameters you provide.\nLet’s create a file object with open() and then access some info about it:\n\nwith open(example_text_file) as file:\n    print(\"Inside the 'with' block\")\n    print(f\"- {file.name=}\")\n    print(f\"- {file.mode=}\")\n    print(f\"- {file.closed=}\")\n\nprint(\"\\nOutside the 'with' block\")\nprint(f\"- {file.closed=}\")\n\nInside the 'with' block\n- file.name='./_data/sample.txt'\n- file.mode='r'\n- file.closed=False\n\nOutside the 'with' block\n- file.closed=True\n\n\n\nfile.name: Returns the name of the file\nfile.mode: Shows the mode in which the file was opened\nfile.closed: Boolean indicating if the file is closed\n\nLet’s see an example where we track our location in the file as we loop through its lines.\n\ndata_file = \"./_tmp/small_file.txt\"\n\n# First, write some data to work with\nwith open(data_file, \"wb\") as file:\n    file.write(b\"a\\n\")\n    file.write(b\"bc\\n\")\n    file.write(b\"def\\n\")\n    file.write(b\"ghij\\n\")\n    file.write(b\"klmno\\n\")\n\n\nprint_file_contents(data_file)\n\nwith open(data_file, \"rb\") as file:\n    print(f\"Before reading line 1:\")\n    print(f\"- {file.tell()=}\")\n\n    for i, line in enumerate(file):\n        print(f\"After reading line {i + 1}:\")\n        print(f\"- {file.tell()=}\")\n        print(f\"- {len(line)=}\")\n\na\nbc\ndef\nghij\nklmno\n\nBefore reading line 1:\n- file.tell()=0\nAfter reading line 1:\n- file.tell()=2\n- len(line)=2\nAfter reading line 2:\n- file.tell()=5\n- len(line)=3\nAfter reading line 3:\n- file.tell()=9\n- len(line)=4\nAfter reading line 4:\n- file.tell()=14\n- len(line)=5\nAfter reading line 5:\n- file.tell()=20\n- len(line)=6\n\n\nThis example used the tell() method:\n\nf.tell() returns an integer giving the file object’s current position in the file represented as number of bytes from the beginning of the file when in binary mode and an opaque number when in text mode.\n\nNote: it says “opaque number” in text mode because the encoding/decoding might make it so that the returned number doesn’t always line up with the number of bytes.\nFinally, let’s do something a bit tricky…\n\nwith open(data_file, \"rb\") as file:\n    print(f\"{file.read(2)=}\")\n    print(f\"{file.read(3)=}\")\n    print(f\"{file.read(4)=}\")\n\n    print(\"going back to the beginning!\")\n    file.seek(0)\n\n    print(\"starting to loop through the lines!\")\n    for line in file:\n        print(line)\n\n        if len(line) % 2 == 0:\n            file.seek(-len(line), 1)\n            extra_read = file.read(len(line))\n            print(f\"{extra_read=}\")\n\nfile.read(2)=b'a\\n'\nfile.read(3)=b'bc\\n'\nfile.read(4)=b'def\\n'\ngoing back to the beginning!\nstarting to loop through the lines!\nb'a\\n'\nextra_read=b'a\\n'\nb'bc\\n'\nb'def\\n'\nextra_read=b'def\\n'\nb'ghij\\n'\nb'klmno\\n'\nextra_read=b'klmno\\n'\n\n\nTo summarize the last two examples:\n\nPosition tracking with tell() and seek()\n\ntell(): Returns the current position of the file pointer\nseek(offset, whence): Moves the pointer to a specified position relative to the location specified by whence\nwhence can be 0 (start), 1 (current position), or 2 (end). (Not all options for whence are available in all modes! See the docs.)\n\n\nYou might not always need to manually move around files like this, but it is an option there for you when you need it!\n\n\nMore File Object Methods\nThe io module provides some other methods that you can use with file objects like readlines(), writelines(), and others. Check out the docs for the module to learn more!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#working-with-context-managers",
    "href": "io_files_contexts.html#working-with-context-managers",
    "title": "9  I/O, Files, & Contexts",
    "section": "Working with Context Managers",
    "text": "Working with Context Managers\nIn Python, the with statement is generally the preferred way to handle files, as it creates a context manager that automatically takes care of closing them. The syntax is straightforward: write with open(filename, mode) as file: and work with your file inside the indented block.\nWhat makes this approach so nice is that it guarantees proper cleanup even if exceptions occur during your file operations. This saves you from having to write explicit try/except blocks to ensure files get closed properly. The with statement also improves code readability by clearly defining the scope of your file operations. Additionally, if you need to work with multiple files at once, you can nest with statements, or put multiple with statements in a single line.\nHere’s a small example demonstrating the use of with:\n\nwith open(example_text_file, \"r\") as file:\n    content = file.read()\n    print(content)\n\n# The file will be closed once you get here,\n# so this will run the `except` clause.\ntry:\n    file.read()\nexcept ValueError as error:\n    print(f\"{error=}\")\n\nHello, world!\nThis text will be added at the end\nerror=ValueError('I/O operation on closed file.')\n\n\nWe have been using the with statement throughout the tutorial, but let’s take a bit of a deeper look at what is going on with it (ha).\nTake a look at this code, where we open a file for writing, then do a write, then explicitly close the file object:\n\nfile = open(\"./_tmp/some_file.txt\", \"w\")\n\nfile.write(\"Some data\\n\")\n\n# You should remember to close the file here after you're done with it!\nfile.close()\n\nCompare it to this code, where you don’t have to manage the lifecycle of the file object:\n\nwith open(\"./_tmp/some_file.txt\", \"w\") as file:\n    file.write(\"Some data\\n\")\n\n# No need to explicitly close the file!\n# `with` takes care of that for you\n\nIn the second example, you don’t have to worry about forgetting to close the file yourself!\nThere is actually a lot more to context managers than what we have covered here. However, you will probably be happy to know that we aren’t going to go into all that in this course!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#error-handling-in-file-operations",
    "href": "io_files_contexts.html#error-handling-in-file-operations",
    "title": "9  I/O, Files, & Contexts",
    "section": "Error Handling in File Operations",
    "text": "Error Handling in File Operations\nThere are a few common errors that file operations can raise. Let’s take a look at some of them now.\n\nFileNotFoundError\nA FileNotFoundError when opening nonexistent files. This occurs when you try to open a file that doesn’t exist:\n\ntry:\n    with open(\"nonexistent_file.txt\", \"r\") as file:\n        content = file.read()\nexcept FileNotFoundError as error:\n    print(f\"{error=}\")\n\nerror=FileNotFoundError(2, 'No such file or directory')\n\n\n\n\nPermissionError\nA PermissionError when lacking file access rights. This happens when your program doesn’t have the necessary permissions to access a file:\n\ntry:\n    with open(\"./_tmp/secret_file.txt\") as file:\n        content = file.read()\nexcept PermissionError as error:\n    print(f\"{error=}\")\n\nerror=PermissionError(13, 'Permission denied')\n\n\n\n\nIsADirectoryError and NotADirectoryError\nIsADirectoryError and NotADirectoryError occur when you confuse files and directories.\nTrying to open a directory as a file:\n\ntry:\n    with open(\"./_tmp\") as file:\n        content = file.read()\nexcept IsADirectoryError as error:\n    print(f\"{error=}\")\n\nerror=IsADirectoryError(21, 'Is a directory')\n\n\nIn this case, we are passing a directory where we expect to get a file, so it raises an error.\nTrying to use a file as a directory:\n\ntry:\n    os.listdir(example_text_file)\nexcept NotADirectoryError as error:\n    print(f\"{error=}\")\n\nerror=NotADirectoryError(20, 'Not a directory')\n\n\nHere we are using the listdir() function, which attempts to return a list containing the names of the entries in the given directory. However, it won’t work because we are passing it a file!\n\n\nCatching OSError\nSometimes, you might want to catch any type of OS error and handle them all in the same way. You can use OSError for this. Let’s rewrite the above examples to all catch OSError instead of the more specific error messages.\n\ntry:\n    with open(\"nonexistent_file.txt\", \"r\") as file:\n        content = file.read()\nexcept OSError as error:\n    print(f\"{error=}\")\n\ntry:\n    with open(\"./_tmp/secret_file.txt\") as file:\n        content = file.read()\nexcept OSError as error:\n    print(f\"{error=}\")\n\ntry:\n    with open(\"./_tmp\") as file:\n        content = file.read()\nexcept OSError as error:\n    print(f\"{error=}\")\n\ntry:\n    os.listdir(example_text_file)\nexcept OSError as error:\n    print(f\"{error=}\")\n\nerror=FileNotFoundError(2, 'No such file or directory')\nerror=PermissionError(13, 'Permission denied')\nerror=IsADirectoryError(21, 'Is a directory')\nerror=NotADirectoryError(20, 'Not a directory')\n\n\n\n\nCatching Multiple Specific Errors\nSometimes you may want to catch multiple different kinds of errors for a single operation. This way, you can give your users nice error messages, which can help them fix any problems that may have occurred. It’s generally a good idea to give as much detail as you think your users will need to help them understand what went wrong.\n\ntry:\n    with open(\"not_a_real_file.txt\") as file:\n        content = file.read()\nexcept FileNotFoundError:\n    print(\"File not found. Please check the file path.\")\nexcept PermissionError:\n    print(\"Permission denied. Check your access rights.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\nFile not found. Please check the file path.\n\n\n\n\n\n\n\n\nTip 9.2: Stop & Think\n\n\n\n\n\nHow could you improve the error messages in the previous code block?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#common-bioinformatics-file-formats",
    "href": "io_files_contexts.html#common-bioinformatics-file-formats",
    "title": "9  I/O, Files, & Contexts",
    "section": "Common Bioinformatics File Formats",
    "text": "Common Bioinformatics File Formats\nThe bioinformatics field relies on numerous specialized file formats to store and share biological data. Familiarity with these formats is important for any bioinformatics programmer. Let’s explore some of the common file formats you’ll encounter.\nFor this section, we will be using biopython. While it is always a fun activity to write your own parsers, it’s generally a good idea to stick with established solutions when they are available.\n\nFASTA\nFASTA is probably the most common sequence format in bioinformatics. It uses a simple structure with header lines (starting with ‘&gt;’ character) followed by the biological sequence data (DNA, RNA, or protein). It is widely used for storing and exchanging sequence data. For example, here are two sequences from UniProt in FASTA format.\n&gt;sp|P00452|RIR1_ECOLI Ribonucleoside-diphosphate reductase 1 subunit alpha OS=Escherichia coli (strain K12) OX=83333 GN=nrdA PE=1 SV=2\nMNQNLLVTKRDGSTERINLDKIHRVLDWAAEGLHNVSISQVELRSHIQFYDGIKTSDIHE\nTIIKAAADLISRDAPDYQYLAARLAIFHLRKKAYGQFEPPALYDHVVKMVEMGKYDNHLL\nEDYTEEEFKQMDTFIDHDRDMTFSYAAVKQLEGKYLVQNRVTGEIYESAQFLYILVAACL\nFSNYPRETRLQYVKRFYDAVSTFKISLPTPIMSGVRTPTRQFSSCVLIECGDSLDSINAT\nSSAIVKYVSQRAGIGINAGRIRALGSPIRGGEAFHTGCIPFYKHFQTAVKSCSQGGVRGG\nAATLFYPMWHLEVESLLVLKNNRGVEGNRVRHMDYGVQINKLMYTRLLKGEDITLFSPSD\nVPGLYDAFFADQEEFERLYTKYEKDDSIRKQRVKAVELFSLMMQERASTGRIYIQNVDHC\nNTHSPFDPAIAPVRQSNLCLEIALPTKPLNDVNDENGEIALCTLSAFNLGAINNLDELEE\nLAILAVRALDALLDYQDYPIPAAKRGAMGRRTLGIGVINFAYYLAKHGKRYSDGSANNLT\nHKTFEAIQYYLLKASNELAKEQGACPWFNETTYAKGILPIDTYKKDLDTIANEPLHYDWE\nALRESIKTHGLRNSTLSALMPSETSSQISNATNGIEPPRGYVSIKASKDGILRQVVPDYE\nHLHDAYELLWEMPGNDGYLQLVGIMQKFIDQSISANTNYDPSRFPSGKVPMQQLLKDLLT\nAYKFGVKTLYYQNTRDGAEDAQDDLVPSIQDDGCESGACKI\n&gt;sp|P37426|RIR1_SALTY Ribonucleoside-diphosphate reductase 1 subunit alpha OS=Salmonella typhimurium (strain LT2 / SGSC1412 / ATCC 700720) OX=99287 GN=nrdA PE=3 SV=1\nMNQSLLVTKRDGRTERINLDKIHRVLDWAAEGLNNVSVSQVELRSHIQFYDGIKTSDIHE\nTIIKAAADLISRDAPDYQYLAARLAIFHLRKKAFGQFEPPALYHHVVKMVELGKYDNHLL\nEDYTEEEFKQMDSFIVHDRDMTFSYAAVKQLEGKYLVQNRVTGEIYESAQFLYILVAACL\nFSNYPRETRLDYVKRFYDAVSTFKISLPTPIMSGVRTPTRQFSSCVLIECGDSLDSINAT\nSSAIVKYVSQRAGIGINAGRIRALGSPIRGGEAFHTGCIPFYKHFQTAVKSCSQGGVRGG\nAATLFYPMWHLEVESLLVLKNNRGVEGNRVRHMDYGVQINKLMYTRLLKGGDITLFSPSD\nVPGLYDAFFADQDEFERLYVKYEHDDSIRKQRVKAVELFSLMMQERASTGRIYIQNVDHC\nNTHSPFDPVVAPVRQSNLCLEIALPTKPLNDVNDENGEIALCTLSAFNLGAIKTLDELEE\nLAILAVRALDALLDYQDYPIPAAKRGAMGRRTLGIGVINFAYWLAKNGKRYSDGSANNLT\nHKTFEAIQYYLLKASNELAKEQGACPWFNETTYAKGILPIDTYKKDLDAIVNEPLHYDWE\nQLRESIKTHGLRNSTLSALMPSETSSQISNATNGIEPPRGYVSIKASKDGILRQVVPDYE\nHLKDAYELLWEMPNNDGYLQLVGIMQKFIDQSISANTNYDPSRFPSGKVPMQQLLKDLLT\nAYKFGVKTLYYQNTRDGAEDAQDDLAPSIQDDGCESGACKI\nNotice how the header line (the one starting with &gt;) has a regular format. That will not always be the case. The format of the header line is highly dependent on the vendor or the software that generated it. This is the same sequence as the first one, except that it was downloaded from NCBI rather than UniProt.\n&gt;NP_416737.1 ribonucleoside-diphosphate reductase 1 subunit alpha [Escherichia coli str. K-12 substr. MG1655]\nMNQNLLVTKRDGSTERINLDKIHRVLDWAAEGLHNVSISQVELRSHIQFYDGIKTSDIHETIIKAAADLI\nSRDAPDYQYLAARLAIFHLRKKAYGQFEPPALYDHVVKMVEMGKYDNHLLEDYTEEEFKQMDTFIDHDRD\nMTFSYAAVKQLEGKYLVQNRVTGEIYESAQFLYILVAACLFSNYPRETRLQYVKRFYDAVSTFKISLPTP\nIMSGVRTPTRQFSSCVLIECGDSLDSINATSSAIVKYVSQRAGIGINAGRIRALGSPIRGGEAFHTGCIP\nFYKHFQTAVKSCSQGGVRGGAATLFYPMWHLEVESLLVLKNNRGVEGNRVRHMDYGVQINKLMYTRLLKG\nEDITLFSPSDVPGLYDAFFADQEEFERLYTKYEKDDSIRKQRVKAVELFSLMMQERASTGRIYIQNVDHC\nNTHSPFDPAIAPVRQSNLCLEIALPTKPLNDVNDENGEIALCTLSAFNLGAINNLDELEELAILAVRALD\nALLDYQDYPIPAAKRGAMGRRTLGIGVINFAYYLAKHGKRYSDGSANNLTHKTFEAIQYYLLKASNELAK\nEQGACPWFNETTYAKGILPIDTYKKDLDTIANEPLHYDWEALRESIKTHGLRNSTLSALMPSETSSQISN\nATNGIEPPRGYVSIKASKDGILRQVVPDYEHLHDAYELLWEMPGNDGYLQLVGIMQKFIDQSISANTNYD\nPSRFPSGKVPMQQLLKDLLTAYKFGVKTLYYQNTRDGAEDAQDDLVPSIQDDGCESGACKI\nNot only is the header different, but the length of each of the lines in the sequence is different as well. You will even sometimes see the sequence all one line as well. A good parser will be able to handle these minor variations in the format.\n\nParsing FASTA Files\nThe simplest way to parse a FASTA file using biopython is by using the SeqIO.parse() function:\n\n# Loop over all records in the given FASTA file\nfor record in SeqIO.parse(\"./_data/example.fasta\", \"fasta\"):\n    # Print out some info about the returned SeqRecord instance\n    print()\n    print(f\"{type(record)=}\")\n    print(f\"{record.id=}\")\n    print(f\"{record.seq=}\")\n    print(f\"{len(record.seq)=}\")\n\n\ntype(record)=&lt;class 'Bio.SeqRecord.SeqRecord'&gt;\nrecord.id='sp|P00452|RIR1_ECOLI'\nrecord.seq=Seq('MNQNLLVTKRDGSTERINLDKIHRVLDWAAEGLHNVSISQVELRSHIQFYDGIK...CKI')\nlen(record.seq)=761\n\ntype(record)=&lt;class 'Bio.SeqRecord.SeqRecord'&gt;\nrecord.id='sp|P37426|RIR1_SALTY'\nrecord.seq=Seq('MNQSLLVTKRDGRTERINLDKIHRVLDWAAEGLNNVSVSQVELRSHIQFYDGIK...CKI')\nlen(record.seq)=761\n\n\nThis lets you iterate over all the records in the FASTA file by giving you SeqRecord instances for each record in the FASTA file. The SeqRecord class has many useful methods, so be sure to check out the docs when using it in your own research!\n\n\n\nFASTQ\nFASTQ extends the FASTA format by adding quality scores for each base in the sequence, making it the standard format for high-throughput sequencing data. Generally, each entry consists of four lines: a header (starting with ‘@’), the sequence, a separator line (starting with ‘+’), and Phred quality scores encoded as ASCII characters. For example:\n@HWI-ST741:607:HCJFYBCXX:2:1101:1362:1894 1:N:0:GCCAAT\nGGCTCATACAAATATTACTCCTTAAACGTGAGTATCGAATACAGCCATCAAAGATCTGAGATCCTTCGAA\n+\nIIIHHHIIIIHEGHIHHIIEHI@@@ECHFH@;D?EHHI@A--AFC-GHII?HHCHEHHH@-4+@EHE---\n@HWI-ST741:607:HCJFYBCXX:2:1101:1489:1973 1:N:0:GCCAAT\nGGAGCTTCATAAAAAATTCGGCTGTGACATTGTAATTCACATGTGTCATCATAGACAAGACCTTTCGTCT\n+\nFC///:/.D@ECC.FHF@.---7G?-AH-6@@-6BHEH?H?@G--55A:@4-6-6-55AHE?G-8-6@-6\nNote that there is the multi-line FASTQ format, but it is not as common.\n\nParsing FASTQ Files\nThis code is almost exactly the same as for parsing the FASTA file. The only difference is that we need to specify \"fastq\" for the SeqIO.parse() function.\n\n# Loop over all records in the given FASTA file\nfor record in SeqIO.parse(\"./_data/example.fastq\", \"fastq\"):\n    # Print out some info about the returned SeqRecord instance\n    print()\n    print(f\"{type(record)=}\")\n    print(f\"{record.id=}\")\n    print(f\"{record.seq=}\")\n    print(f\"{len(record.seq)=}\")\n\n\ntype(record)=&lt;class 'Bio.SeqRecord.SeqRecord'&gt;\nrecord.id='HWI-ST741:607:HCJFYBCXX:2:1101:1362:1894'\nrecord.seq=Seq('GGCTCATACAAATATTACTCCTTAAACGTGAGTATCGAATACAGCCATCAAAGA...GAA')\nlen(record.seq)=70\n\ntype(record)=&lt;class 'Bio.SeqRecord.SeqRecord'&gt;\nrecord.id='HWI-ST741:607:HCJFYBCXX:2:1101:1489:1973'\nrecord.seq=Seq('GGAGCTTCATAAAAAATTCGGCTGTGACATTGTAATTCACATGTGTCATCATAG...TCT')\nlen(record.seq)=70\n\n\nOne thing that’s really nice about biopython is that you can use the same interface for multiple different types of files!\n\n\n\n\n\n\nTip 9.3: Stop & Think\n\n\n\n\n\nWhat do you think would happen if you tried to parse a FASTA file, but passed \"fastq\" as the second argument to SeqIO.parse()?\n\n\n\n\n\n\nTabular Data\nWhile not a “bioinformatics” format per se, CSV and TSV are so common and important that I wanted to at least show an example of how to parse them in Python. Python’s built-in csv module makes working with tabular data like CSV and TSV files easy and flexible. It simplifies converting between tabular formats and Python data structures, streamlining both data import and export without added complexity. You can customize delimiters and use DictReader and DictWriter for more readable, field-based access.\nLet’s see it in action.\n\nParsing CSV/TSV\nSay we have a CSV file called example.csv representing a graph that looks like this:\nTaxa1,Taxa2,57\nTaxa1,Taxa3,89\nTaxa1,Taxa4,120\nTaxa2,Taxa3,73\nLet’s see how to parse it:\n\nwith open(\"./_data/example.csv\", newline=\"\") as csv_file:\n    for record in csv.DictReader(csv_file, fieldnames=(\"Source\", \"Target\", \"Score\")):\n        print(record)\n        print(record[\"Source\"], record[\"Target\"], sep=\" =&gt; \")\n        print()\n\n{'Source': 'Taxa1', 'Target': 'Taxa2', 'Score': '57'}\nTaxa1 =&gt; Taxa2\n\n{'Source': 'Taxa1', 'Target': 'Taxa3', 'Score': '89'}\nTaxa1 =&gt; Taxa3\n\n{'Source': 'Taxa1', 'Target': 'Taxa4', 'Score': '120'}\nTaxa1 =&gt; Taxa4\n\n{'Source': 'Taxa2', 'Target': 'Taxa3', 'Score': '73'}\nTaxa2 =&gt; Taxa3\n\n\n\nThe following code opens and reads a CSV file, then processes and prints each record in a specific format.\n\n# Open the CSV file located at \"./_data/example.csv\" in read mode\n# - The 'newline=\"\"' argument ensures consistent newline handling across\n#   platforms. It activates universal newlines mode, but line endings are\n#   returned to the caller untranslated.\nwith open(\"./_data/example.csv\", newline=\"\") as csv_file:\n    # Use csv.DictReader to iterate through each row of the CSV file\n    # - fieldnames=(\"Source\", \"Target\", \"Score\") specifies column names to use\n    # - If the CSV file already has headers, you would typically omit this\n    #   parameter\n    for record in csv.DictReader(\n        csv_file,\n        fieldnames=(\"Source\", \"Target\", \"Score\"),\n    ):\n        # Print the entire record as a dictionary\n        print(record)\n\n        # Print just the Source and Target values, separated by \" =&gt; \"\n        print(record[\"Source\"], record[\"Target\"], sep=\" =&gt; \")\n\n        # Print an empty line for better readability between records\n        print()\n\n{'Source': 'Taxa1', 'Target': 'Taxa2', 'Score': '57'}\nTaxa1 =&gt; Taxa2\n\n{'Source': 'Taxa1', 'Target': 'Taxa3', 'Score': '89'}\nTaxa1 =&gt; Taxa3\n\n{'Source': 'Taxa1', 'Target': 'Taxa4', 'Score': '120'}\nTaxa1 =&gt; Taxa4\n\n{'Source': 'Taxa2', 'Target': 'Taxa3', 'Score': '73'}\nTaxa2 =&gt; Taxa3\n\n\n\nIn this case, we specified the field names, since our input file did not have a header row. There are a lot of other options that can be specified, but this simple example will take you pretty far!\nLet’s see one more example, but this time the CSV file has a header row. It’s pretty much the same, except that we don’t need to specify the fieldnames.\n\nwith open(\"./_data/example_with_header.csv\", newline=\"\") as csv_file:\n    for record in csv.DictReader(csv_file):\n        print(record)\n        print(record[\"Source\"], record[\"Target\"], sep=\" =&gt; \")\n        print()\n\n{'Source': 'Taxa1', 'Target': 'Taxa2', 'Score': '57'}\nTaxa1 =&gt; Taxa2\n\n{'Source': 'Taxa1', 'Target': 'Taxa3', 'Score': '89'}\nTaxa1 =&gt; Taxa3\n\n{'Source': 'Taxa1', 'Target': 'Taxa4', 'Score': '120'}\nTaxa1 =&gt; Taxa4\n\n{'Source': 'Taxa2', 'Target': 'Taxa3', 'Score': '73'}\nTaxa2 =&gt; Taxa3\n\n\n\n\n\n\n\n\n\nTip 9.4: Stop & Think\n\n\n\n\n\nWhat do you think would happen if you did not specify the field names in a CSV file that did not have a header line?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#example-processing-fastq-files",
    "href": "io_files_contexts.html#example-processing-fastq-files",
    "title": "9  I/O, Files, & Contexts",
    "section": "Example: Processing FASTQ Files",
    "text": "Example: Processing FASTQ Files\nTo wrap up, let’s see a small example that reads in FASTQ files for two samples, and then generates plots of the distribution of quality scores and the GC content across reads in both samples.\n\n# Define a list of sample names for processing\nsample_names = [\"Sample_1\", \"Sample_2\"]\n\n# Create a dictionary mapping sample names to their respective FASTQ file\n# paths\nfastq_files = {\n    \"Sample_1\": \"./_data/sample_1.fastq\",\n    \"Sample_2\": \"./_data/sample_2.fastq\",\n}\n\n# Initialize an empty list to store processed data from each sequence record\nrecords = []\n\n# Loop through each sample\nfor sample in sample_names:\n    # Parse each FASTQ file using BioPython's SeqIO module\n    for record in SeqIO.parse(fastq_files[sample], \"fastq\"):\n        # Calculate the mean quality score for the current sequence\n        quality_score = np.mean(record.letter_annotations[\"phred_quality\"])\n\n        # Calculate the GC content as a percentage using BioPython's SeqUtils\n        gc_content = SeqUtils.gc_fraction(record) * 100\n\n        # Add the sample information, quality score, and GC content to our\n        # records list\n        records.append(\n            {\n                \"Sample\": sample,\n                \"Mean Quality Score\": quality_score,\n                \"GC Content (%)\": gc_content,\n            }\n        )\n\n# Convert the collected records into a pandas DataFrame for analysis\nquality_score_data = pd.DataFrame(records)\n\n# Display the DataFrame to show the collected data\ndisplay(quality_score_data)\n\n# Create a kernel density estimate (KDE) plot for the quality scores,\n# separating the samples by color (hue)\nsns.displot(\n    quality_score_data,\n    kind=\"kde\",  # Create a kernel density estimate plot\n    x=\"Mean Quality Score\",  # Use quality scores for x-axis\n    hue=\"Sample\",  # Color by sample\n    fill=True,  # Fill the area under the curves\n    height=2,  # Set plot height\n    aspect=2,  # Set plot width:height ratio\n)\n\nsns.displot(\n    quality_score_data,\n    kind=\"kde\",  # Create a kernel density estimate plot\n    x=\"GC Content (%)\",  # Use GC content for x-axis\n    hue=\"Sample\",  # Color by sample\n    fill=True,  # Fill the area under the curves\n    height=2,  # Set plot height\n    aspect=2,  # Set plot width:height ratio\n)\n\n\n\n\n\n\n\n\nSample\nMean Quality Score\nGC Content (%)\n\n\n\n\n0\nSample_1\n23.757143\n58.571429\n\n\n1\nSample_1\n24.114286\n62.857143\n\n\n2\nSample_1\n22.328571\n54.285714\n\n\n3\nSample_1\n23.357143\n65.714286\n\n\n4\nSample_1\n22.157143\n71.428571\n\n\n...\n...\n...\n...\n\n\n1995\nSample_2\n33.885714\n55.714286\n\n\n1996\nSample_2\n32.700000\n54.285714\n\n\n1997\nSample_2\n31.071429\n45.714286\n\n\n1998\nSample_2\n29.771429\n42.857143\n\n\n1999\nSample_2\n33.471429\n52.857143\n\n\n\n\n2000 rows × 3 columns",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#wrap-up",
    "href": "io_files_contexts.html#wrap-up",
    "title": "9  I/O, Files, & Contexts",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nIn this chapter, we’ve explored the fundamentals of file handling in Python, with a particular focus on bioinformatics applications. We covered how to read from, write to, and append to files using different modes like text and binary. We also learned about context managers with the with statement, which ensure proper resource cleanup, and explored common file-related error handling techniques.\nBeyond the basics, we examined how to work with common bioinformatics file formats like FASTA and FASTQ using BioPython, and saw how to process tabular data with Python’s csv module. The practical example of processing FASTQ files demonstrated how these concepts might come together in real bioinformatics workflows.\nThese file handling skills are essential for any bioinformatics programmer, as many analyses involve importing, processing, and exporting data from various file formats. As you continue your programming journey, these techniques will serve as key components in your applications.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#suggested-reading",
    "href": "io_files_contexts.html#suggested-reading",
    "title": "9  I/O, Files, & Contexts",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nPython’s Input & Output docs\nPython’s io module docs\nContext Managers and Python’s with Statement",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "io_files_contexts.html#practice-problems",
    "href": "io_files_contexts.html#practice-problems",
    "title": "9  I/O, Files, & Contexts",
    "section": "Practice Problems",
    "text": "Practice Problems\nGive these problems a try if you’d like some extra practice! They’re organized into groups based on similar levels of difficulty.\nYou can find the solutions here: Appendix H\n\nGroup 1\n\nOpen a file called data.txt for reading, print its type, then close it.\nWrite “Hello, World!” into a file named test.txt.\nRead and print all text from a file named sample.txt.\nRead a file line by line and print each line without the trailing newline character(s).\nAppend the text “New Entry” to log.txt.\nPrint the file’s name and mode after opening it.\nWrite three lines to multi.txt: “One”, “Two”, “Three”, each on its own line.\nUse a for-loop to write the numbers 1-5 to a file (one per line).\nPrint \"File is closed\" if file is closed after exiting a with-block.\nUse readline() to read and print just the first line of sample.txt.\nCreate a function that prints the contents of a file it is given.\nUse a for loop to write a list of fruits into a file, one fruit per line.\nRead and print the first eight characters of sample.txt.\nDemonstrate that opening an existing file in write mode (\"w\") mode erases its contents.\nUse a try-except block to print a message if not_a_file.txt does not exist.\nPrint file position (using .tell()) before and after reading 4 bytes.\nWrite binary bytes b'ABC' to a file called bytes.bin.\nRead the binary file you just created (bytes.bin) and print the first five bytes.\nUse \"rt\" mode to read text and \"wb\" mode to write bytes.\nPrint the error message if a file open operation raises an OSError.\nPrint the first line from a file, then use .seek(0) to go back to the beginning of the file and re-print the first line.\nUse with statement to write the line \"Finished!\" into finished.txt.\nOpen the file finished.txt and append the line \"Appending again!\".\nCreate a dictionary, and write each key-value pair to a file (format: key =&gt; value).\nPrint current working directory using os.getcwd() module.\nList files in the current directory with os.listdir().\nPass a file name to os.listdir(), then handle the error using try/except.\nAfter writing three lines to a file called sample.txt, read the file and print the number of lines. (Use writelines() and readlines().)\nUse seek to skip the first 3 bytes then print the rest of the file.\nCatch any OSError when trying to open a file.\n\nSolutions: Section H.2\n\n\nGroup 2\n\nRead all lines from data.txt into a list, then write every second line to even_lines.txt.\nWrite user input (entered with input()) to a file called user.txt.\nOpen data.txt for writing and write 10 lines (\"Line {i}\"). Then, open the same file again and append a summary line: \"Total lines: 10\".\nWrite each character of a string to a new line in a text file.\nAsk for a filename. Try to read and print it, or print “Not found!” if the file does not exist.\nWrite an integer list to a text file, then read it and compute their sum.\nRead up to the 10th character of a file and print those characters backwards.\nWrite a file, then read its contents twice using seek().\nWrite three words to a file, each on their own line. Then, print all the lines of that file in uppercase.\nWrite some lines to a file, including some empty lines. Then, read the file back, counting the number of empty lines.\nWrite two lists (genes and counts) into a file as gene,count rows.\nWrite some lines to a file, some of which contain the word \"gene\". Then, open that file and print every line that contains the word \"gene\".\nRead the contents from one file and write it uppercased to another file. (Read the input file line-by-line.)\nTry to open a file that doesn’t exist without crashing the program.\nCreate a list of dictionaries like this: {\"A\": 1, \"B\": 2, \"C\": 3}. Then write the data as a CSV file with a header line.\nCreate a small FASTA file. Then, read the file and count how many lines in a file start with “&gt;”.\nCopy the header lines from the FASTA file you just created into another file. Do not print the &gt; in the output file.\nWrite a few lines to a file. One of the lines should be \"exit\". Then, read the lines of the file you created, but stop as soon as you read the \"exit\" line.\nOpen an output file, write one line, then print the output of file.closed. Next, use with to open the file, and after the block, print the result of file.closed again.\nWrite three numbers to a binary file as bytes, then read, and print them as integers.\n\nSolutions: Section H.3\n\n\nGroup 3\n\nUsing biopython, write code that opens a FASTA file and (1) prints the sequence ID and length for each sequence, and (2) prints the mean sequence length. (Use the FASTA sequence you created earlier.)\nWrite the contents of a dictionary to a TSV file. Each line should be like key\\tvalue. Then read the file, insert any lines where the value is greater than or equal to 10 into a new dictionary.\nUsing pandas, create a data frame with the following data: {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [7, 8, 9]}, and write it to a CSV without the row index. Read the resulting file using csv.DictReader. Print any record in which the value in field “A” is &gt;= 2 and the value in field “C” is &lt;= 8.\nWrite code that opens a FASTQ file, then prints the id and average quality score for the first 10 records.\nRead a binary file and print each byte in hexadecimal. (Use the built-in hex() function.)\nTry to read and print the contents of a list of files. If any file doesn’t exist, skip it and print a message about the file not being found.\nWrite the given gene_data to a file. Then, read the lines of the file, extracting gene names and sequences from each line using using regular expressions. Finally, print each gene name and sequence in the format “name =&gt; sequence”.\nCreate a file containing 50 random words chosen from the following list [\"apple\", \"pie\", \"is\", \"good\"]. Read that file and count how many times each word occurs. Print the dictionary sorted by word count. Don’t forget to set the random seed for reproducibility!\nWithout using the CSV module, read a CSV file. If any of the lines have a different number of fields, stop the iteration and print an error message.\nGiven a file path, open the file either as text or binary based on its extension (.txt – text mode, .bin – binary mode), and print the contents. Make sure to handle file not found errors!\n\nSolutions: Section H.4",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>I/O, Files, & Contexts</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/think_answers.html",
    "href": "errors_exceptions/think_answers.html",
    "title": "Appendix A — Stop & Think Answers — Chapter 6",
    "section": "",
    "text": "Tip 6.1: When Python tries to evaluate the name gene and doesn’t find it in any scope, it raises a NameError, which matches the exception type specified in the except clause. This causes a message with the error details to be printed to the console.\nTip 6.2: FileNotFoundError\nTip 6.3: We could check if the expression value is “na” before trying to convert it, or use a try/except block to catch the ValueError and set a default value (like None, 1, 0, or NaN).\nTip 6.4: When analyzing sequencing datasets, one error might trigger others in a cascade. For example, a file reading error might lead to missing data, which then causes calculation errors. This chain makes it harder to find the root cause of the error.\nTip 6.5: Specific exceptions might include: FileNotFoundError, PermissionError, IsADirectoryError. All of these could be caught by OSError, which is the parent class for file-related errors.\nTip 6.6: The finally clause is useful when working with resources that need to be released regardless of success or failure, such as closing file handles or database connections.\nTip 6.7: Possible custom exceptions: InvalidSequenceError, AlignmentFailedError, LowCoverageError, DifferentialExpressionError, etc.",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Stop & Think Answers</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html",
    "href": "errors_exceptions/practice_solutions.html",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "",
    "text": "Solution Section 6.9.1",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.1",
    "href": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.1",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "",
    "text": "try:\n    value = float(\"abc\")\nexcept ValueError:\n    print(\"Not a valid number\")",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.2",
    "href": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.2",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "Solution Section 6.9.2",
    "text": "Solution Section 6.9.2\ncounts = {\"A\": 1, \"C\": 2, \"G\": 0, \"T\": 4}\ntotal = sum(counts.values())\n\ntry:\n    n_ratio = counts[\"N\"] / total\nexcept KeyError:\n    print(\"N is not present in the counts dictionary\")\n    n_ratio = None",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.3",
    "href": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.3",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "Solution Section 6.9.3",
    "text": "Solution Section 6.9.3\ntry:\n    silly_divide(5, 0)\nexcept ZeroDivisionError:\n    print(\"you can't divide by zero!\")\nexcept Exception as error:\n    print(f\"a mysterious error occurred: {error=}\")",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.4",
    "href": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.4",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "Solution Section 6.9.4",
    "text": "Solution Section 6.9.4\ndef fold_change(expression_1, expression_2)\n    try:\n        return expression_1 / expression_2\n    except ZeroDivisionError:\n        print(\"expression_2 was zero!\")\n        return None",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.5",
    "href": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.5",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "Solution Section 6.9.5",
    "text": "Solution Section 6.9.5\nclass SequenceLengthError(Exception):\n    pass\n\nMIN_LENGTH = 50\nMAX_LENGTH = 150\n\ndef validate_sequence_length(sequence):\n    sequence_length = len(sequence)\n\n    if sequence_length &lt; MIN_LENGTH:\n        raise SequenceLengthError(f\"sequence length {sequence_length} was too short!\")\n\n    if sequence_length &gt; MAX_LENGTH:\n        raise SequenceLengthError(f\"sequence length {sequence_length} was too long!\")\n\n    return None",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.6",
    "href": "errors_exceptions/practice_solutions.html#solution-sec-problem-6.6",
    "title": "Appendix B — Practice Problem Solutions — Chapter 6",
    "section": "Solution Section 6.9.6",
    "text": "Solution Section 6.9.6\ndef run_simulation(max_turns):\n    if max_turns &lt; 1:\n        raise ValueError(f\"Expected at least 1 iteration, but got {max_turns=}\")\n\n    if max_turns &gt; 1000:\n        raise ValueError(f\"Expected at most 1000 iterations, but got {max_turns=}\")\n\n    # Simulation code would follow\n    pass",
    "crumbs": [
      "Appendices",
      "Errors & Exceptions",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/think_answers.html",
    "href": "exploratory_data_analysis/think_answers.html",
    "title": "Appendix C — Stop & Think Answers — Chapter 7",
    "section": "",
    "text": "Tip 7.1 response:  Looking at a summary of your data has a ton of benefits.  Summarizing your data can help you:\n\nIdentify data import issues, like importing the wrong file\nIdentify data quality issues, like missing values or outliers\nBecome familiar with the data, including data types and ranges\n\nThe more familiar you are with your data, the better!\nTip 7.3 response: Clean visualizations aren’t just important for presenting data to others, they are also important for presenting data to yourself!  A messy graph may hide trends, overload the viewer, or be otherwise difficult to interpret, potentially leading to misinterpretations or missed patterns.  Professional-looking visualizations can also be shared with colleagues through presentations and reports with minimal modifications, freeing up time later.  Lastly, it’s never bad to get in the habit of producing clear and effective visualizations of your data!\nTip 7.6 response: Choosing a good color palette is critical for communicating information about your data.  Improper color choice can easily lead to misinterpretations.  In the heatmap example, we use a diverging color scheme to show the difference between positive and negative correlations, but the balance of those colors must be correct:\n\nIf the center of the palette was on 0.2 rather than zero, it would visually suggest that values around 0.2 are “neutral” or “average,” rather than “positive”\nWith asymmetric color intensity (brightest blue at -0.2, brightest orange at 1.0), it would create visual bias, making positive correlations appear weaker than negative ones of the same magnitude\n\nIn both cases, viewers would easily be thrown off by the color scheme, even with a legend available.\nTip 7.2 response: Filtering columns by category can help manage cognitive load and distraction when working with large datasets, allowing you to focus on particular variables. It can make it easier to identify patterns, facilitate more targeted analysis, and help build intuition about how different aspects of the data relate to each other.\nTip 7.5 response: Correlation values can show potential relationships between variables and suggest further avenues for investigation. However, they have limitations: they only measure linear relationships, can be heavily influenced by outliers, and don’t indicate causation. They should be considered as starting points for deeper analysis rather than conclusive findings.\nTip 7.4 response: While we are most used to linear scales, logarithmic scales are useful in many situations:\n⁃ When data spans several orders of magnitude, log scales lett you visualize both small and large values effectively on the same plot ⁃ When data follows exponential growth patterns, like in the early stages of an epidemic ⁃ When looking for proportional or percentage changes rather than count changes, like in the case of this library example\nIn fact, visualizing data on a logarithmic scale can sometimes reveal trends that are not apparent from count data, as in this example using cancer data.\nLogarithmic transformations also have some nifty statistical applications, like reducing skew in data.\nTip 7.7 response: Heatmaps add an extra visual layer that is not present in tables (color), making strong correlations visually obvious.  Clustering reveals groups of variables that behave similarly, and the dendrograms show how those variables are related to each other.  Together, these components make patterns easier to spot than a table alone and add hierarchical structure that can’t be easily represented in a data table.\nTip 7.8 response: When merging datasets, it’s important to consider:\n\nWhether incomplete data is acceptable or problematic.  If problematic, choose an inner join.\nIf either dataset is primary to your analysis.  If no, choose an outer join.  If yes, choose a left or right join according to the dataset.\n\nAlong the way, consider how representative the resulting dataset will be and whether you will be able to use it effectively for any planned analyses or visualizations.",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Stop & Think Answers</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/practice_solutions.html",
    "href": "exploratory_data_analysis/practice_solutions.html",
    "title": "Appendix D — Practice Problem Solutions — Chapter 7",
    "section": "",
    "text": "Solution Section 7.9.1",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.1",
    "href": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.1",
    "title": "Appendix D — Practice Problem Solutions — Chapter 7",
    "section": "",
    "text": "df = pd.DataFrame(state_cancer_data)",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.2",
    "href": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.2",
    "title": "Appendix D — Practice Problem Solutions — Chapter 7",
    "section": "Solution Section 7.9.2",
    "text": "Solution Section 7.9.2\ndf[\"Cancer Deaths Per 100k\"] = df[\"Cancer Deaths\"] / df[\"Population\"] * 100_000",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.3",
    "href": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.3",
    "title": "Appendix D — Practice Problem Solutions — Chapter 7",
    "section": "Solution Section 7.9.3",
    "text": "Solution Section 7.9.3\ndf.query(\"`Cancer Deaths Per 100k` &gt;= 180 and `Median Household Income` &lt; 68_500\")",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.4",
    "href": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.4",
    "title": "Appendix D — Practice Problem Solutions — Chapter 7",
    "section": "Solution Section 7.9.4",
    "text": "Solution Section 7.9.4\ndf.plot(kind=\"scatter\", x=\"Median Household Income\", y=\"Cancer Deaths Per 100k\")",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.5",
    "href": "exploratory_data_analysis/practice_solutions.html#solution-sec-problem-7.5",
    "title": "Appendix D — Practice Problem Solutions — Chapter 7",
    "section": "Solution Section 7.9.5",
    "text": "Solution Section 7.9.5\ndf.plot(kind=\"scatter\", x=\"Percent Aged 65+\", y=\"Cancer Deaths Per 100k\")",
    "crumbs": [
      "Appendices",
      "Exploratory Data Analysis",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "stats_models/think_answers.html",
    "href": "stats_models/think_answers.html",
    "title": "Appendix E — Stop & Think Answers — Chapter 8",
    "section": "",
    "text": "Tip 8.1: Relying solely on p-values can lead to overinterpreting small, biologically meaningless differences as important simply because they’re statistically significant, especially with large sample sizes.\nTip 8.2: You might choose a nonparametric test like Mann-Whitney when your data doesn’t follow a normal distribution or when you have small sample sizes and can’t verify distributional assumptions.\nTip 8.5: This is definitely a personal preference type of questions! But I kind of like the 2nd option. It’s a bit “fancy” but it’s nice because it only uses data contained in the data frame itself and doesn’t require going through the keys in the REGIONS map.\nTip 8.3: A result can be statistically significant but have such a small effect size that it’s biologically meaningless, or a result can have a large biological effect but fail to reach statistical significance due to small sample size.\nTip 8.4: In drug studies, a medication might show a statistically significant reduction in some biomarker (p &lt; 0.05), but the actual change might be so small (tiny effect size) that it doesn’t translate to any meaningful clinical improvement for patients.\nTip 8.6: Regions whose confidence intervals don’t overlap have significantly different cancer death rates. From the plot, it appears the West has significantly lower death rates than several other regions, particularly the Southeast.\nTip 8.7: An R-squared value close to zero indicates that the linear model explains almost none of the variation in the dependent variable, suggesting there’s no linear relationship between the predictor and response variables.\nTip 8.8: You can determine important predictors by examining their coefficients and p-values in the summary output. Predictors with larger absolute coefficient values and p-values &lt; 0.05 (like X1 and X2 in our example) are more important to the model.\nTip 8.9: Although we didn’t go over the biplot, it can be a useful tool for interpreting the principal components. That said, you can still examine the loadings returned by the PCA model to understand how each feature relates to PC 1 (the x-axis). Specifically, you can compute the angle each loading vector makes with the x-axis. Features with small angles (or angles close to 180°) align closely with PC 1. For example, you could use something like result.loadings.apply(lambda row: np.arctan2(row[1], row[0]) * 180 / np.pi, axis=1) to calculate these angles. In this dataset, petal length and petal width are most strongly associated with PC 1.\nTip 8.10: One potential explanation that k-means performed less well for virginica is because it overlaps more with versicolor in the feature space, while setosa is more distinct. This suggests virginica and versicolor share more similar morphological characteristics.",
    "crumbs": [
      "Appendices",
      "Statistics & Modeling",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Stop & Think Answers</span>"
    ]
  },
  {
    "objectID": "stats_models/practice_solutions.html",
    "href": "stats_models/practice_solutions.html",
    "title": "Appendix F — Practice Problem Solutions — Chapter 8",
    "section": "",
    "text": "The solutions use the following imports:\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster as cluster\nimport scipy.stats as stats\nimport seaborn as sns\nimport statsmodels.formula.api\n\n\nSolution Section 8.7.1\n\nnp.random.seed(2503478)\ngroup_A = stats.norm(loc=10, scale=2).rvs(30)\ngroup_B = stats.norm(loc=15, scale=2).rvs(30)\nresult = stats.ttest_ind(group_A, group_B)\nprint(result)\n\nTtestResult(statistic=np.float64(-8.473531241332175), pvalue=np.float64(9.899983754486164e-12), df=np.float64(58.0))\n\n\n\n\nSolution Section 8.7.2\n\nnp.random.seed(493567)\ngroup_A = stats.norm(loc=10, scale=2).rvs(30)\ngroup_B = stats.norm(loc=15, scale=2).rvs(30)\ngroup_C = stats.norm(loc=11, scale=2).rvs(30)\nresult = stats.f_oneway(group_A, group_B, group_C)\nprint(result)\n\nF_onewayResult(statistic=np.float64(56.614548038664914), pvalue=np.float64(1.7891834179400852e-16))\n\n\n\n\nSolution Section 8.7.3\n\nresult = stats.tukey_hsd(group_A, group_B, group_C)\nprint(result)\n\nTukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\nComparison  Statistic  p-value  Lower CI  Upper CI\n (0 - 1)     -4.656     0.000    -5.787    -3.525\n (0 - 2)     -0.643     0.369    -1.774     0.488\n (1 - 0)      4.656     0.000     3.525     5.787\n (1 - 2)      4.013     0.000     2.882     5.144\n (2 - 0)      0.643     0.369    -0.488     1.774\n (2 - 1)     -4.013     0.000    -5.144    -2.882\n\n\n\n\n\nSolution Section 8.7.4\n\nnp.random.seed(932847)\nx1 = np.random.uniform(-10, 10, 50)\nx2 = np.random.uniform(-10, 10, 50)\ny = 3 * x1 + np.random.normal(0, 2, 50)\ndf = pd.DataFrame({\"X1\": x1, \"X2\": x2, \"Y\": y})\nmodel = statsmodels.formula.api.ols(\"Y ~ X1 + X2\", data=df).fit()\nprint(model.summary().tables[1])\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.4118      0.295     -1.395      0.170      -1.006       0.182\nX1             3.0064      0.046     65.950      0.000       2.915       3.098\nX2            -0.0319      0.053     -0.606      0.547      -0.138       0.074\n==============================================================================\n\n\n\n\nSolution Section 8.7.5\nTwo clusters mainly separate setosa from versicolor and virginica, which makes biological sense given that setosa is the most distinct species.\n\niris = pd.read_csv(\"../_data/iris.csv\")\nsns.pairplot(iris, hue=\"Species\")\n_centroids, labels = cluster.vq.kmeans2(iris.drop(columns=\"Species\"), k=2)\nsns.pairplot(iris.assign(Cluster=labels), hue=\"Cluster\")",
    "crumbs": [
      "Appendices",
      "Statistics & Modeling",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "io_files_contexts/think_answers.html",
    "href": "io_files_contexts/think_answers.html",
    "title": "Appendix G — Stop & Think Answers — Chapter 9",
    "section": "",
    "text": "Tip 9.1: This file no longer included Hello, this is my first file! because the write mode (\"w\") will overwrite the contents of existing files by default.\nTip 9.2: You could include the name of the file that caused the error.\nTip 9.3: It won’t work correctly! At the time of writing this chapter, biopython gives a warning about the file format.\nTip 9.4: If the fieldnames argument is omitted, the values in the first row of the file will be used as the field names and will be omitted from the results.",
    "crumbs": [
      "Appendices",
      "I/O, Files, & Contexts",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Stop & Think Answers</span>"
    ]
  },
  {
    "objectID": "io_files_contexts/practice_solutions.html",
    "href": "io_files_contexts/practice_solutions.html",
    "title": "Appendix H — Practice Problem Solutions — Chapter 9",
    "section": "",
    "text": "Set Up\nSet up some fake data that we will use in the practice problems.\nwith open(\"sample.txt\", \"w\") as file:\n    file.write(\"first line\\nsecond line\\nthird line\\n\")\n\ndata_lines = [\n    \"1. Basics\\n\",\n    \"2. Collections\\n\",\n    \"3. Algorithms\\n\",\n    \"4. Functions\\n\",\n    \"5. OOP\\n\",\n    \"6. Errors\\n\",\n    \"7. EDA\\n\",\n    \"8. Stats\\n\",\n    \"9. I/O\\n\",\n]\nwith open(\"data.txt\", \"w\") as file:\n    file.writelines(data_lines)",
    "crumbs": [
      "Appendices",
      "I/O, Files, & Contexts",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "io_files_contexts/practice_solutions.html#sec-io-files-contexts-practice-problem-solutions-group-1",
    "href": "io_files_contexts/practice_solutions.html#sec-io-files-contexts-practice-problem-solutions-group-1",
    "title": "Appendix H — Practice Problem Solutions — Chapter 9",
    "section": "Group 1",
    "text": "Group 1\n\nOpen a file called data.txt for reading, print its type, then close it.\n\n\nfile = open(\"data.txt\", \"r\")\nprint(type(file))\nfile.close()\n\n&lt;class '_io.TextIOWrapper'&gt;\n\n\n\nWrite “Hello, World!” into a file named test.txt.\n\n\nwith open(\"test.txt\", \"w\") as f:\n    f.write(\"Hello, World!\")\n\n\nRead and print all text from a file named sample.txt.\n\n\nwith open(\"sample.txt\") as f:\n    content = f.read()\n    print(content)\n\nfirst line\nsecond line\nthird line\n\n\n\n\nRead a file line by line and print each line without the trailing newline character(s).\n\n\nwith open(\"sample.txt\") as file:\n    for line in file:\n        print(line.strip())\n\nfirst line\nsecond line\nthird line\n\n\n\nAppend the text “New Entry” to log.txt.\n\n\nwith open(\"log.txt\", \"a\") as file:\n    file.write(\"New Entry\\n\")\n\n\nPrint the file’s name and mode after opening it.\n\n\nwith open(\"sample.txt\") as file:\n    print(file.name)\n    print(file.mode)\n\nsample.txt\nr\n\n\n\nWrite three lines to multi.txt: “One”, “Two”, “Three”, each on its own line.\n\n\nwith open(\"multi.txt\", \"w\") as file:\n    file.write(\"One\\n\")\n    file.write(\"Two\\n\")\n    file.write(\"Three\\n\")\n\n\nUse a for-loop to write the numbers 1-5 to a file (one per line).\n\n\nwith open(\"numbers.txt\", \"w\") as file:\n    for i in range(1, 6):\n        file.write(str(i) + \"\\n\")\n\n\nPrint \"File is closed\" if file is closed after exiting a with-block.\n\n\nwith open(\"sample.txt\") as file:\n    pass\n\nif file.closed:\n    print(\"File is closed\")\n\nFile is closed\n\n\n\nUse readline() to read and print just the first line of sample.txt.\n\n\nwith open(\"sample.txt\") as file:\n    print(file.readline().strip())\n\nfirst line\n\n\n\nCreate a function that prints the contents of a file it is given.\n\n\ndef print_file(file_path):\n    with open(file_path) as file:\n        print(file.read())\n\n\nUse a for loop to write a list of fruits into a file, one fruit per line.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nwith open(\"fruits.txt\", \"w\") as file:\n    for fruit in fruits:\n        file.write(fruit + \"\\n\")\n\n\nRead and print the first eight characters of sample.txt.\n\n\nwith open(\"sample.txt\", \"rb\") as file:\n    print(file.read(8))\n\nb'first li'\n\n\n\nDemonstrate that opening an existing file in write mode (\"w\") mode erases its contents.\n\n\nwith open(\"test.txt\", \"w\") as file:\n    file.write(\"contents\")\n\nwith open(\"test.txt\", \"w\") as file:\n    file.write(\"Overwritten!\")\n\nwith open(\"test.txt\") as file:\n    contents = file.read()\n    assert contents == \"Overwritten!\"\n\n\nUse a try-except block to print a message if not_a_file.txt does not exist.\n\n\ntry:\n    with open(\"not_a_file.txt\") as file:\n        data = file.read()\nexcept FileNotFoundError as error:\n    print(f\"{error=}\")\n\nerror=FileNotFoundError(2, 'No such file or directory')\n\n\n\nPrint file position (using .tell()) before and after reading 4 bytes.\n\n\nwith open(\"sample.txt\", \"rb\") as file:\n    print(file.tell())\n    file.read(4)\n    print(file.tell())\n\n0\n4\n\n\n\nWrite binary bytes b'ABC' to a file called bytes.bin.\n\n\nwith open(\"bytes.bin\", \"wb\") as file:\n    file.write(b\"ABCDEFGHIJK\")\n\n\nRead the binary file you just created (bytes.bin) and print the first five bytes.\n\n\nwith open(\"bytes.bin\", \"rb\") as f:\n    print(f.read(5))\n\nb'ABCDE'\n\n\n\nUse \"rt\" mode to read text and \"wb\" mode to write bytes.\n\n\nwith open(\"sample.txt\", \"rt\") as file:\n    print(file.read())\nwith open(\"bytes.bin\", \"wb\") as file:\n    file.write(b\"xyz\")\n\nfirst line\nsecond line\nthird line\n\n\n\n\nPrint the error message if a file open operation raises an OSError.\n\n\ntry:\n    with open(\"/fake/file.txt\") as file:\n        content = file.read()\nexcept OSError as error:\n    print(f\"{error=}\")\n\nerror=FileNotFoundError(2, 'No such file or directory')\n\n\n\nPrint the first line from a file, then use .seek(0) to go back to the beginning of the file and re-print the first line.\n\n\nwith open(\"sample.txt\") as file:\n    print(file.readline().strip())\n    file.seek(0)\n    print(file.readline().strip())\n\nfirst line\nfirst line\n\n\n\nUse with statement to write the line \"Finished!\" into finished.txt.\n\n\nwith open(\"finished.txt\", \"w\") as file:\n    file.write(\"Finished!\\n\")\n\n\nOpen the file finished.txt and append the line \"Appending again!\".\n\n\nwith open(\"finished.txt\", \"a\") as file:\n    file.write(\"Appending again!\\n\")\n\n\nCreate a dictionary, and write each key-value pair to a file (format: key =&gt; value).\n\n\nd = {\"A\": 1, \"B\": 2}\nwith open(\"dict.txt\", \"w\") as file:\n    for k, v in d.items():\n        file.write(f\"{k} =&gt; {v}\\n\")\n\n\nPrint current working directory using os.getcwd() module.\n\nimport os\nprint(os.getcwd())\n\nList files in the current directory with os.listdir().\n\nimport os\nprint(os.listdir(\".\"))\n\nPass a file name to os.listdir(), then handle the error using try/except.\n\n\nimport os\n\ndirname = \"sample.txt\"\n\ntry:\n    os.listdir(dirname)\nexcept NotADirectoryError:\n    print(f\"'{dirname}' is not a directory!\")\n\n'sample.txt' is not a directory!\n\n\n\nAfter writing three lines to a file called sample.txt, read the file and print the number of lines. (Use writelines() and readlines().)\n\n\nlines = [\"first line\\n\", \"second line\\n\", \"third line\\n\"]\n\nwith open(\"sample.txt\", \"w\") as file:\n    file.writelines(lines)\n\nwith open(\"sample.txt\") as file:\n    print(len(file.readlines()))\n\n3\n\n\n\nUse seek to skip the first 3 bytes then print the rest of the file.\n\n\nwith open(\"sample.txt\", \"rb\") as file:\n    file.seek(3)\n    print(file.read())\n\nb'st line\\nsecond line\\nthird line\\n'\n\n\n\nCatch any OSError when trying to open a file.\n\n\ntry:\n    with open(\"maybe_missing.txt\") as file:\n        data = file.read()\nexcept OSError as error:\n    print(\"Caught OSError:\", error)\n\nCaught OSError: [Errno 2] No such file or directory: 'maybe_missing.txt'",
    "crumbs": [
      "Appendices",
      "I/O, Files, & Contexts",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "io_files_contexts/practice_solutions.html#sec-io-files-contexts-practice-problem-solutions-group-2",
    "href": "io_files_contexts/practice_solutions.html#sec-io-files-contexts-practice-problem-solutions-group-2",
    "title": "Appendix H — Practice Problem Solutions — Chapter 9",
    "section": "Group 2",
    "text": "Group 2\n\nRead all lines from data.txt into a list, then write every second line to even_lines.txt.\n\n\nwith open(\"data.txt\") as file:\n    lines = file.readlines()\n\nwith open(\"even_lines.txt\", \"w\") as file:\n    for i, line in enumerate(lines):\n        if i % 2 == 1:\n            file.write(line)\n\n# Check your work!\nwith open(\"even_lines.txt\") as file:\n    for line in file:\n        print(line.strip())\n\n2. Collections\n4. Functions\n6. Errors\n8. Stats\n\n\n\nWrite user input (entered with input()) to a file called user.txt.\n\ntext = input(\"Enter something: \")\nwith open(\"user.txt\", \"w\") as f:\n    f.write(text)\n\nOpen data.txt for writing and write 10 lines (\"Line {i}\"). Then, open the same file again and append a summary line: \"Total lines: 10\".\n\n\nwith open(\"data.txt\", \"w\") as file:\n    for i in range(10):\n        file.write(f\"Line {i + 1}\\n\")\n\nwith open(\"data.txt\", \"a\") as file:\n    file.write(\"Total lines: 10\\n\")\n\n# Check your work!\nwith open(\"data.txt\") as file:\n    print(file.read().strip())\n\nLine 1\nLine 2\nLine 3\nLine 4\nLine 5\nLine 6\nLine 7\nLine 8\nLine 9\nLine 10\nTotal lines: 10\n\n\n\nWrite each character of a string to a new line in a text file.\n\n\nmessage = \"coding is cool\"\nwith open(\"chars.txt\", \"w\") as file:\n    for letter in message:\n        file.write(f\"{letter}\\n\")\n\n# Check your work!\nwith open(\"chars.txt\") as file:\n    print(file.read().strip())\n\nc\no\nd\ni\nn\ng\n \ni\ns\n \nc\no\no\nl\n\n\n\nAsk for a filename. Try to read and print it, or print “Not found!” if the file does not exist.\n\nfilename = input(\"Filename: \")\ntry:\n    with open(filename) as f:\n        print(f.read())\nexcept FileNotFoundError:\n    print(f\"{filename} was not found!\")\nTODO: make a note about the stdin stuff needind to put the {} around python to try it for yourself.\n\nWrite an integer list to a text file, then read it and compute their sum.\n\n\nnumbers = [1, 2, 3, 4]\n\nwith open(\"numbers.txt\", \"w\") as file:\n    for number in numbers:\n        file.write(str(number) + \"\\n\")\n\nwith open(\"numbers.txt\") as file:\n    total = sum(int(line.strip()) for line in file)\n\nprint(total)\n\n10\n\n\n\nRead up to the 10th character of a file and print those characters backwards.\nwith open('sample.txt') as f:\n    text = f.read(10)\n    print(text[::-1])\nWrite a file, then read its contents twice using seek().\n\n\nwith open(\"temp.txt\", \"w\") as file:\n    file.write(\"Magic Beans\\n\")\n\nwith open(\"temp.txt\") as file:\n    contents = file.read()\n    print(contents.strip())\n\n    file.seek(0)\n\n    contents = file.read()\n    print(contents.strip())\n\nMagic Beans\nMagic Beans\n\n\nTODO: decide on a couple of file names and just use those\n\nWrite three words to a file, each on their own line. Then, print all the lines of that file in uppercase.\n\n\nwords = [\"apple\", \"pie\", \"is\", \"good\"]\n\nwith open(\"numbers.txt\", \"w\") as file:\n    for word in words:\n        file.write(f\"{word}\\n\")\n\nwith open(\"numbers.txt\") as file:\n    for line in file:\n        print(line.strip().upper())\n\nAPPLE\nPIE\nIS\nGOOD\n\n\n\nWrite some lines to a file, including some empty lines. Then, read the file back, counting the number of empty lines.\n\n\nwith open(\"sample.txt\", \"w\") as file:\n    for line in [\"this\", \"\", \"is\", \"a\", \"\", \"line\"]:\n        file.write(f\"{line}\\n\")\n\nwith open(\"sample.txt\") as file:\n    blank_line_count = sum(line.strip() == \"\" for line in file)\n\nprint(f\"there were {blank_line_count} empty lines!\")\n\nthere were 2 empty lines!\n\n\n\nWrite two lists (genes and counts) into a file as gene,count rows.\n\n\ngenes = [\"nrdA\", \"nrdJ\"]\ncounts = [10, 20]\n\nwith open(\"pairs.csv\", \"w\") as file:\n    for gene, count in zip(genes, counts):\n        file.write(f\"{gene},{count}\\n\")\n\n# Check your work!\nwith open(\"pairs.csv\") as file:\n    print(file.read().strip())\n\nnrdA,10\nnrdJ,20\n\n\n\nWrite some lines to a file, some of which contain the word \"gene\". Then, open that file and print every line that contains the word \"gene\".\n\n\nwith open(\"data.txt\", \"w\") as file:\n    file.writelines(\n        [\n            \"gene therapy\\n\",\n            \"protein sequences\\n\",\n            \"gene annotation\\n\",\n            \"analyzing gene expression\\n\",\n            \"multiple sequence alignment\\n\",\n        ]\n    )\n\nwith open(\"data.txt\") as file:\n    for line in file:\n        if \"gene\" in line:\n            print(line.strip())\n\ngene therapy\ngene annotation\nanalyzing gene expression\n\n\n\nRead the contents from one file and write it uppercased to another file. (Read the input file line-by-line.)\n\n\nwith open(\"data.txt\") as input_file, open(\"upper.txt\", \"w\") as output_file:\n    for line in input_file:\n        output_file.write(line.upper())\n\n# Check your work!\nwith open(\"upper.txt\") as file:\n    for line in file:\n        print(line.strip())\n\nGENE THERAPY\nPROTEIN SEQUENCES\nGENE ANNOTATION\nANALYZING GENE EXPRESSION\nMULTIPLE SEQUENCE ALIGNMENT\n\n\n\nTry to open a file that doesn’t exist without crashing the program.\n\n\ntry:\n    with open('/fake/file.txt') as file:\n        _ = file.read()\nexcept OSError as error:\n    print(f\"{error=}\")\n\nerror=FileNotFoundError(2, 'No such file or directory')\n\n\n\nCreate a list of dictionaries like this: {\"A\": 1, \"B\": 2, \"C\": 3}. Then write the data as a CSV file with a header line.\n\n\nrows = [\n    {\"A\": 1, \"B\": 4, \"C\": 7},\n    {\"A\": 2, \"B\": 5, \"C\": 8},\n    {\"A\": 3, \"B\": 6, \"C\": 9},\n]\n\nwith open(\"table.csv\", \"w\") as file:\n    file.write(\"A,B,C\\n\")\n    for row in rows:\n        values = [str(value) for value in row.values()]\n        line = \",\".join(values)\n        file.write(f\"{line}\\n\")\n\n# Check your work!\nwith open(\"table.csv\") as file:\n    for line in file:\n        print(line.strip())\n\nA,B,C\n1,4,7\n2,5,8\n3,6,9\n\n\n\nCreate a small FASTA file. Then, read the file and count how many lines in a file start with “&gt;”.\n\n\nwith open(\"sequences.fasta\", \"w\") as file:\n    file.write(\"&gt;seq_1\\n\")\n    file.write(\"ACTG\\n\")\n    file.write(\"&gt;seq_2\\n\")\n    file.write(\"GGCAC\\n\")\n    file.write(\"&gt;seq_3\\n\")\n    file.write(\"AAACTA\\n\")\n\n\nwith open(\"sequences.fasta\") as file:\n    record_count = sum(line.startswith(\"&gt;\") for line in file)\n\nprint(record_count)\n\n3\n\n\n\nCopy the header lines from the FASTA file you just created into another file. Do not print the &gt; in the output file.\n\n\nwith open(\"sequences.fasta\") as fasta_file, open(\"headers.txt\", \"w\") as output_file:\n    for line in fasta_file:\n        if line.startswith(\"&gt;\"):\n            output_line = line.strip()[1:] + \"\\n\"\n            output_file.write(output_line)\n\n# Check your work!\nwith open(\"headers.txt\") as file:\n    for line in file:\n        print(line.strip())\n\nseq_1\nseq_2\nseq_3\n\n\n\nWrite a few lines to a file. One of the lines should be \"exit\". Then, read the lines of the file you created, but stop as soon as you read the \"exit\" line.\n\n\nwith open(\"data.txt\", \"w\") as file:\n    file.writelines(\n        [\n            \"line 1\\n\",\n            \"line 2\\n\",\n            \"exit\\n\",\n            \"line 3\\n\",\n        ]\n    )\n\nwith open(\"data.txt\") as file:\n    for line in file:\n        line = line.strip()\n        if line == \"exit\":\n            break\n\n        print(line)\n\nline 1\nline 2\n\n\n\nOpen an output file, write one line, then print the output of file.closed. Next, use with to open the file, and after the block, print the result of file.closed again.\n\n\nfile = open(\"output.txt\", \"w\")\nfile.write(\"gene 1\\n\")\nprint(file.closed)\nfile.close()\n\nwith open(\"output.txt\", \"w\") as file:\n    file.write(\"gene 2\\n\")\n\nprint(file.closed)\n\nFalse\nTrue\n\n\n\nWrite three numbers to a binary file as bytes, then read, and print them as integers.\n\n\nnumbers = [7, 8, 9]\n\nwith open(\"numbers.dat\", \"wb\") as file:\n    file.write(bytes(numbers))\n\nwith open(\"numbers.dat\", \"rb\") as file:\n    data = file.read()\n    print(type(data))\n    print(data)\n\nprint(list(data))\n\n&lt;class 'bytes'&gt;\nb'\\x07\\x08\\t'\n[7, 8, 9]",
    "crumbs": [
      "Appendices",
      "I/O, Files, & Contexts",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "io_files_contexts/practice_solutions.html#sec-io-files-contexts-practice-problem-solutions-group-3",
    "href": "io_files_contexts/practice_solutions.html#sec-io-files-contexts-practice-problem-solutions-group-3",
    "title": "Appendix H — Practice Problem Solutions — Chapter 9",
    "section": "Group 3",
    "text": "Group 3\n\nUsing biopython, write code that opens a FASTA file and (1) prints the sequence ID and length for each sequence, and (2) prints the mean sequence length. (Use the FASTA sequence you created earlier.)\n\n\nsequence_count = 0\ntotal_length = 0\n\nfrom Bio import SeqIO\n\nfor record in SeqIO.parse(\"sequences.fasta\", \"fasta\"):\n    sequence_count += 1\n    seq_length = len(record.seq)\n    total_length += seq_length\n\n    print(record.id, seq_length, sep=\"\\t\")\n\nprint(\"\\nTotal sequences:\", sequence_count)\nprint(\"\\nMean length:\", total_length / sequence_count)\n\nseq_1   4\nseq_2   5\nseq_3   6\n\nTotal sequences: 3\n\nMean length: 5.0\n\n\n\nWrite the contents of a dictionary to a TSV file. Each line should be like key\\tvalue. Then read the file, insert any lines where the value is greater than or equal to 10 into a new dictionary.\n\n\ndata = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 10, \"e\": 20, \"f\": 30}\nwith open(\"dict.tsv\", \"w\") as file:\n    for key, value in data.items():\n        line = f\"{key}\\t{value}\\n\"\n        file.write(line)\n\nfiltered_data = {}\nwith open(\"dict.tsv\") as file:\n    for line in file:\n        key, value = line.strip().split(\"\\t\")\n        if int(value) &gt;= 10:\n            filtered_data[key] = value\n\nprint(filtered_data)\n\n{'d': '10', 'e': '20', 'f': '30'}\n\n\n\nUsing pandas, create a data frame with the following data: {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [7, 8, 9]}, and write it to a CSV without the row index. Read the resulting file using csv.DictReader. Print any record in which the value in field “A” is &gt;= 2 and the value in field “C” is &lt;= 8.\n\n\nimport csv\nimport pandas as pd\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [7, 8, 9]})\ndf.to_csv(\"df.csv\", index=False)\n\nwith open(\"df.csv\", newline=\"\") as file:\n    for record in csv.DictReader(file):\n        if int(record[\"A\"]) &gt;= 2 and int(record[\"C\"]) &lt;= 8:\n            print(record)\n\n{'A': '2', 'B': '5', 'C': '8'}\n\n\n\nWrite code that opens a FASTQ file, then prints the id and average quality score for the first 10 records.\n\n\nfrom Bio import SeqIO\nimport numpy as np\n\n# TODO: get the data in the write location\nfor i, record in enumerate(SeqIO.parse(\"../_data/sample_1.fastq\", \"fastq\")):\n    if i &gt;= 10:\n        break\n\n    quality_score = np.mean(record.letter_annotations[\"phred_quality\"])\n    print(record.id, quality_score, sep=\" =&gt; \")\n\nread_0 =&gt; 23.757142857142856\nread_1 =&gt; 24.114285714285714\nread_2 =&gt; 22.32857142857143\nread_3 =&gt; 23.357142857142858\nread_4 =&gt; 22.15714285714286\nread_5 =&gt; 25.071428571428573\nread_6 =&gt; 25.87142857142857\nread_7 =&gt; 22.185714285714287\nread_8 =&gt; 23.87142857142857\nread_9 =&gt; 24.257142857142856\n\n\n\nRead a binary file and print each byte in hexadecimal. (Use the built-in hex() function.)\n\n\nwith open(\"data.bin\", \"wb\") as file:\n    file.write(b\"apple pie\")\n\nwith open(\"data.bin\", \"rb\") as file:\n    data = file.read()\n\nfor byte in data:\n    print(hex(byte))\n\n0x61\n0x70\n0x70\n0x6c\n0x65\n0x20\n0x70\n0x69\n0x65\n\n\n\nTry to read and print the contents of a list of files. If any file doesn’t exist, skip it and print a message about the file not being found.\n\n\nfilenames = [\"fake.txt\", \"data.txt\", \"nope.txt\"]\n\nfor filename in filenames:\n    print()\n    try:\n        with open(filename) as file:\n            print(f\"found {filename}!\")\n            print(file.read().strip())\n            print(\"DONE!\")\n    except FileNotFoundError:\n        print(f\"file '{filename}' not found\")\n\n\nfile 'fake.txt' not found\n\nfound data.txt!\nline 1\nline 2\nexit\nline 3\nDONE!\n\nfile 'nope.txt' not found\n\n\n\nWrite the given gene_data to a file. Then, read the lines of the file, extracting gene names and sequences from each line using using regular expressions. Finally, print each gene name and sequence in the format “name =&gt; sequence”.\n\n\nimport re\n\ngene_data = [\n    \"gene: nrdA; seq: AACCTTG\\n\",\n    \"gene: nrdJd; seq: ACACGGT\\n\",\n    \"gene: pol; seq: AAACGGTAA\\n\",\n]\n\nwith open(\"gene_data.txt\", \"w\") as file:\n    file.writelines(gene_data)\n\npattern = r\"gene: ([a-zA-Z]+); seq: ([ACTG]+)\"\n\nwith open(\"gene_data.txt\") as file:\n    for line in file:\n        matches = re.fullmatch(pattern, line.strip())\n        gene_name = matches[1]\n        sequence = matches[2]\n        print(gene_name, sequence, sep=\" =&gt; \")\n\nnrdA =&gt; AACCTTG\nnrdJd =&gt; ACACGGT\npol =&gt; AAACGGTAA\n\n\n\nCreate a file containing 50 random words chosen from the following list [\"apple\", \"pie\", \"is\", \"good\"]. Read that file and count how many times each word occurs. Print the dictionary sorted by word count. Don’t forget to set the random seed for reproducibility!\n\n\nfrom collections import Counter\nimport random\n\nrandom.seed(2341)\n\nwith open(\"words.txt\", \"w\") as file:\n    for word in random.choices([\"apple\", \"pie\", \"is\", \"good\"], k=50):\n        file.write(word + \"\\n\")\n\nwith open(\"words.txt\") as f:\n    words = f.read().split()\n\ncounts = Counter(words)\n\nfor word in sorted(counts):\n    print(word, counts[word])\n\napple 13\ngood 16\nis 12\npie 9\n\n\n\nWithout using the CSV module, read a CSV file. If any of the lines have a different number of fields, stop the iteration and print an error message.\n\n\nwith open(\"df.csv\", \"a\") as file:\n    file.write(\"1,2,3,4\\n\")\n\nwith open(\"df.csv\") as file:\n    fields = file.readline().strip().split(\",\")\n    expected_length = len(fields)\n\n    for line in file:\n        line = line.strip()\n        fields = line.split(\",\")\n\n        if len(fields) != expected_length:\n            print(\n                \"ERROR\",\n                f\"line '{line}'\",\n                f\"expected: {expected_length} fields\",\n                f\"found: {len(fields)} fields\",\n                sep=\" -- \"\n            )\n            break\n\nERROR -- line '1,2,3,4' -- expected: 3 fields -- found: 4 fields\n\n\n\nGiven a file path, open the file either as text or binary based on its extension (.txt – text mode, .bin – binary mode), and print the contents. Make sure to handle file not found errors!\n\n\npath = \"file.txt\"\n\nif path.endswith(\".bin\"):\n    mode = \"rb\"\nelse:\n    mode = \"r\"\n\ntry:\n    with open(path, mode) as file:\n        print(file.read())\nexcept FileNotFoundError as error:\n    print(f\"file '{path}' not found!\")\n\nfile 'file.txt' not found!",
    "crumbs": [
      "Appendices",
      "I/O, Files, & Contexts",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Practice Problem Solutions</span>"
    ]
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "Appendix I — Regular Expressions",
    "section": "",
    "text": "Importing the Regular Expression module\nRegular expressions (regex) are powerful tools for working with text data, and are commonly used in bioinformatics applications. They are essentially a specialized mini-language for pattern matching in strings.\nRegular expressions allow you to:\nLet’s take a whirlwind tour of the basics of regular expressions in Python. This will not be comprehensive, but should provide you with just enough of the basics to get some real work done, and be able to understand more comprehensive documentation.\nTo use regular expressions, you first need to import the re module.\nimport re",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  },
  {
    "objectID": "regex.html#basic-pattern-matching",
    "href": "regex.html#basic-pattern-matching",
    "title": "Appendix I — Regular Expressions",
    "section": "Basic Pattern Matching",
    "text": "Basic Pattern Matching\nThe simplest use of regex is searching for an exact substring within a larger string:\n\n# This checks if \"banana\" appears anywhere in the string\nif re.search(r\"banana\", \"apple and banana\"):\n    print(\"found banana!\")\n\nfound banana!\n\n\nThe r before the pattern string creates a “raw string”. This is a good practice with regex to avoid problems with backslashes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  },
  {
    "objectID": "regex.html#special-symbols-and-character-classes",
    "href": "regex.html#special-symbols-and-character-classes",
    "title": "Appendix I — Regular Expressions",
    "section": "Special Symbols and Character Classes",
    "text": "Special Symbols and Character Classes\nRegex provides special symbols to match categories of characters or to control the matching in some other way. Here are some of the more common ones.\n\n.: Matches any character except newline\n^: Matches at the start of the string\n$ Matches at the end of the string\n+: Matches one or more of the preceding regex\n*: Matches zero or more of the preceding regex\n?: Matches zero or one repetitions of the preceding regex\n{m}: Matches exactly m copies of the preceding regex\n{m,n}: Matches from m to n copies of the preceding regex\n\\d: Matches any digit (0-9)\n\\w: Matches any “word character” (alphanumeric + underscore)\n\\s: Matches any whitespace\n[]: Used to indicate a set of characters\n(): Used to indicate a capture group\n\nThere are many more available in Python, but this set will get you pretty far.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  },
  {
    "objectID": "regex.html#anchoring-patterns",
    "href": "regex.html#anchoring-patterns",
    "title": "Appendix I — Regular Expressions",
    "section": "Anchoring Patterns",
    "text": "Anchoring Patterns\nSometimes you want to check if a pattern appears at a specific position in a string. The ^ symbol anchors the match to the beginning of the string:\n\n# Check if \"apple\" appears at the beginning of the string\n# The ^ character is an anchor that means \"match at the start of the string\"\nif re.search(r\"^apple\", \"banana and apple\"):\n    # This won't execute because \"banana and apple\" doesn't start with \"apple\"\n    print(\"apple found at the beginning\")\nelse:\n    # This will execute because the pattern doesn't match\n    print(\"apple not found at the beginning\")\n\napple not found at the beginning\n\n\nThe $ symbol anchors the match to the end of the string:\n\n# Check if \"banana\" appears at the end of the string\n# The $ character is an anchor that means \"match at the end of the string\"\nif re.search(r\"banana$\", \"banana and apple\"):\n    # This won't execute because \"banana and apple\" doesn't end with \"banana\"\n    print(\"banana found at the end\")\nelse:\n    # This will execute because the pattern doesn't match\n    print(\"banana not found at the end\")\n\nbanana not found at the end\n\n\nYou can use both anchors to require a full-string exact match:\n\n# Check if the entire string is exactly \"abcd\"\n# Using both ^ and $ together checks if the pattern matches the entire string\n# ^ = start anchor, $ = end anchor\nif re.search(r\"^abcd$\", \"abcdef\"):\n    # This won't execute because \"abcdef\" is not exactly \"abcd\"\n    print(\"full match\")\nelse:\n    # This will execute because the pattern doesn't completely match\n    # The string \"abcdef\" has extra characters beyond \"abcd\"\n    print(\"not a full match\")\n\nnot a full match",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  },
  {
    "objectID": "regex.html#character-classes",
    "href": "regex.html#character-classes",
    "title": "Appendix I — Regular Expressions",
    "section": "Character Classes",
    "text": "Character Classes\nYou can filter based on groups of characters.\n\n# Loop through a list of sample names\nfor sample in [\"Treatment 1\", \"Treatment 2\", \"Control 1\", \"Control 2\"]:\n    # Search for the pattern \"Treatment\" followed by space and one or more digits\n    # - \\d is a character class that matches any digit\n    # - \\d+ means \"one or more digits\"\n    if re.search(r\"Treatment \\d+\", sample):\n        # This will only print for strings that match our pattern\n        # In this case, \"Treatment 1\" and \"Treatment 2\" will match\n        print(\"found a treatment sample\")\n\nfound a treatment sample\nfound a treatment sample",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  },
  {
    "objectID": "regex.html#capture-groups",
    "href": "regex.html#capture-groups",
    "title": "Appendix I — Regular Expressions",
    "section": "Capture Groups",
    "text": "Capture Groups\nYou can pull out specific pieces of info using capture groups.\nIn this example, we have some strings that represent users. The basic format is name=X;age=Y. Our goal is to first check if the given string matches this format, and if it does, pull out the name and age values from the string. Here is the regex that we can use to do it:\n\nuser_info = re.compile(r\"name=(\\w+);age=(\\d+)\")\n\nHere is the break down:\n\n\\w+: at least one word character\n(\\w+): “capture” the group of at least one word character (this represents the name)\nname=(\\w+): match patterns like name=Bob, and extract the name value (Bob)\n\\d+: at least one digit character\n(\\d+): “capture” the group of at least one digit (this represents the age)\nage=(\\d+): match patterns like age=47, and extract the age value (47)\nname=(\\w+);age=(\\d+): combines the above name and age pattern and requires that they are separated with a semicolon\nre.compile: compiles the regex so we can use it for matching later\n\nNow, let’s use that regex.\n\n# List of user data strings in different formats\nusers = [\"name=Rafael;age=34\", \"NAME=Dev,age:25\", \"name=Page;age=46\"]\n\n# Loop through each user string in the list\nfor user in users:\n    # Apply our regex pattern to search for matches in the current string\n    # re.search() returns a match object if found, None otherwise\n    result = re.search(user_info, user)\n\n    # Check if a match was found\n    if result is not None:\n        # Extract the first capture group (the name)\n        # - This is what matched (\\w+) after \"name=\"\n        # - result[0] would be the entire match,\n        # - result[1] is the first capture group\n        user_name = result[1]\n\n        # Extract the second capture group (the age)\n        # - This is what matched (\\d+) after \"age=\"\n        # - result[2] is the second capture group\n        user_age = result[2]\n\n        # Display the extracted information\n        print(f\"name: {user_name}; age: {user_age}\")\n    else:\n        # If no match was found, show which string failed to match our pattern\n        # This happens with \"NAME=Dev,age:25\" since it uses uppercase and\n        # different format\n        print(f\"no match for {user}\")\n\nname: Rafael; age: 34\nno match for NAME=Dev,age:25\nname: Page; age: 46\n\n\nWhen we run this code, only the first and third strings match our pattern. The second string uses uppercase “NAME” and has a different format (comma and colon instead of semicolon), so it doesn’t match our regex pattern.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  },
  {
    "objectID": "regex.html#wrap-up",
    "href": "regex.html#wrap-up",
    "title": "Appendix I — Regular Expressions",
    "section": "Wrap-Up",
    "text": "Wrap-Up\nIn this section, we covered the basics of using regular expressions in Python including, exact matching, anchored matching, character classes, and capture groups. Examples include searching for patterns, validating string formats, and extracting structured data. This should give you just enough info for you to get started using regular expressions in your own work.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Regular Expressions</span>"
    ]
  }
]