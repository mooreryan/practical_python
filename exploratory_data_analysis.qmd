---
author: "Ryan M. Moore, PhD"
date-modified: last-modified
date: "2025-04-01"
jupyter: python3
---

# Intro to Exploratory Data Analysis with Python {#sec-eda}

In this tutorial, we will learn the basics of exploratory data analysis in Python using Pandas (and just a tiny bit of seaborn).

## Introduction

Exploratory Data Analysis, or EDA for short, is the process of building an understanding of your data. Before jumping into complex statistical analyses or building predictive models, EDA helps you understand what your data actually contains. It's about visually and statistically summarizing your dataset, identifying patterns, spotting anomalies, and generating hypotheses.

EDA is a critical step in discovery-based research (sometimes known as foundational or exploratory research). As biologists, you will be familiar with hypothesis-driven research, whereby you start with the answer (the hypothesis), and try to work back to either prove or disprove it using the scientific method. Discovery-based research fits in even before hypothesis-driven research can begin, and is especially useful in cases where we know so little about the topic or system in question that we can't craft useful hypotheses. One of its main goals is to build understanding of complex systems and generate hypotheses that can be tested in the more classical style of hypothesis-driven research, and EDA is a critical step in this process.

While EDA is often closely connected with discovery-based research, it is important to note that it is also a critical aspect of hypothesis-driven processes as well. For example, EDA can be a powerful tool for data quality control and assumption checking. It's important to identify missing values, outliers, or other bad data that could compromise your analysis. Further, many statistical methods have assumptions about your data (like normality or constant variance of errors). EDA helps you verify if these assumptions are reasonable. Without proper exploration of your data, you might miss critical insights or, worse, draw incorrect conclusions from your analyses.

EDA has a role in helping you to build an intuition for your problem domain and your data. Regularly engaging with EDA will help you get a "feel" for your data and better understand its strengths and limitations. This intuition is critical for effective communication of your findings and for productive discussion of your data and problem domain with collaborators and stakeholders, or in publications.

### Important Python libraries for doing EDA

Python has a strong set of libraries for exploratory data analysis (EDA). Here are some of the more common ones:

- [Pandas](https://pandas.pydata.org/): Essential for working with tabular data, offering powerful DataFrame operations.
- [NumPy](https://numpy.org/): Provides fast array operations, forming the backbone of numerical computing in Python.
- [SciPy](https://scipy.org/): Useful for advanced statistical analysis and scientific computing.
- Statsmodels: Extends on the statistical models provided by SciPy and provides and alternative interface.
- [Matplotlib](https://matplotlib.org/): A versatile library for creating static, animated, and interactive plots.
- [Seaborn](https://seaborn.pydata.org/): Simplifies statistical visualization with built-in themes and functions.
- [Jupyter](https://jupyter.org/) and [Quarto](https://quarto.org/): Computational notebooks

There are many more, but you will see these popping up again and again.

### Why We're Using Pandas

We're using Pandas in this tutorial because:

- Works well with tabular data: Most biological data is structured like a spreadsheet or database table, and Pandas is built for handling this format.
- Widely used: It's a common tool in both academia and industry.
- Relatively easy to use: Pandas provides a straightforward way to explore and manipulate data.
- Has useful built-ins: Filtering, grouping, summarizing, and plotting data often take just a few lines of code.
- Plays well with others: Pandas integrates smoothly with visualization and statistical tools.
- Uses similar concepts to R's [tidyverse](https://www.tidyverse.org/), which many of you have experience with from your previous coursework

We'll also use a bit of seaborn for visualization, as it can help with certain types of plots or when data is in a certain format.

### Practical Examples

In this tutorial, we'll learn exploratory data analysis (EDA) by working through real research questions with real datasets. Instead of covering every Pandas function upfront, we'll introduce tools as we need them.

We'll use datasets from the CORGIS collection, which offers accessible real-world data. Our examples include:

- State Demographics: Analyzing population patterns and economic indicators across the U.S.
- Cancer Statistics: Examining cancer rates and their potential links to demographics.
- Vaccination Impact: Exploring historical disease data to see how vaccines have shaped public health.

These examples will help you learn Pandas in context, and use techniques that are similar to those you could use when getting started with a real research project.

### Key Pandas Functionality

After working through the examples, we'll summarize the essential Pandas operations, including:

- Loading and examining data
- Selecting, filtering, and sorting
- Grouping and aggregating
- Basic visualization
- Merging and joining datasets

By focusing on core functions, you'll gain practical skills without getting lost in the details. Let's dive in!

## Import Needed Libraries

The first thing we need to do is to import some libraries. We are only using numpy for a couple things in this tutorial: specifying data types the `NaN` value.

```{python}
import numpy as np
import pandas as pd
import seaborn as sns

pd.options.mode.copy_on_write = "warn"

pd.set_option("display.max_rows", 10)
```

_Note: While we don't need to import it, you will also need to have SciPy installed to run the clustered heatmaps._

## State Demographics Data

To start, we are going to look at the state demographics data from [The Collection of Really Great, Interesting, Situated Datasets (CORGIS)](https://corgis-edu.github.io/corgis/). CORGIS is a collection of datasets that have been cleaned and otherwise made ready for teaching/learning purposes. It was created in part by [Dr. Cory Bart](https://www.cis.udel.edu/people/faculty/austin-cory-bart/), who is a professor at UD.

This [state demographics data](https://corgis-edu.github.io/corgis/csv/state_demographics/) includes a lot of info about states that we will be able to use to try and explain some of the cancer trends that we see in the next section. To give you an idea of the kinds of data we'll be working with, here are some of the data categories:

- Population
- Age
- Ethnicities
- Housing
- Income
- Employment

### Importing Data

The first thing we need to do is import the data. To do that, we can use the [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function:

```{python}
state_demographics = pd.read_csv("./_data/state_demographics.csv")
```

### Data Overview

After importing data, it's always a good idea to check out its basic info, things like shape, column names, basic summary statistics, etc.

To get the number of rows and columns in a data frame, we use [shape()][https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html]:

```{python}
state_demographics.shape
```

That's not too much data, so let's look at the table directly. We will use the [head()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html#pandas.DataFrame.head) function to take only the first few rows:

```{python}
state_demographics.head()
```

To get summary statistics of the numeric rows of a data frame, we use [describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html#pandas.DataFrame.describe). This can help you to get an overall sense of your data.

```{python}
state_demographics.describe()
```

Do you notice how the values in the table have a lot of precision, and are all using scientific notation? Sometimes this is what we want, but we really don't need all that here, and it's only serving to clutter up the view. We can control the precision of the numbers in the table using the `.style.format()` pattern. Since we have some large numbers in there, let's add a thousands place separator as well.

```{python}
state_demographics.describe().style.format(
    # Set precision of numbers to 2 decimal places
    precision=2,
    # Use a comma to separate out the thousands in the big numbers
    thousands=",",
)
```

One more thing we can do to clean up this view is to sort the variables by their name.

```{python}
(
    state_demographics.describe()
    # Sort the data frame by the row index (a.k.a., the row names)
    .sort_index(axis="columns")
    .style.format(precision=2, thousands=",")
)
```

That's a pretty nice looking summary now!

_Note: Do you see how we put that little pipeline in parentheses? This is so that we can separate operations on their own line, which can improve readability, and also let us add comments as needed._

I want to make something clear. We haven't done anything to change the data frame that we imported.

```{python}
state_demographics.head()
```

As you see, it's the same data frame we started with. All the functions we have used so far have returned new data frames. You will see this pattern a lot--many Pandas functions return new data rather modifying existing data.

::: {#tip-07-why-summarize-data .callout-tip title="Stop & Think" collapse="false"}
Why is it a good idea to look at a summary of data that you imported?
:::

### Filtering Columns

This table has a lot of different kinds of data about state demographics. The nice thing is that each of the different categories is used as a prefix to the column name, e.g., data about income is prefixed with `Income`, data about population is prefixed with `Population`, and so on. We can leverage this labeling scheme to chop our data frame into more manageable chunks.

We can use the [filter()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html#pandas.DataFrame.filter) function to filter columns in a bunch of different ways. For now we will use the regular expression (regex) argument to specify that we want to match at the start of the column name. For example, getting all the data columns about ethnicities:

```{python}
(
    state_demographics
    # Use set_index to convert the state column to the row names
    .set_index("State")
    # Use filter() to keep columns matching the given pattern
    .filter(regex=r"^Ethnicities")
    # Only display the first 5 rows
    .head()
)
```

_Note: If you need a refresher on regular expressions check out @sec-regex. _

We needed to use the `set_index()` here so that the state name would still be present on the resulting data frames.

This works for other categories as well. Here it is for `Income`:

```{python}
state_demographics.set_index("State").filter(regex=r"^Income").head()
```

We can pass these filtered tables to `describe()` and other functions as well:

```{python}
state_demographics.set_index("State").filter(regex=r"^Income").describe()
```

This is another way that we can start to get a feel for our data.

::: {#tip-07-why-filter-columns .callout-tip title="Stop & Think" collapse="false"}
Why might filtering columns by category prefixes (like "Population" or "Income") be useful during exploratory data analysis?
:::

### Exploring Your Data

Now that we have a basic idea of what our data looks like, we can start to explore it a bit more. The best place to start is to actually look at the data. Pandas gives you the ability to create basic charts without having to use a 3rd-party package. As long as you don't want anything too complex, it will be fine to start with.

#### Basic Population Plots

Let's start by plotting some basic state population info. We can use Panda's [plot()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html#pandas.DataFrame.plot) function for this:

```{python}
(
    state_demographics
    # Take only the columns that start with Population.201X, where X is some digit
    # E.g., Population.2010, or Population.2014.
    .filter(regex=r"^Population\.201\d")
    # Draw a bar plot
    .plot(kind="bar")
)
```

That's not bad, but the axes are a bit weird. Let's adjust them. The simplest way to do that is to be more specific about which data we will need in our chart. Then, we can explicitly set the x and y axes.

```{python}
# Make a list of the columns that we want to keep
columns = ["State", "Population.2010 Population", "Population.2014 Population"]

# Use the "bracket notation" to select only those columns specified in the list we just
# created.
plot_data = (
    state_demographics[columns]
    # Rename the population columns to something shorter.
    # It will make the chart legends look nicer.
    .rename(
        # We want to rename columns.  The keys of this dictionary are the old column
        # names, and the values are the new column names.
        columns={
            "Population.2010 Population": "2010 Population",
            "Population.2014 Population": "2014 Population",
        }
    )
)

# Plot the data subset
plot_data.plot(
    # Make it a bar chart
    kind="bar",
    # Put "State" on the x-axis
    x="State",
    # And put both population columns on the y-axis
    y=["2010 Population", "2014 Population"],
    figsize=(8, 3),
)
```

Not bad! The plotting function was smart enough to put both population data series on the chart, and to include a nice legend so that we can tell them apart.

At least we have the State names on the x-axis now, but they are pretty smooshed together. There are a bunch of ways we could fix it:

- Shrink the label size
- Adjust the chart proportions
- Use a horizontal bar chart (and adjust the proportions)

Shrinking the label size:

```{python}
plot_data.plot(
    kind="bar",
    x="State",
    y=["2010 Population", "2014 Population"],
    fontsize=6,
)
```

That works, but now the labels are tiny, and it shrunk both the x and y axis labels. You can shrink just the x-axis tick labels, but to do so you need to "eject" out of the pandas API and drop down into [matplotlib](https://matplotlib.org/) code.

```{python}
import matplotlib.pyplot as plt

# Create your plot
ax = plot_data.plot(
    kind="bar",
    x="State",
    y=["2010 Population", "2014 Population"],
)

# Adjust only x-axis tick labels
# 8 is the fontsize, adjust it as needed
ax.tick_params(axis="x", labelsize=8)

# Show the chart
plt.show()
```

_Don't worry too much about the details of this. I just wanted to show you that the plots returned by Pandas are really matplotlib objects, and can be interacted with in the usual way when required._

Let's adjust the figure proportions next:

```{python}
plot_data.plot(
    kind="bar",
    x="State",
    y=["2010 Population", "2014 Population"],
    # Set the width to 8 units, and the height to 3
    figsize=(8, 3),
    # Set the font size of all tick labels to 9
    fontsize=9
)
```

That's fairly readable. Above, I mentioned how long data and plots tend to fit better on the screen both in your reports and when actively doing the analysis. So let's switch to a horizontal bar chart. That way, we can give the states a little more room on the plot.

```{python}
plot_data.plot(
    # Draw a horizontal bar chart. Note the `h` at the end of `barh`.
    kind="barh",
    x="State",
    y=["2010 Population", "2014 Population"],
    figsize=(5, 8),
)
```

_Note: You might find it a little weird that we still specify `State` as the x values and `Population` as the `y` values, even thought the chart shows the on the opposite axis. Just roll with it :)_

That's looking pretty good now! You might think it is a little bit weird for the state names to be going in reverse alphabetical order as you go down the page. I suppose this is chosen because in "normal" math plots, the origin is 0, and the values "increase" as you move away from the origin. However, it just feels weird for it to do this when the y-axis is categorical rather than continuous. So let's reverse it.

```{python}
(
    plot_data
    # Reverse sort the rows based on the State column
    .sort_values("State", ascending=False).plot(
        kind="barh",
        x="State",
        y=["2010 Population", "2014 Population"],
        figsize=(5, 8),
    )
)
```

One last thing. Let's add an axis label to the x-axis. I know there is a legend there, but I think it is still good practice to label all axes in a plot.

```{python}
(
    plot_data
    # Reverse sort the rows based on the State column
    .sort_values("State", ascending=False).plot(
        kind="barh",
        x="State",
        y=["2010 Population", "2014 Population"],
        figsize=(5, 8),
        xlabel="Population"
    )
)
```

That's what I'm talking about! As you can see, with just a few lines of code, you can make totally reasonable looking plots.

Depending on your use case, you might want to sort the data so that the bars are always decreasing. That way, it's easier for the viewer to look at overall trends, rather than being able to quickly pick out specific states.

To do this we will sort by population. However, we are plotting two different years on the chart, so we need to decide which way to sort it. Reasonable options might be:

- Pick one of the years and sort by that one
- Sort by the mean
- Sort by the max

Any of them could work depending on your situation, but let's keep it simple and sort by the 2014 population:

```{python}
(
    plot_data
    # Reverse sort the rows based on the State column
    .sort_values("2014 Population").plot(
        kind="barh",
        x="State",
        y=["2010 Population", "2014 Population"],
        figsize=(5, 8),
        xlabel="Population",
    )
)
```

::: {#tip-07-why-nice-plots .callout-tip title="Stop & Think" collapse="false"}
Why might it be important to have clean, professional looking data visualizations, even during the exploratory data analysis phase of your project?
:::

#### Percent Population Change

Notice any trends? One thing that we can kind of see is that there are some states that look like they had bigger population changes than other states. One of our data columns already tracks this: `Population.Population Percent Change`. Let's take a look at the basic plot. We will use a lot of the same options as we did in the last one.

```{python}
state_demographics.plot(
    kind="barh",
    x="State",
    y="Population.Population Percent Change",
    figsize=(5, 8),
    xlabel="Population",
)
```

This plot is pretty good, but we can do better with a bit more effort. Here's a couple things that will improve it:

- Remove the legend since there is only one series to plot
- Adjust the bar color so that population increase blue and decrease is orange
- Sort states from most increase to most decrease

```{python}
plot_data = state_demographics[
    ["State", "Population.Population Percent Change"]
].sort_values("Population.Population Percent Change")

# Create list with colors based on positive/negative values
colors = [
    # Negative values will be orange, positive values will be blue
    "tab:orange" if x < 0 else "tab:blue"
    # Do this calculation for each value in the series that we want to plot.
    for x in plot_data["Population.Population Percent Change"]
]


plot_data.plot(
    kind="barh",
    x="State",
    y="Population.Population Percent Change",
    figsize=(5, 8),
    xlabel="Population Change (%)",
    # Specify the list of colors to use
    color=colors,
    # Remove the legend
    legend=False,
)
# Put a gray line at x=0 to help guide the viewer's attention.
plt.axvline(x=0, color="#666666")
```

To adjust the colors, we had to create a list of colors the same length as the data that we wanted to plot. In this way, each row will be given its correct color.

_Note: `tab:blue` and `tab:orange` are built-in colors in [matplotlib](https://matplotlib.org/stable/gallery/color/color_cycle_default.html#sphx-glr-gallery-color-color-cycle-default-py)._

_Note: You can specify these labeled arguments in any order._

That's really not a bad plot! I wonder if the states with the biggest population changes were already some of the most populous states?

```{python}
state_demographics.plot(
    kind="scatter",
    x="Population.2010 Population",
    y="Population.Population Percent Change",
)
```

Doesn't look like a super strong trend there, but let's log the x scale as maybe orders of magnitude matter here.

```{python}
state_demographics.plot(
    kind="scatter",
    x="Population.2010 Population",
    y="Population.Population Percent Change",
    # Log the x axis
    logx=True,
)
```

Nope, nothing there.

::: {#tip-07-log-scale-purpose .callout-tip title="Stop & Think" collapse="false"}
When might you use a logarithmic scale on either axis of a plot, and what insights can this reveal that a linear scale might not?
:::

Do you think any of the state demographic data is correlated with the percent population change? To try and answer this question, we can calculate all the correlation values for the columns in the state demographics data frame.

```{python}
# Get the correlation between columns in the data frame
correlation_matrix = state_demographics.corr(
    # Restrict the calculation to only numeric columns
    numeric_only=True
)
```

::: {#tip-07-what-correlation-tells-us .callout-tip title="Stop & Think" collapse="false"}
What can correlation values tell us during exploratory data analysis, and what are their limitations?
:::

Now that we have the correlation values, let's take a look at them. First, we need to make a data frame with the data in a nice format that makes it easy to plot the data.

```{python}
# Display the correlations values between the percent population change and the other
# variables.
plot_data = (
    # Select only the percent change column
    correlation_matrix[["Population.Population Percent Change"]]
    # Then sort the rows based on their correlation to the percent population change
    .sort_values("Population.Population Percent Change")
    # Remove the Population.Population Percent Change row from the results, since
    # we don't care about the "self-correlation"
    .drop("Population.Population Percent Change")
    # Convert the row names to a column called "Variable"
    .reset_index(names="Variable")
    # Rename the 2nd column for a nicer looking chart
    .rename(columns={"Population.Population Percent Change": "Correlation"})
)
plot_data
```

_Note: earlier we used `drop()` to remove columns. Now you see that it can also be used to drop rows, depending on the arguments used. You will find Pandas has a lot of functions like this._

Next, we can create a list of colors for the bars of the plot. We want to color variables with a positive correlation to the percent population change blue, and a negative correlation orange.

```{python}
# Create list with colors based on positive/negative values
colors = [
    # Negative values will be orange, positive values will be blue
    "tab:orange" if x < 0 else "tab:blue"
    # Do this calculation for each correlation value in the series.
    for x in plot_data["Correlation"]
]
```

Finally, let's make the plot. The code will be very similar to the previous plots we have made.

```{python}
plot_data.plot(
    kind="barh",
    x="Variable",
    y="Correlation",
    figsize=(5, 10),
    xlabel="Correlation with Percent Population Change",
    legend=False,
    color=colors,
)
plt.axvline(x=0, color="#666666")
```

That's nice! Do you see any interesting trends?

- Age data
  - Increasing proportion of young population is strongly correlated with positive population change
  - Increasing proportion of elderly population is highly correlated with decreasing population.
  - Makes sense...
- Ethnicity data
  - These are all positively correlated with population change: "Hispanic or Latino", "Languages other than English at home", "Foreign Born".
  - In contrast, "White Alone" and "White Alone, not Hispanic or Latino" are negatively correlated with population change.
  - Is this suggesting immigration is driving some of the population growth?
- Education & Income data
  - "Households with a computer", "Households with internet", "Bachelor's Degree or Higher" are positively correlated with population change
  - "Persons Below Poverty Level" is negatively correlated with population change
  - Maybe it's suggesting people are moving to more educated or prosperous areas?

These are all avenues that you might want to take a look at if this data was important to your research.

As you can see, there are a lot variables there and most of them have pretty weak correlation. This is a good time to show you how to filter rows based on some criteria of the data. Let's filter out any rows that have correlation values between -0.15 and 0.15. To do this, we can use [query()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query):

```{python}
plot_data_with_some_correlation = plot_data.query(
    "Correlation < -0.15 or Correlation > 0.15"
)
```

The `query()` function is very cool and highly flexible and dynamic. We will see a few more examples of it later in the tutorial. For now, just know that you can access columns of your data frame and use them to filter rows in a natural looking way.

Once we have the filtered data, we can regenerate the color list, and plot the data.

```{python}
# We need to redo the colors again.
colors = [
    "tab:orange" if x < 0 else "tab:blue"
    for x in plot_data_with_some_correlation["Correlation"]
]

plot_data_with_some_correlation.plot(
    kind="barh",
    x="Variable",
    y="Correlation",
    figsize=(5, 10),
    xlabel="Correlation with Percent Population Change",
    legend=False,
    color=colors,
)
plt.axvline(x=0, color="#666666")
```

It's generally not a good idea to take correlation values at face value: it doesn't measure all types of dependencies and it can be tempting to assign causation to things that have high correlation. So for correlation, it's always a good idea to look at your data whenever possible. Let's do that now with some of the most highly correlated or anti-correlated variables. We are going to use [seaborn](https://seaborn.pydata.org/) for this, as it makes it super simple to compare multiple variables in a single plot. We can even put regression lines with confidence intervals on the plots by setting the `kind="reg"` argument!

Here is a plot containing some of the most highly correlated variables:

```{python}
columns = [
    "Population.Population Percent Change",
    "Age.Percent Under 5 Years",
    "Housing.Households with a computer",
    "Ethnicities.Hispanic or Latino",
]
sns.pairplot(
    state_demographics[columns],
    kind="reg",
    x_vars=columns[0],
    y_vars=columns[1:],
    height=4,
)
```

And one with the most highly anti-correlated variables:

```{python}
columns = [
    "Population.Population Percent Change",
    "Ethnicities.White Alone, not Hispanic or Latino",
    "Age.Percent 65 and Older",
    "Miscellaneous.Living in Same House +1 Years",
]
sns.pairplot(
    state_demographics[columns],
    kind="reg",
    x_vars=columns[0],
    y_vars=columns[1:],
    height=4,
)
```

Neat, at least we have discovered a couple of variables that we may want to look into. One definite potential issue I see here is that the states that have dropped in population seem to be off on their own in all these plots. It would probably be a good idea to see if we are violating any major assumptions of the basic linear model with them, or at least see if they (or any other points) have too much leverage and are misleading us. But that is a topic for a different course!

Before we move on, let's do one more thing with correlation as it is so common: a correlation heatmap! We can use seaborn's [clustermap()](https://seaborn.pydata.org/generated/seaborn.clustermap.html) for this:

```{python}
clustermap_plot = sns.clustermap(
    correlation_matrix,
    # Specify the complete linkage for calculating clusters
    method="complete",
    # The relative space the dendrograms will occupy
    dendrogram_ratio=0.05,
    # Use the "icefire" diverging palette
    cmap="icefire",
    # Make sure the min color value occurs at -1
    vmin=-1,
    # Make sure the max color value occurs at 1
    vmax=1,
    figsize=(12, 12),
    # Remove the x-axis tick labels
    xticklabels=True,
    # Remove the y-axis tick labels
    yticklabels=True,
    # Set the options for the color palette legend
    cbar_kws={
        "label": "Correlation",  # Set the label for the color palette legend
        "location": "bottom",  # Set the location of the color palette legend
    },
    # Set the location for the color palette legend
    # This is for top left
    # cbar_pos=(
    #     0.03,  # Distance from the left
    #     0.92,  # Distance from the bottom
    #     0.10,  # Width
    #     0.05,  # Height
    # ),
    cbar_pos=(
        0.65,  # Distance from the left
        0.20,  # Distance from the bottom
        0.25,  # Width
        0.14,  # Height
    ),
)
```

Cool! There are a couple things to note about this:

- One important consideration is setting the minimum and maximum values of your color palette. While you don't always need to adjust these parameters (and sometimes only need to set one or two of them), being aware of this option is important. Making these adjustments ensures the most informative part of your color palette corresponds to the most relevant range in your data.
- For fine-tuning your colorbar, check out the `cbar_kws` parameter, which passes arguments directly to matplotlib's [colorbar()](https://matplotlib.org/stable/api/_as_gen/matplotlib.figure.Figure.colorbar.html) method. This pattern of documentation referral is something you'll encounter frequently in the Python ecosystem. Libraries will often direct you to another component's documentation for parameter details, especially when they're simply passing those arguments through to underlying functions.
- Don't forget about customizing your clustering approach! The linkage type and distance metric can impact your hierarchical clustering results. The [SciPy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) provides comprehensive details on these options, allowing you to select methods that best represent the relationships in your data.

::: {#tip-07-good-color-palette .callout-tip title="Stop & Think" collapse="false"}
Why is it important to ensure that your color palettes represent the correct data? For example,

- How would it change your interpretation if the center of the palette (the black part) was on 0.2 rather than zero?
- How would it change your interpretation if the brightest blue was -0.2 but the brightest orange was 1.0?

:::

There is a lot to unpack with this figure, but the most obvious thing that I see is that blob of bright orange in the bottom right. If you look at the data in those columns, you will see that they are numbers with real magnitude that will be pretty highly influenced by the number of people in the state. A lot of the other columns are not like this. Wouldn't it be interesting to take those counting-style numbers and normalize them by the state population? E.g., something like manufacturers shipments per 10k people. Let's do that now.

```{python}
# Make a copy of the state demographics data
normalized_state_demographics = state_demographics.copy()

# These are the columns that we want to divide by the population
columns = [
    "Employment.Firms.Men-Owned",
    "Employment.Firms.Minority-Owned",
    "Employment.Firms.Nonminority-Owned",
    "Employment.Firms.Nonveteran-Owned",
    "Employment.Firms.Total",
    "Employment.Firms.Veteran-Owned",
    "Employment.Firms.Women-Owned",
    "Employment.Nonemployer Establishments",
    "Housing.Households",
    "Housing.Housing Units",
    "Miscellaneous.Manufacturers Shipments",
    "Miscellaneous.Veterans",
    "Sales.Accommodation and Food Services Sales",
    "Sales.Retail Sales",
]

# Loop through each of the columns
for column in columns:
    # Normalize the data: X / Population * 10,000 people
    normalized_data = (
        normalized_state_demographics[column]
        / state_demographics["Population.2010 Population"]
        * 10_000
    )
    # Replace the original column with the normalized column
    normalized_state_demographics[column] = normalized_data
```

Now that we have normalized data, we can generate the correlation matrix and plot the heatmap.

```{python}
# Generate a correlation matrix of the numeric columns
correlation_matrix = normalized_state_demographics.corr(numeric_only=True)

# Draw the clustered heatmap
clustermap_plot = sns.clustermap(
    correlation_matrix,
    # Specify the complete linkage for calculating clusters
    method="complete",
    # The relative space the dendrograms will occupy
    dendrogram_ratio=0.05,
    # Use the "icefire" diverging palette
    cmap="icefire",
    # Make sure the min color value occurs at -1
    vmin=-1,
    # Make sure the max color value occurs at 1
    vmax=1,
    figsize=(12, 12),
    # Remove the x-axis tick labels
    xticklabels=True,
    # Remove the y-axis tick labels
    yticklabels=True,
    # Set the options for the color palette legend
    cbar_kws={
        "label": "Correlation",  # Set the label for the color palette legend
        "location": "bottom",  # Set the location of the color palette legend
    },
    # Set the location for the color palette legend
    cbar_pos=(
        0.65,  # Distance from the left
        0.20,  # Distance from the bottom
        0.25,  # Width
        0.14,  # Height
    ),
)

```

After "normalizing out" the effect of population on a bunch of the variables, we can see some trends that were a bit masked before. For example, we can now see some correlation between education, wealth, and income, as well as some potentially interesting trends around ethnicity and age.

::: {#tip-07-why-correlation-heatmap .callout-tip title="Stop & Think" collapse="false"}
What insights can a correlation heatmap with clustering provide that a simple correlation table cannot?
:::

### State Demographics Wrap-Up

We went over a lot of material in this section! I hope it gave you a sense of how exploratory data analysis can go using Pandas and Seaborn: you begin by exploring the data to get a sense of it, identify patterns and trends, and then dive deeper into those patterns to better understand the system you're analyzing.

## Cancer data

Now that you have a basic understanding of using Pandas for EDA, let's take a look at some public health data.

We're going to look at the [cancer data](https://corgis-edu.github.io/corgis/csv/cancer/) from CORGIS. This dataset contains information about cancer deaths between 2007 and 2013 in each state. Specifically, these are deaths from breast, lung, and colorectal cancer. In addition to the total death rate, the rates have also been broken down by age, race, and sex.

### Basic Info

To start, we need to read the CSV file with the cancer data.

```{python}
cancer = pd.read_csv("./_data/cancer.csv")
cancer.head()
```

Like before, we will use `describe()` to get a basic overview of the numeric data columns.

```{python}
cancer.describe().style.format(precision=1)
```

### Effect of Population

In the state demographics data, we saw a strong dependency between some of the variables and state population. We would also expect there to be a pretty strong dependency between the total number of cancer deaths and the total state population. Let's see if that is the case:

```{python}
cancer.plot(
    kind="scatter",
    x="Total.Population",
    y="Total.Number",
    loglog=True,
)
```

Just to make it super clear, let's do the same plot, but this time use the rate of cancer deaths per 100k people rather than the raw totals. (I bet you can guess how it will look!)

```{python}
cancer.plot(
    kind="scatter",
    x="Total.Population",
    y="Total.Rate",
    logx=True,
)
```

Because of this, we will use the rates per 100k people rather than total numbers for this section.

### Comparing States

Let's see if there are any high-level differences between individual states and rates of cancer deaths.

```{python}
(
    cancer
    # Sort the values by the rate of cancer deaths
    .sort_values("Total.Rate").plot(
        # Make a horizontal bar chart
        kind="barh",
        x="State",
        y="Total.Rate",
        # Adjust the figure size so the labels print nicely
        figsize=(5, 8),
        # Give an informative x-axis label
        xlabel="Cancer Rate (per 100k people)",
        # Don't bother with the legend as we only have one data series to plot
        legend=False,
    )
)
```

There is about a 2.5 times difference between the state with the highest rate of cancer deaths (West Virginia) as compared to the state with the lowest (Utah).

Let's make a boxplot to see the spread of the data.

```{python}
cancer.plot(kind="box", y="Total.Rate")
```

Cool, so we see some variation in the rate of cancer deaths across states. Let's try and find out if there are any variables in the state demographics data that are correlated with death rates for different types of cancer.

### Cancer Deaths and Demographics

This dataset has data for three types of cancer, breast cancer, colorectal cancer, and lung cancer, so we will want to pull out those columns.

```{python}
cancer_death_rates = cancer[
    ["State", "Types.Breast.Total", "Types.Colorectal.Total", "Types.Lung.Total"]
]
cancer_death_rates.head()
```

We want to include the normalized state demographic data in with the cancer data, but we don't want all the columns. Earlier, we saw that we can use `filter()` to select columns using regular expressions. We will do that again here to select only the categories of variables that we are interested in.

```{python}
# Create a filtered version of normalized_state_demographics data frame containing:
# - State column
# - Columns starting with "Age"
# - Columns starting with "Education"
# - Columns starting with "Ethnicities"
# - Columns starting with "Housing"
# - Columns starting with "Income"
# This uses regex patterns with filter() to select columns,
# then combines them using pd.concat()
filtered_normalized_state_demographics = pd.concat(
    [
        normalized_state_demographics.filter(["State"]),
        normalized_state_demographics.filter(regex=r"^Age"),
        normalized_state_demographics.filter(regex=r"^Education"),
        normalized_state_demographics.filter(regex=r"^Ethnicities"),
        normalized_state_demographics.filter(regex=r"^Housing"),
        normalized_state_demographics.filter(regex=r"^Income"),
    ],
    axis="columns",
)
filtered_normalized_state_demographics.head()
```

Now, we can merge the data:

```{python}
cancer_demographics = cancer_death_rates.merge(
    filtered_normalized_state_demographics, on="State", how="inner"
)
cancer_demographics.head()
```

Let's do another correlation matrix:

```{python}
cancer_demographics_full_correlation_matrix = cancer_demographics.corr(
    numeric_only=True
)
cancer_demographics_full_correlation_matrix
```

This will give every variable against all other variables, but in this case we don't want to plot all that. We just want to see the correlation of the state demographic data to the cancer death data, and not the state demographic data with itself again.

So, let's filter out the rows and columns that we don't need.

```{python}
cancer_columns = ["Types.Breast.Total", "Types.Colorectal.Total", "Types.Lung.Total"]
cancer_demographics_correlation_matrix = cancer_demographics_full_correlation_matrix[
    cancer_columns
].drop(cancer_columns, axis="rows")
cancer_demographics_correlation_matrix
```

And now we can generate another heatmap.

```{python}
sns.clustermap(
    cancer_demographics_correlation_matrix,
    # Specify the complete linkage for calculating clusters
    method="complete",
    # The relative space the dendrograms will occupy
    dendrogram_ratio=0.15,
    # Use the "icefire" diverging palette
    cmap="icefire",
    # Make sure the min color value occurs at -1
    vmin=-1,
    # Make sure the max color value occurs at 1
    vmax=1,
    # figsize=(12, 12),
    # Remove the x-axis tick labels
    xticklabels=True,
    # Remove the y-axis tick labels
    yticklabels=True,
    # Set the options for the color palette legend
    cbar_kws={
        "label": "Correlation",  # Set the label for the color palette legend
        "location": "bottom",  # Set the location of the color palette legend
    },
    # Set the location for the color palette legend
    # This is for top left
    cbar_pos=(
        0.03,  # Distance from the left
        0.92,  # Distance from the bottom
        0.10,  # Width
        0.05,  # Height
    ),
)
```

We can definitely see some patterns emerging. The groups most highly correlated with cancer deaths were people 65 and older, followed by people below the poverty line. In contrast, groups that were least correlated with cancer deaths included those who lived in households with a computer and fewer people per household.

There are also some interesting patterns related to ethnicity. For example, there is almost no correlation between ethnicity and colorectal or lung cancer death for Pacific Islanders, but a negative correlation for breast cancer. Meanwhile, there is a higher correlation between breast cancer deaths and ethnicity for Black people than for colorectal or lung cancer, which are both close to zero.

These patterns bring up some interesting questions. Are Pacific Islanders less likely to develop breast cancer than Black people? Or do the two groups tend to develop different types of breast cancer? Or are there social determinants or biases in healthcare that make breast cancer more deadly for Black people?

Exploratory data analysis, as you've seen, will show trends, but it is up to scientists and other domain experts to interpret the data and to determine causes.

### Cancer Data Wrap-Up

In this section we learned some tricks about how to combine multiple datasets, and how to look for interesting data trends by including more metadata into our analysis.

You may have noticed that there are a lot more variables in the cancer dataset. You could definitely imagine doing a lot more with this data!

## Public Health Data

Let's check out some public health data next. This data is a bit different in that it has data for the states over many years, and it includes multiple diseases. This makes it a neat resource to learn a few more Pandas tricks!

### Basics

We start by importing the data.

```{python}
disease = pd.read_csv("./_data/health.csv")
disease.head()
```

The first thing I want to do is clean it up a little bit. I like columns to use title case and to avoid abbreviations that aren't in common usage. Additionally, the other datasets we looked at didn't have entries in all caps, so I would like to fix that as well.

```{python}
disease = (
    # Start with the disease DataFrame
    disease
    # Rename the columns to more readable format with capital letters
    .rename(
        columns={
            # Change 'disease' to 'Disease'
            "disease": "Disease",
            # Change 'increase' to 'Increase'
            "increase": "Increase",
            # Change 'loc' to 'State'
            "loc": "State",
            # Change 'number' to 'Cases'
            "number": "Cases",
            # Change 'population' as 'Population'
            "population": "Population",
            # Change 'year' to 'Year'
            "year": "Year",
        }
    )
    # Use assign() to create or modify columns without modifying the original
    # DataFrame
    .assign(
        # Convert the Disease column text to title case
        # (first letter of each word capitalized)
        Disease=lambda df: df["Disease"].str.title(),
        # Convert the State column text to title case
        # (first letter of each word capitalized)
        State=lambda df: df["State"].str.title(),
    )
)
disease
```

We use the method chaining style again: each method returns a DataFrame that the next method operates on.

That `lambda` usage in `assign()` might be a bit obscure. Let's break it down.

```python
.assign(
    Disease=lambda df: df["Disease"].str.title()
)
```

- `Disease` will be the column that holds the result of the evaluation of the `lambda` function
- `lambda df:`
  - This creates a small anonymous function that takes a `DataFrame` named `df` as input.
  - Which data frame? Well, it's the data frame that we are currently working on. If that is too mind-bending, just roll with it for now.
- `df["Disease"]` selects just the Disease column from the data frame `df`
- `.str.title()` is a Pandas string method that converts text to "Title Case" (capitalizes the first letter of each word).

So when you see `Disease=lambda df: df["Disease"].str.title()` in the `assign()` method, it means "create a new Disease column by taking the values from the existing Disease column and converting each value to title case."

_Note: Pandas has a special way of working with string data. To learn more, check out the [working with text data](https://pandas.pydata.org/docs/user_guide/text.html) docs._

### Disease Rates

Similar to the other datasets, we will want to look at disease rates and not raw numbers since we don't want the state population to influence our results. This time, let's modify the existing data frame directly. We could have used `assign()` again, but I want to give you a little variety, in case you see something similar in the wild.

```{python}
disease["CasesPer100k"] = disease["Cases"] / disease["Population"] * 100_000
disease.head()
```

Some of these columns are no longer needed, so let's drop them.

```{python}
disease = disease.drop(columns=["Increase", "Population"])
disease
```

### Grouping Data

Now that we have cleaned things up a little, let's summarize the data.

```{python}
disease.describe()
```

Huh, that really didn't give us useful information did it? The way this data is structured means that multiple columns are needed to identify unique observations. In this case a single observation is uniquely identified by a combination of state-year-disease. When we use describe, or other summarizing methods which we will see in a bit, we will first need to group the observations by subsets of the columns that uniquely identify them.

For example, to see averages for diseases per year across all states, we would group by `Year` and `Disease`:

```{python}
disease.groupby(by=["Year", "Disease"]).describe()
```

Or, to see averages for diseases per state across all years, we would group by `State` and `Disease`:

```{python}
disease.groupby(by=["State", "Disease"]).describe()
```

And so on, with any combination required.

_Note: We didn't show it here, but you can also group by single columns, too._

You may have noticed that there is too much data to really get a sense of it in the interactive Quarto prompt. The quickest way around this is to use the [to_clipboard()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html#pandas.DataFrame.to_clipboard) method, and then paste the data into your favorite spreadsheet program and you can get a better look.

```{python}
disease.groupby(by=["State", "Disease"]).describe().to_clipboard()
```

The `to_clipboard()` function can be very handy!

### Plotting the Data

Having gotten ourselves acquainted with the data, we should make some plots! Let's see how common the diseases were across all states and all years for which we have data.

```{python}
(
    # Select only the columns we need for this chart
    disease[["Disease", "Cases"]]
    # Group the data by disease
    .groupby(by="Disease")
    # Take the sum of the cases
    .agg({"Cases": "sum"})
    # The groupby() operation will make Disease the row names,
    # but we want them as a column.
    .reset_index("Disease")
    # Sort the rows by number of cases
    .sort_values("Cases", ascending=False)
    # Plot the data
    .plot(
        # Do a bar chart
        kind="bar",
        # The x-axis is disease name
        x="Disease",
        # The y-axis is the number of cases
        y="Cases",
        # We want to log transform the y-axis
        logy=True,
        # Drop the legend since we only have a single series of data to plot
        legend=False,
        # Give the y-axis a label
        ylabel="Cases",
    )
)
```

Measles is by far the most common disease in the dataset. Let's see if the prevalence of different diseases changes over time.

```{python}
plot_data = (
    disease[["Disease", "Year", "Cases"]]
    .groupby(by=["Disease", "Year"])
    .agg({"Cases": "sum"})
    .reset_index(["Disease", "Year"])
)

plot_data.plot(
    kind="line",
    x="Year",
    y="Cases",
)
```

Oops! That's not what we want. Pandas uses [matplotlib](https://matplotlib.org/) under the hood, and by default, it works more naturally with a slightly different data format. For reference, here is the data we just tried to plot:

```{python}
plot_data
```

Let's adjust the format to one that works better for the plotting function:

```{python}
plot_data = (
    # Select the columns we need
    disease[["Disease", "Year", "Cases"]]
    # Group by disease-year pairs
    .groupby(by=["Disease", "Year"])
    # Get the sum of the cases across the grouping variables
    .agg({"Cases": "sum"})
    # Convert the row names back to columns
    .reset_index(["Disease", "Year"])
    # Pivot! (See below for an explanation)
    .pivot(
        # Set the Year column to be the row index of the resulting table
        index="Year",
        # Take the new columns from the unique values in the Disease column
        columns="Disease",
        # Take the values for the new columns from the data in the Cases column
        values="Cases",
    )
    # Pivoting sets Year as the row names, so convert it back to a column
    .reset_index("Year")
)

# Don't worry about this--it's just to make the data print in a nicer way.
plot_data.columns.name = None

plot_data
```

As you can see, we've taken the original data and made it wider. _(If you're familiar with R's [Tidyverse](https://www.tidyverse.org/), you may recognize this operation as taking [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) and turning into so-called "messy" data.)_

The only new thing here is the [pivot()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html#pandas.DataFrame.pivot) function. Pivoting data can be a little tricky to get used to. The basic operation is taking unique values from a column and using each of those unique values to create a new column. (It's actually more flexible than that, but that's the general idea.)

In this case, we want to make new columns from the unique values in the `Disease` column (e.g., Measles, Mumps, and Rubella). The values of those new columns will be taken from the `Cases` column.

Now we can plot it!

```{python}
disease_by_year_plot = plot_data.plot(
    # We want a line chart
    kind="line",
    # Put Year on the x-axis
    # (The y-axis is set implicitly as the other columns)
    x="Year",
    # Log transform the y-axis
    logy=True,
    # Give the y-axis a label
    ylabel="Cases",
)
```

What do you notice about this data? Let me list out some of the questions/observations that might come to mind when first looking at this plot:

- Why do many of the diseases have sharp declines and eventually drop out of the plot after a while?
- Why is there a gap in the Pertussis (whooping cough) data?
- Why is Pertussis the only one that has been on a steady increase since the 1970s?
- What's with the big spike in measles cases in the 1980s?
- Why don't all the diseases have data going back to the earliest years in the dataset?
- Are there any state-by-state trends, or do they mainly follow the national trends?
- Hang on, these are all diseases that we have good vaccination programs for....

_Note: The corgis page with this data doesn't provide much info, but I think it's fairly safe to assume that when there are no cases reported for a given disease in a given year, that probably means there were no observed cases of that disease in that year._

Once you get to that last point, a lot of the other questions are probably pretty easy to explain! Let's see if the introduction of the vaccine for these diseases corresponds with the decline in cases.

The polio vaccine was tested in 1954 and introduced in 1955 by Jonas Salk, becoming a significant breakthrough in the fight against polio. Let's draw a little vertical dashed line at the year 1955 and see what it looks like.

```{python}
plot_data.plot(kind="line", x="Year", logy=True, ylabel="Cases")
# Draw a gray, dashed, vertical line at year 1955
plt.axvline(x=1955, color="#999999", linestyle="--")
```

Yep! As you might have guessed, the polio cases really start a sharp decline after the introduction of the vaccine.

What about smallpox? The history of smallpox vaccination is a bit more circuitous than polio, but two key dates I found were 1939, which saw the start of egg-based smallpox vaccine production by the Texas Department of Health, and 1948, which was when they began to be used in vaccination campaigns (see Wikipedia's [Smallpox vaccine](https://en.wikipedia.org/wiki/Smallpox_vaccine) article). So let's draw the plot again with those two dates highlighted.

```{python}
plot_data.plot(kind="line", x="Year", logy=True, ylabel="Cases")
plt.axvline(x=1939, color="#999999", linestyle="--")
plt.axvline(x=1948, color="#999999", linestyle="--")
```

Again, that sharp decline we see lines up pretty well with those key dates in the history of the smallpox vaccination program.

So, we have now gotten a pretty solid working hypothesis that the drop offs in cases for these diseases are probably due to the introduction of vaccination programs! But what about the interesting variability shown in the measles and pertussis data? I will leave that as an exercise for the reader....

### Public Health Data Wrap-Up

By examining public health data, we've gained experience working with time series data. We've also seen how the patterns we observe in this data can help direct our research questions and approaches.

## Pandas Tour

To wrap up, let's summarize everything by giving you a "little" reference that you can use for guidance.

For this, we can use the classic coffee data.

```{python}
coffee = pd.read_csv("./_data/coffee_small.csv")
coffee
```

### Creating Data Frames

There are a ton of ways to create data frames in Pandas. (E.g., check out the docs for [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) and the pages linked from that page.) However, to keep it simple, we will stick with [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv) and creating data frames from Python dictionaries.

Reading from a CSV file:

```python
df = read_csv("/path/to/data.csv")
```

Using a dictionary to specify columns:

```{python}
df = pd.DataFrame(
    {
        "Gene": ["gene_1", "gene_2", "gene_3"],
        "Sequence": ["ACTG", "AAGT", "GGCT"],
        "Sample": ["Sample 1", "Sample 2", "Sample 2"],
    }
)
df
```

### Subsetting and Filtering Rows and Columns

#### Selecting Columns

To get individual columns, use the bracket notation.

```{python}
coffee["Country"]
```

To select multiple columns, use the bracket notation, but pass in a list of column names rather than a single value.

```{python}
coffee[["Country", "Year"]]
```

It is often a good idea to save the columns that you want to get in a list, and pass that instead. This can help keep things clear and neat, especially if you have a ton of columns that you're selecting. Additionally, it can be useful if you build up the list of columns to subset programmatically.

```{python}
columns = ["Country", "Year"]
coffee[columns]
```

Sometimes, you may only want a single row, but you want the output to be a `DataFrame` rather than a `Series`.

```{python}
coffee[["Country"]]
```

Sometimes you want to select columns based on patterns. For example, to take all the columns that start with `Score`, we can use `filter()`:

```{python}
coffee.filter(regex=r"^Score")
```

That is using a regular expression (regex) to specify matching any column whose name starts with `Score`. (If you need an introduction to regular expressions see @sec-regex.)

You can also use a substring matching version:

```{python}
coffee.filter(like="Score")
```

#### Filtering Data

A common operation is to keep or reject rows of data based on their values in certain columns. For example, to keep study participants aged 65 and over, or to reject any counties with median income greater than $35,000. We can do this using `query()`.

Here are some examples with the coffee data.

Select rows where the country is Mexico:

```{python}
coffee.query("Country == 'Mexico'")
```

Select rows where the country is Colombia and the year is 2014:

```{python}
coffee.query("Country == 'Colombia' and Year == 2014")
```

Select rows where the aroma and flavor scores are both at least 8:

```{python}
coffee.query("`Score.Aroma` >= 8 and `Score.Aroma` >= 8")
```

Do you see how we put backticks around the column names? This is because it had a dot (`.`) in the name. Whenever you have special characters, like dot or spaces, you will need to use backticks around the variable name.

Select rows where the aftertaste score was at least 8 and the acidity was below 7:

```{python}
coffee.query("`Score.Aftertaste` >= 8 or `Score.Acidity` < 7")
```

Select rows where the year is prior to 2015, or the Flavor and the Aftertaste are above 7.75.

```{python}
coffee.query("Year < 2015 or (`Score.Flavor` > 7.75 and `Score.Aftertaste` > 7.75)")
```

You can also use the bracket notation for filtering data:

```{python}
coffee[coffee["Year"] >= 2014]
```

You will find that Pandas has many ways to achieve the same thing. While this flexibility can be helpful especially if you are doing some exploratory analysis for your research, it is often a good idea to stick to one way of doing things, especially within a single script or data pipeline.

#### Selecting Rows and Columns

Sometimes, you may need to select rows and columns at the same time. For this, you can use `loc()` and `iloc()`. These tend to feel more natural in situations in which you have a row index. Our data does not, so let's use this little data frame instead to illustrate what I mean.

```{python}
df = pd.DataFrame(
    {"A": [1, 3, 5, 6], "B": [2, 1, 4, 7], "C": [4, 3, 3, 1], "D": [9, 7, 4, 2]},
    index=["Patient 1", "Patient 2", "Patient 3", "Patient 4"],
)
df
```

In this data frame, row names are "Patient 1" to "Patient 4" and the column names are A-D.

Getting a single value:

```{python}
df.loc["Patient 2", "C"]
```

Getting the values for a single row as a series:

```{python}
df.loc["Patient 4", :]
```

Getting a single column as a series:

```{python}
df.loc[:, "A"]
```

Getting multiple rows for a single column:

```{python}
df.loc[["Patient 1", "Patient 3"], "B"]
```

Getting multiple contiguous rows for a single column. (We can use Python's slice notation for this. Just be aware that the end of the slice is included in the output when you use `loc()`, unlike the usual Python slicing.)

```{python}
df.loc["Patient 1":"Patient 3", "B"]
```

Getting multiple columns for a single row:

```{python}
df.loc["Patient 2", ["B", "D"]]
```

Getting multiple contiguous columns for a single row using slicing.

```{python}
df.loc["Patient 2", "B":"D"]
```

Getting multiple rows and multiple columns:

```{python}
df.loc[["Patient 1", "Patient 4"], ["B", "D"]]
```

And of course, you can mix and match slicing as required to get exactly the data you need:

```{python}
df.loc["Patient 1":"Patient 3", ["A", "C"]]
```

#### Subsetting Rows

There are a few nice functions for sampling and downsizing your data frames. This can be helpful if you're working with huge data, or if you just want to get a better handle on things before working on the full data.

Get the first few rows of a table:

```{python}
coffee.head()
```

Get the last few rows of a table:

```{python}
coffee.tail()
```

Sample 5% of the rows of a table:

```{python}
coffee.sample(frac=0.05)
```

Randomly select 10 rows from a table:

```{python}
coffee.sample(n=10)
```

Select the top 10 highest entries for a column:

```{python}
coffee.nlargest(10, "Score.Flavor")
```

Select the 10 smallest entries for a column:

```{python}
coffee.nsmallest(10, "Score.Aroma")
```

### Reshaping Data

You will often find yourself needing to change the layout of your data. This can include sorting, reindexing (i.e., adding or changing indices), renaming columns, concatenating, pivoting, and melting data frames.

#### Sorting Rows and Columns

Sorting a data frame by a specific column:

```{python}
coffee.sort_values("Score.Aftertaste")
```

The default sort order is ascending. If you want descending order, you must ask for it:

```{python}
coffee.sort_values("Score.Aftertaste", ascending=False)
```

You can sort values based on multiple columns as well. In addition, you can specify multiple values to the `ascending` option so that you can have some columns sorted ascending and others sorted descending. Let's first sort by ascending year, and then by descending flavor (that is within a year, put the best scoring coffees at the top).

```{python}
coffee.sort_values(["Year", "Score.Flavor"], ascending=[True, False])
```

Finally, you can sort based on the names of row and column indices. Here is an example that sorts by the names of the columns.

```{python}
coffee.sort_index(axis="columns")
```

In pandas, every row and column needs a unique identifier called an index. While pandas will automatically assign numeric indexes if none are provided, you can also specify custom row and column names to better organize and access your data.

#### Renaming Columns

Renaming columns is very useful for dealing with your colleague's messy data! You will learn this one like the back of your hand.

```{python}
coffee.rename(
    columns={
        "Score.Acidity": "Acidity",
        "Score.Aftertaste": "Aftertaste",
        "Score.Aroma": "Aroma",
        "Score.Flavor": "Flavor",
    }
)
```

In cases like this, where there is a pattern in the column names that you want to get rid of, you can use a Python function.

```{python}
coffee.rename(columns=lambda name: name.replace("Score.", ""))
```

Nice!

#### Dropping Columns

Dropping columns is easy with `drop()`:

```{python}
coffee.drop(columns=["Country", "Year"])
```

#### Reindexing

"Reindexing" sounds a bit weird, but it is all about editing row and column names. A common operation involves turning row names into an explicit column, or turning a column into row names. Let's go back to our tiny data frame again.

```{python}
df = pd.DataFrame(
    {"A": [1, 3, 5, 6], "B": [2, 1, 4, 7], "C": [4, 3, 3, 1], "D": [9, 7, 4, 2]},
    index=["Patient 1", "Patient 2", "Patient 3", "Patient 4"],
)
df
```

Currently, the row names are the patient IDs. Let's change those to a column called `"PatientID"`:

```{python}
df = df.reset_index(names="PatientID")

df
```

Now the row index is the numbers 0-3. If we want to turn the `PatientID` column back into the row index, we use `set_index()` like this:

```{python}
df = df.set_index("PatientID")
df
```

There are times when having a row index can be convenient, but generally I have data as columns only as it can make for cleaner data manipulation.

#### Concatenating

Sometimes you have multiple data frames that you want to glue together. This can be done either by glueing rows and keeping columns the same, or by glueing columns and keeping rows the same.

To see it in action, let's make two small data frames. One for group A:

```{python}
group_a = pd.DataFrame({"Treatment": [1, 2, 3], "Control": [4, 5, 6]})
group_a
```

And one for group B:

```{python}
group_b = pd.DataFrame({"Treatment": [10, 20, 30], "Control": [40, 50, 60]})
group_b
```

##### Glue Rows

Glue the rows (this also works for more than two data frames):

```{python}
pd.concat([group_a, group_b])
```

That's fine, but you probably still want the group info. You can add in the column before concatenating:

```{python}
pd.concat(
    [
        group_a.assign(Group="A"),
        group_b.assign(Group="B"),
    ]
)
```

There we go! Now we haven't lost the info.

##### Glue Columns

Glue the columns:

```{python}
print(pd.concat([group_a, group_b], axis="columns"))
```

Now that is a little silly to glue the columns in this way since it results in duplicate column names. You might want to follow it with a rename operation. Let's pretend that the two data frames represent two study groups, each having values for treatment and control. Then you might do something like this:

```{python}
pd.concat(
    [
        group_a.add_prefix("GroupA"),
        group_b.add_prefix("GroupB"),
    ],
    axis="columns",
)
```

Nice!

#### Pivoting and Melting

Alright, now this can get a little bit tricky. These operations are sometimes called pivot wider and pivot longer. Let's just talk about the basics, but know that you can get pretty fancy with this if required.

(We will use the renamed columns so the output looks a little nicer.)

```{python}
coffee_renamed = coffee.rename(columns=lambda name: name.replace("Score.", ""))
coffee_tidy = coffee_renamed.melt(
    # These variables identify observations
    id_vars=["Country", "Year"],
    # The column name for the variables
    var_name="Category",
    # The column name for the values of those variables
    value_name="Score",
)

coffee_tidy
```

Certain operations require [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) or are more natural when the data is in this format. For example, making plots using [seaborn](https://seaborn.pydata.org/).

```{python}
import seaborn as sns

sns.boxplot(coffee_tidy.sort_values("Category"), x="Category", y="Score", hue="Country")
```

(Converting the `coffee_tidy` data frame back to the messy form is a bit trickier because Country-Year pairs don't form unique pairs. So we will use a different example for this.)

```{python}
df = pd.DataFrame(
    {
        "Group": ["A", "A", "A", "A", "B", "B", "B", "B"],
        "Condition": [
            "Treatment",
            "Treatment",
            "Control",
            "Control",
            "Treatment",
            "Treatment",
            "Control",
            "Control",
        ],
        "Replicate": [1, 2, 1, 2, 1, 2, 1, 2],
        "Result": [53.6, 57.2, 66.3, 61.4, 48.6, 49.2, 63.5, 67.8],
    }
)

df
```

Now, we use `pivot()` to convert it back to the so-called "messy" format:

```{python}
df_messy = df.pivot(
    index=["Group", "Replicate"],
    columns="Condition",
    values="Result",
).reset_index()
df_messy.columns.name = None
df_messy
```

To go back to the tidy format:

```{python}
df_tidy = df_messy.melt(
    id_vars=["Group", "Replicate"],
    var_name="Condition",
    value_name="Result",
)
df_tidy
```

Check it out:

```{python}

a = (
    df.sort_values(by=["Group", "Condition", "Replicate"])
    .sort_index(axis="columns")
    .reset_index(drop=True)
)

display(a)

b = (
    df_tidy.sort_values(by=["Group", "Condition", "Replicate"])
    .sort_index(axis="columns")
    .reset_index(drop=True)
)

display(b)

assert a.equals(b)
```

_Note: we use `display()` here rather than print to get nice looking tables when we need to "print" multiple items in a single codeblock._

This is a bit of a trickier topic, but I still wanted to give you an idea of how it works for when you run into it in your own research.

### Making New Columns

There are a couple of ways to make new columns in a data frame.

The first way is with `assign()` which creates a new data frame with the requested column added to it.

Let's say that you wanted to make a score summary that is a linear combination of a few of the score columns.

```{python}
coffee.assign(
    Score=lambda df: 1.5 * df["Score.Aroma"]
    + 2 * df["Score.Flavor"]
    + 0.75 * df["Score.Aftertaste"]
)
```

If you check the original data frame, you will see that the `Score` column that we just created is not there:

```{python}
try:
    coffee["Score"]
except KeyError:
    print("no Score column!")
```

If you want to actually save that column in your original data frame, you need to use the bracket notation again.

```{python}
coffee["Score"] = (
    1.5 * coffee["Score.Aroma"]
    + 2 * coffee["Score.Flavor"]
    + 0.75 * coffee["Score.Aftertaste"]
)
coffee
```

And now the `Score` column is part of the `coffee` data frame. Let's drop it out so it doesn't stick around for the rest of the tutorial.

```{python}
coffee = coffee.drop(columns="Score")
```

### Grouping Data

Grouping and aggregating data is a very common operation. For example, you might want to run aggregation functions on groups of data like, what's the mean Flavor score for countries in this data set?

```{python}
coffee.groupby("Country").agg({"Score.Flavor": "mean"})
# coffee.groupby("Country").agg({"Score.Aroma": ["mean", "min", "mean"]})
```

You can run the aggregations on multiple columns at once. Here we take the mean flavor and the sum of the aroma. (Yes, that's a weird thing to do, it's just here to show you that you don't have to run the same function for each column.)

```{python}
coffee.groupby("Country").agg({"Score.Flavor": "mean", "Score.Aroma": "sum"})
```

You can even run multiple aggregation functions on each column:

```{python}
coffee.groupby("Country").agg(
    {"Score.Flavor": ["mean", "min", "max"], "Score.Aroma": "sum"}
)
```

In many cases you need to group based on multiple columns. In this dataset, it makes sense to group by Country-Year pairs:

```{python}
coffee.groupby(["Country", "Year"]).agg(
    {"Score.Flavor": ["mean", "std"], "Score.Aroma": ["mean", "std"]}
)
```

That would be the mean and standard deviation of flavor and aroma for all country-year pairs.

### Combining Data Sets

Another useful bit of functionality is merging datasets that have overlapping columns. These is a lot like doing joins in a relational database. While Pandas does have a `join()` function, we are mainly going to be using `merge()` instead, as it doesn't require that the tables have meaningful row indices.

Let's say we have a data frame that includes useful information about the countries for which we have coffee data. (Okay, we're not using necessarily "useful" data in this example, but work with me a bit.)

```{python}
country_capitals = pd.DataFrame(
    {
        "Country": ["Colombia", "Guatemala", "Mexico"],
        "Capital": ["Bogot", "Guatemala City", "Mexico City"],
    }
)
country_capitals
```

Okay, now let's say we wanted that info in our coffee data frame. We can join them using `merge()`.

```{python}
coffee.merge(country_capitals)
```

There is actually a lot to the `merge()` function which we won't get into too much here, but one important thing is how missing data is handled between data frames. Let's make some more data to look at this.

First, a data frame for jobs:

```{python}
jobs = pd.DataFrame(
    {
        "Name": ["Rahaf", "Nanjin", "Lujain", "Lovisa"],
        "Job": ["Chef", "Courier", "Engineer", "Bookmaker"],
    }
)
jobs
```

And another one for ages:

```{python}
ages = pd.DataFrame(
    {
        "Name": ["Rahaf", "Nanjin", "Lujain", "Lovisa"],
        "Age": ["47", "26", "31", "61"],
    }
)
ages
```

To demonstrate the joins, we will need to take subsets of the data frames in which some of the rows overlap and some of them don't:

```{python}
jobs_subset = jobs.head(3)
ages_subset = ages.tail(3)

display(jobs_subset)
display(ages_subset)
```

#### Inner Join

The inner join keeps rows that only belong to both sets of data. (This is like the inner join in SQL.)

```{python}
jobs_subset.merge(ages_subset, how="inner")
```

#### Outer Join

The outer join keeps all the rows, even if some of the rows are only present in one data frame or the other. Missing values will be `NaN` by default. (This is like the full outer join in SQL.)

```{python}
jobs_subset.merge(ages_subset, how="outer")
```

#### Left Join

The left join keeps all the rows in the left data frame, even if they are not present in the right data frame. Missing values will be `NaN` by default. (This is like the left outer join in SQL.)

```{python}
jobs_subset.merge(ages_subset, how="left")
```

#### Right Join

The right join keeps all the rows in the right data frame, even if they are not present in the left data frame. Missing values will be `NaN` by default. (This is like the right outer join in SQL.)

```{python}
jobs_subset.merge(ages_subset, how="right")
```

::: {#tip-07-merge-considerations .callout-tip title="Stop & Think" collapse="false"}
When merging datasets, what factors should you consider when choosing between inner, outer, left, and right joins?
:::

### Pandas Summary

Pandas is a huge and fairly complex library. But you can of get a lot of real work done by getting comfortable with a fairly small subset, and then working your way through more advanced concepts over time.

## Wrap-Up

In this chapter, we went over the basics of exploratory data analysis (EDA) in Python using the Pandas library. We went through three examples datasets together letting our curiosity guide us. Then, we went through a big tour of the Pandas library for you to use as a reference to some of the most frequently used Pandas functions. Pandas is a big library with a lot of different functions and different ways to do things. This chapter should give you enough Pandas skills to start using it in your own research, and give you the basics you need to get more in-depth with the library later in your career.

## Suggested Readings

Pandas is a massive library. These resources can help get a handle on it:

- [Pandas cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
  Pandas docs
- [Pandas User Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)
- [The pandas DataFrame: Make Working With Data Delightful](https://realpython.com/pandas-dataframe)

## Practice Problems

Consider the following data when answering the following problems.

```{python}
state_cancer_data = {
    "State": [
        "Delaware",
        "Maryland",
        "Virginia",
        "Pennsylvania",
        "New York",
        "New Jersey",
    ],
    "Cancer Deaths": [
        13_000,
        72_000,
        99_000,
        202_000,
        249_000,
        117_000,
    ],
    "Population": [
        6_300_000,
        40_500_000,
        56_100_000,
        88_800_000,
        135_700_000,
        61_500_000,
    ],
    "Percent Aged 65+": [
        19,
        16,
        16,
        19,
        17,
        17,
    ],
    "Median Household Income": [
        68_000,
        85_000,
        74_000,
        62_000,
        69_000,
        83_000,
    ],
}
```

### 7.1 {#sec-problem-7.1}

Create a Pandas DataFrame to represent the state cancer and demographic data given above.

### 7.2 {#sec-problem-7.2}

Create a new column called `Cancer Deaths Per 100k` that represents the number of cancer cases per 100,000 people for each state.

### 7.3 {#sec-problem-7.3}

Which states have a cancer rate of at least 180 cases per 100k and a median household income less than $68,500?

### 7.4 {#sec-problem-7.4}

What is the relationship between median household income and the rate of cancer deaths?

### 7.5 {#sec-problem-7.5}

What is the relationship between the percent of the population aged 65 and older and the rate of cancer deaths?
